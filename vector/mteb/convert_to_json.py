"""
å°† mteb æ£€ç´¢æ•°æ®é›†è½¬æ¢ä¸º JSON æ–‡ä»¶
è¾“å‡ºæ ¼å¼ï¼šåŒ…å« query_list å’Œ document_list çš„ JSON å¯¹è±¡ï¼Œä¸¤ä¸ªåˆ—è¡¨éƒ½å·²å»é‡

è¾“å‡ºæ ¼å¼ç¤ºä¾‹:
{
  "query_list": ["æŸ¥è¯¢æ–‡æœ¬1", "æŸ¥è¯¢æ–‡æœ¬2", ...],
  "document_list": ["æ–‡æ¡£æ–‡æœ¬1", "æ–‡æ¡£æ–‡æœ¬2", ...]
}

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨é»˜è®¤çš„å°æ•°æ®é›† (nfcorpus)
    python vector/mteb/convert_to_json.py

    # æŒ‡å®šæ•°æ®é›†å’Œè¾“å‡ºæ–‡ä»¶
    python vector/mteb/convert_to_json.py nfcorpus output.json

    # æŒ‡å®šæ‹†åˆ†ï¼ˆtrain/test/validationï¼‰
    python vector/mteb/convert_to_json.py nfcorpus output.json --split train

    # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†
    python vector/mteb/convert_to_json.py --list
"""
from datasets import load_dataset, load_from_disk
from typing import Dict, Any, Optional, List
import sys
import os
import json
from pathlib import Path

# å¤ç”¨ mteb_data_view.py ä¸­çš„æ•°æ®é›†åˆ—è¡¨å’ŒåŠ è½½å‡½æ•°
SMALL_DATASETS = {
    "nfcorpus": {
        "name": "mteb/nfcorpus",
        "description": "NFCorpus - çº¦3,600ä¸ªæ–‡æ¡£å’Œ323ä¸ªæŸ¥è¯¢ï¼Œéå¸¸å°",
        "size": "å¾ˆå°"
    },
    "scidocs": {
        "name": "mteb/scidocs",
        "description": "SciDocs - ç§‘å­¦æ–‡æ¡£æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "scifact": {
        "name": "mteb/scifact",
        "description": "SciFact - ç§‘å­¦äº‹å®æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "arguana": {
        "name": "mteb/arguana",
        "description": "ArguAna - è®ºè¯æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "quora": {
        "name": "mteb/quora",
        "description": "Quora - Quoraé‡å¤é—®é¢˜æ£€æµ‹æ•°æ®é›†",
        "size": "ä¸­ç­‰"
    },
    "msmarco": {
        "name": "mteb/msmarco",
        "description": "MS MARCO - å¤§è§„æ¨¡æ£€ç´¢æ•°æ®é›†ï¼ˆè¾ƒå¤§ï¼‰",
        "size": "å¾ˆå¤§"
    }
}

# ç¼“å­˜æ–‡æ¡£æŸ¥æ‰¾ç»“æœ
_doc_cache = {}
_query_cache = {}
# IDåˆ°æ–‡æ¡£çš„æ˜ å°„ï¼ˆé¢„å…ˆæ„å»ºï¼Œé¿å…é‡å¤æŸ¥æ‰¾ï¼‰
_doc_id_map = None
# IDåˆ°æŸ¥è¯¢çš„æ˜ å°„ï¼ˆé¢„å…ˆæ„å»ºï¼Œé¿å…é‡å¤æŸ¥æ‰¾ï¼‰
_query_id_map = None

def get_cache_info():
    """è·å–ç¼“å­˜ç›®å½•ä¿¡æ¯"""
    cache_dir = os.environ.get("HF_HOME") or os.path.expanduser("~/.cache/huggingface")
    datasets_cache = os.path.join(cache_dir, "datasets")
    return datasets_cache

def load_dataset_with_cache(dataset_name: str, use_cache: bool = True) -> tuple:
    """åŠ è½½æ•°æ®é›†å’Œcorpusï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶"""
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°ä¿å­˜çš„æ•°æ®é›†
    local_path = f".data/{dataset_name.replace('/', '_')}"
    if os.path.exists(local_path) and use_cache:
        try:
            dataset = load_from_disk(local_path)
            print(f"âœ… ä»æœ¬åœ°åŠ è½½æ•°æ®é›†: {local_path}")
            corpus = load_corpus_if_needed(dataset_name, use_cache)
            return dataset, corpus
        except Exception as e:
            print(f"âš ï¸  æœ¬åœ°åŠ è½½å¤±è´¥: {e}ï¼Œå°†å°è¯•ä»ç½‘ç»œåŠ è½½")

    # ä»ç½‘ç»œæˆ–ç¼“å­˜åŠ è½½
    download_mode = None if use_cache else "force_redownload"
    try:
        dataset = load_dataset(dataset_name, download_mode=download_mode)
        print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½å¤±è´¥: {e}")
        from datasets import get_dataset_config_names
        configs = get_dataset_config_names(dataset_name)
        if configs:
            dataset = load_dataset(dataset_name, configs[0], download_mode=download_mode)
            print(f"âœ… ä½¿ç”¨é…ç½®: {configs[0]}")
        else:
            raise

    corpus = load_corpus_if_needed(dataset_name, use_cache)
    return dataset, corpus

def load_queries_if_needed(dataset_name: str, use_cache: bool = True) -> Optional[Dict[str, Any]]:
    """å°è¯•åŠ è½½æŸ¥è¯¢é›†åˆï¼ˆqueriesï¼‰"""
    # æ£€æŸ¥æœ¬åœ°ä¿å­˜çš„queries
    queries_local_path = f".data/{dataset_name.replace('/', '_')}_queries"
    if os.path.exists(queries_local_path) and use_cache:
        try:
            queries = load_from_disk(queries_local_path)
            return queries
        except:
            pass

    # æå–åŸºç¡€åç§°
    base_name = dataset_name.replace("mteb/", "")
    queries_names = [
        f"{dataset_name}-queries",
        f"mteb/{base_name}-queries",
        f"{base_name}_queries",
    ]

    download_mode = None if use_cache else "force_redownload"

    for name in queries_names:
        try:
            queries = load_dataset(name, download_mode=download_mode)
            return queries
        except:
            continue

    # å°è¯•ä½¿ç”¨é…ç½®å
    try:
        queries = load_dataset(dataset_name, "queries", download_mode=download_mode)
        return queries
    except:
        pass

    return None

def load_corpus_if_needed(dataset_name: str, use_cache: bool = True) -> Optional[Dict[str, Any]]:
    """å°è¯•åŠ è½½æ–‡æ¡£é›†åˆï¼ˆcorpusï¼‰"""
    # æ£€æŸ¥æœ¬åœ°ä¿å­˜çš„corpus
    corpus_local_path = f".data/{dataset_name.replace('/', '_')}_corpus"
    if os.path.exists(corpus_local_path) and use_cache:
        try:
            corpus = load_from_disk(corpus_local_path)
            return corpus
        except:
            pass

    # æå–åŸºç¡€åç§°
    base_name = dataset_name.replace("mteb/", "")
    corpus_names = [
        f"{dataset_name}-corpus",
        f"mteb/{base_name}-corpus",
        base_name,
        f"{base_name}_corpus",
    ]

    download_mode = None if use_cache else "force_redownload"

    for name in corpus_names:
        try:
            corpus = load_dataset(name, download_mode=download_mode)
            return corpus
        except:
            continue

    # å°è¯•ä½¿ç”¨é…ç½®å
    try:
        corpus = load_dataset(dataset_name, "corpus", download_mode=download_mode)
        return corpus
    except:
        pass

    return None

def build_query_id_map(queries: Optional[Dict[str, Any]]) -> Dict[str, str]:
    """é¢„å…ˆæ„å»ºIDåˆ°æŸ¥è¯¢çš„æ˜ å°„ï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡"""
    global _query_id_map

    if _query_id_map is not None:
        return _query_id_map

    _query_id_map = {}

    if queries is None:
        return _query_id_map

    print("æ­£åœ¨æ„å»ºæŸ¥è¯¢IDæ˜ å°„ï¼ˆè¿™åªéœ€è¦ä¸€æ¬¡ï¼‰...")

    for split_name in queries.keys():
        split_data = queries[split_name]
        if len(split_data) == 0:
            continue

        first_item = split_data[0]
        id_field = None
        text_field = None

        # è¯†åˆ«å­—æ®µ
        for key in first_item.keys():
            key_lower = key.lower()
            if 'id' in key_lower and id_field is None:
                id_field = key
            if ('text' in key_lower or 'content' in key_lower or
                'query' in key_lower):
                if text_field is None or 'text' in key_lower or 'query' in key_lower:
                    text_field = key

        if not text_field:
            for key in first_item.keys():
                value = first_item[key]
                if isinstance(value, str) and len(value) > 10:
                    text_field = key
                    break

        if text_field:
            # æ‰¹é‡æ„å»ºæ˜ å°„
            total = len(split_data)
            for idx, item in enumerate(split_data):
                if id_field:
                    query_id = str(item.get(id_field))
                else:
                    query_id = str(idx)

                query_text = item.get(text_field)
                if query_text:
                    _query_id_map[query_id] = query_text

                if (idx + 1) % 1000 == 0:
                    print(f"  å·²å¤„ç†æŸ¥è¯¢: {idx + 1}/{total}")

    print(f"âœ… æŸ¥è¯¢æ˜ å°„æ„å»ºå®Œæˆï¼Œå…± {len(_query_id_map)} ä¸ªæŸ¥è¯¢")
    return _query_id_map

def build_doc_id_map(corpus: Optional[Dict[str, Any]]) -> Dict[str, str]:
    """é¢„å…ˆæ„å»ºIDåˆ°æ–‡æ¡£çš„æ˜ å°„ï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡"""
    global _doc_id_map

    if _doc_id_map is not None:
        return _doc_id_map

    _doc_id_map = {}

    if corpus is None:
        return _doc_id_map

    print("æ­£åœ¨æ„å»ºæ–‡æ¡£IDæ˜ å°„ï¼ˆè¿™åªéœ€è¦ä¸€æ¬¡ï¼‰...")

    for split_name in corpus.keys():
        split_data = corpus[split_name]
        if len(split_data) == 0:
            continue

        first_item = split_data[0]
        id_field = None
        text_field = None

        # è¯†åˆ«å­—æ®µ
        for key in first_item.keys():
            key_lower = key.lower()
            if 'id' in key_lower and id_field is None:
                id_field = key
            if ('text' in key_lower or 'content' in key_lower or
                'passage' in key_lower or 'body' in key_lower):
                if text_field is None or 'text' in key_lower or 'content' in key_lower:
                    text_field = key

        if not text_field:
            for key in first_item.keys():
                value = first_item[key]
                if isinstance(value, str) and len(value) > 50:
                    text_field = key
                    break

        if text_field:
            # æ‰¹é‡æ„å»ºæ˜ å°„
            total = len(split_data)
            for idx, item in enumerate(split_data):
                if id_field:
                    doc_id = str(item.get(id_field))
                else:
                    # å¦‚æœæ²¡æœ‰IDå­—æ®µï¼Œä½¿ç”¨ç´¢å¼•
                    doc_id = str(idx)

                doc_text = item.get(text_field)
                if doc_text:
                    _doc_id_map[doc_id] = doc_text

                if (idx + 1) % 10000 == 0:
                    print(f"  å·²å¤„ç†æ–‡æ¡£: {idx + 1}/{total}")

    print(f"âœ… æ–‡æ¡£æ˜ å°„æ„å»ºå®Œæˆï¼Œå…± {len(_doc_id_map)} ä¸ªæ–‡æ¡£")
    return _doc_id_map

def get_document_text(corpus: Optional[Dict[str, Any]], doc_id: Any) -> Optional[str]:
    """æ ¹æ®æ–‡æ¡£IDè·å–æ–‡æ¡£æ–‡æœ¬ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„æ˜ å°„ï¼‰"""
    if corpus is None:
        return None

    # ä½¿ç”¨é¢„æ„å»ºçš„æ˜ å°„
    doc_map = build_doc_id_map(corpus)
    cache_key = str(doc_id)

    # å…ˆä»æ˜ å°„ä¸­æŸ¥æ‰¾
    if cache_key in doc_map:
        return doc_map[cache_key]

    # å¦‚æœæ˜ å°„ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
    if cache_key in _doc_cache:
        return _doc_cache[cache_key]

    return None

def get_query_text(queries: Optional[Dict[str, Any]], query_id: Any, query_id_map: Optional[Dict[str, str]] = None) -> Optional[str]:
    """æ ¹æ®æŸ¥è¯¢IDè·å–æŸ¥è¯¢æ–‡æœ¬ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„æ˜ å°„ï¼‰"""
    if queries is None:
        return None

    # ä½¿ç”¨é¢„æ„å»ºçš„æ˜ å°„
    if query_id_map:
        cache_key = str(query_id)
        if cache_key in query_id_map:
            return query_id_map[cache_key]

    # å¦‚æœæ˜ å°„ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
    cache_key = str(query_id)
    if cache_key in _query_cache:
        return _query_cache[cache_key]

    return None

def is_likely_id(value: str) -> bool:
    """åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å¯èƒ½æ˜¯IDè€Œä¸æ˜¯å®é™…æ–‡æœ¬"""
    if not isinstance(value, str):
        return True
    # IDé€šå¸¸è¾ƒçŸ­ï¼Œä¸”ä¸åŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·ï¼ˆé™¤äº†è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ï¼‰
    if len(value) < 3:
        return True
    if len(value) < 20 and not any(c in value for c in [' ', '.', ',', '?', '!', ':', ';']):
        # å¯èƒ½æ˜¯IDï¼Œä½†ä¹Ÿè¦æ£€æŸ¥æ˜¯å¦åƒæ–‡æœ¬
        # å¦‚æœåŒ…å«å¸¸è§å•è¯ï¼Œå¯èƒ½æ˜¯æ–‡æœ¬
        common_words = ['the', 'and', 'or', 'is', 'are', 'was', 'were', 'what', 'how', 'why', 'when', 'where']
        value_lower = value.lower()
        if any(word in value_lower for word in common_words):
            return False
        # å¦‚æœå…¨æ˜¯æ•°å­—æˆ–å­—æ¯æ•°å­—ç»„åˆä¸”å¾ˆçŸ­ï¼Œå¯èƒ½æ˜¯ID
        if value.replace('-', '').replace('_', '').isalnum() and len(value) < 15:
            return True
    return False

def extract_query_and_document(example: Dict[str, Any], corpus: Optional[Dict[str, Any]], doc_id_map: Optional[Dict[str, str]] = None, queries: Optional[Dict[str, Any]] = None, query_id_map: Optional[Dict[str, str]] = None) -> Optional[Dict[str, Any]]:
    """ä»æ ·æœ¬ä¸­æå–queryã€documentå’Œscoreï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    query = None
    document = None
    score = None
    query_id_value = None

    # æŸ¥æ‰¾queryå­—æ®µ
    for key, value in example.items():
        key_lower = key.lower()
        if 'query' in key_lower:
            if isinstance(value, str):
                # å¦‚æœå·²ç»æ˜¯æ–‡æœ¬ï¼ˆé•¿åº¦è¾ƒé•¿æˆ–åŒ…å«å¸¸è§å•è¯ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                if len(value) > 20 or not is_likely_id(value):
                    query = value
                else:
                    # å¯èƒ½æ˜¯IDï¼Œä¿å­˜èµ·æ¥ç¨åæŸ¥æ‰¾
                    query_id_value = value
            else:
                query_id_value = str(value)
            break

    # å¦‚æœqueryæ˜¯IDï¼Œå°è¯•ä»queriesæ•°æ®é›†ä¸­æŸ¥æ‰¾
    if not query and query_id_value:
        if queries and query_id_map:
            query = get_query_text(queries, query_id_value, query_id_map)
        # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œä¸”çœ‹èµ·æ¥åƒIDï¼Œè¿”å›Noneï¼ˆè·³è¿‡è¿™ä¸ªæ ·æœ¬ï¼‰
        if not query and is_likely_id(query_id_value):
            return None
        # å¦‚æœæ‰¾ä¸åˆ°ä½†å¯èƒ½ä¸æ˜¯IDï¼Œä½¿ç”¨åŸå§‹å€¼
        if not query:
            query = query_id_value

    if not query:
        return None

    # ä¼˜å…ˆæŸ¥æ‰¾å·²æœ‰çš„æ–‡æ¡£æ–‡æœ¬å­—æ®µï¼ˆé¿å…IDæŸ¥æ‰¾ï¼‰
    for key, value in example.items():
        key_lower = key.lower()
        if isinstance(value, str) and len(value) > 10:
            # ä¼˜å…ˆåŒ¹é…åŒ…å«æ–‡æ¡£å†…å®¹çš„å­—æ®µ
            if any(x in key_lower for x in ['positive', 'passage', 'document', 'text', 'content', 'body']):
                if document is None or len(value) > len(document or ""):
                    document = value

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£æ–‡æœ¬ï¼Œå°è¯•é€šè¿‡IDæŸ¥æ‰¾
    if not document and corpus and doc_id_map:
        for key, value in example.items():
            key_lower = key.lower()
            if ('id' in key_lower or 'passage' in key_lower or 'doc' in key_lower) and not ('query' in key_lower):
                if isinstance(value, (int, str)):
                    doc_id = str(value)
                    if doc_id in doc_id_map:
                        document = doc_id_map[doc_id]
                        break
                    # å¦‚æœIDåœ¨æ˜ å°„ä¸­æ‰¾ä¸åˆ°ï¼Œä¸”çœ‹èµ·æ¥åƒIDï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                    elif is_likely_id(doc_id):
                        return None

    # æŸ¥æ‰¾scoreå­—æ®µ
    # ä¼˜å…ˆçº§ï¼š1. æ˜ç¡®çš„scoreå­—æ®µ 2. positive/negativeå­—æ®µ 3. relevance/rank/rating/labelå­—æ®µ

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰positive/negativeå­—æ®µï¼ˆæ£€ç´¢æ•°æ®é›†å¸¸ç”¨ï¼‰
    has_positive = False
    has_negative = False
    for key, value in example.items():
        key_lower = key.lower()
        if 'positive' in key_lower and not 'negative' in key_lower:
            has_positive = True
            # å¦‚æœpositiveå­—æ®µå­˜åœ¨ä¸”æ˜¯æ–‡æ¡£æ–‡æœ¬ï¼Œè¯´æ˜è¿™æ˜¯æ­£æ ·æœ¬
            if isinstance(value, str) and len(value) > 10:
                score = 1.0
                break
        elif 'negative' in key_lower and not 'positive' in key_lower:
            has_negative = True
            # å¦‚æœnegativeå­—æ®µå­˜åœ¨ä¸”æ˜¯æ–‡æ¡£æ–‡æœ¬ï¼Œè¯´æ˜è¿™æ˜¯è´Ÿæ ·æœ¬
            if isinstance(value, str) and len(value) > 10:
                score = 0.0
                break

    # å¦‚æœæ²¡æœ‰é€šè¿‡positive/negativeç¡®å®šscoreï¼ŒæŸ¥æ‰¾æ˜ç¡®çš„scoreå­—æ®µ
    if score is None:
        for key, value in example.items():
            key_lower = key.lower()
            if 'score' in key_lower:
                # scoreå¯èƒ½æ˜¯æ•°å­—æˆ–å­—ç¬¦ä¸²
                if isinstance(value, (int, float)):
                    score = float(value)
                elif isinstance(value, str):
                    try:
                        score = float(value)
                    except (ValueError, TypeError):
                        score = None
                else:
                    try:
                        score = float(value)
                    except (ValueError, TypeError):
                        score = None
                if score is not None:
                    break

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°scoreå­—æ®µï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å­—æ®µå
    if score is None:
        for key, value in example.items():
            key_lower = key.lower()
            # å¯èƒ½çš„scoreç›¸å…³å­—æ®µå
            if any(x in key_lower for x in ['relevance', 'rank', 'rating', 'label']):
                if isinstance(value, (int, float)):
                    score = float(value)
                elif isinstance(value, str):
                    try:
                        score = float(value)
                    except (ValueError, TypeError):
                        pass
                else:
                    try:
                        score = float(value)
                    except (ValueError, TypeError):
                        pass
                if score is not None:
                    break

    # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°scoreï¼Œä½†æ•°æ®é›†ä¸­æœ‰positiveå­—æ®µï¼Œé»˜è®¤è®¾ä¸º1.0ï¼ˆæ­£æ ·æœ¬ï¼‰
    # å¦‚æœæœ‰negativeå­—æ®µï¼Œé»˜è®¤è®¾ä¸º0.0ï¼ˆè´Ÿæ ·æœ¬ï¼‰
    if score is None:
        if has_positive:
            score = 1.0
        elif has_negative:
            score = 0.0

    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿queryå’Œdocumentéƒ½ä¸æ˜¯ID
    if query and document:
        # å¦‚æœqueryæˆ–documentçœ‹èµ·æ¥è¿˜æ˜¯IDï¼Œè·³è¿‡
        if is_likely_id(query) or is_likely_id(document):
            return None
        result = {"query": query, "document": document}
        if score is not None:
            result["score"] = score
        return result
    return None

def convert_dataset_to_json(dataset_name: str, output_file: str, split: str = "train", use_cache: bool = True, reset_cache: bool = False) -> Dict[str, Any]:
    """å°†æ•°æ®é›†è½¬æ¢ä¸ºJSONæ ¼å¼"""
    print("=" * 60)
    print(f"æ­£åœ¨è½¬æ¢æ•°æ®é›†: {dataset_name}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ‹†åˆ†: {split}")
    print("=" * 60)

    # é‡ç½®å…¨å±€ç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
    global _doc_id_map, _doc_cache, _query_id_map, _query_cache
    if reset_cache:
        _doc_id_map = None
        _doc_cache = {}
        _query_id_map = None
        _query_cache = {}

    # åŠ è½½æ•°æ®é›†
    cache_dir = get_cache_info()
    print(f"ğŸ“¦ ç¼“å­˜ç›®å½•: {cache_dir}")
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")

    dataset, corpus = load_dataset_with_cache(dataset_name, use_cache)

    # æ£€æŸ¥æ‹†åˆ†æ˜¯å¦å­˜åœ¨
    if split not in dataset:
        available_splits = list(dataset.keys())
        print(f"âŒ æ‹†åˆ† '{split}' ä¸å­˜åœ¨")
        print(f"å¯ç”¨çš„æ‹†åˆ†: {available_splits}")
        if available_splits:
            split = available_splits[0]
            print(f"ä½¿ç”¨æ‹†åˆ†: {split}")
        else:
            raise ValueError("æ•°æ®é›†æ²¡æœ‰å¯ç”¨çš„æ‹†åˆ†")

    split_data = dataset[split]
    print(f"\nğŸ“Š æ‹†åˆ†ä¿¡æ¯:")
    print(f"  - æ‹†åˆ†åç§°: {split}")
    print(f"  - æ ·æœ¬æ•°é‡: {len(split_data)}")
    fields = list(split_data[0].keys()) if len(split_data) > 0 else []
    print(f"  - å­—æ®µ: {fields}")

    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«æ–‡æ¡£æ–‡æœ¬å’ŒæŸ¥è¯¢æ–‡æœ¬ï¼ˆä¸éœ€è¦corpus/queriesï¼‰
    has_document_text = False
    has_query_text = False
    if len(split_data) > 0:
        first_item = split_data[0]
        for key, value in first_item.items():
            key_lower = key.lower()
            if isinstance(value, str) and len(value) > 10:
                if any(x in key_lower for x in ['positive', 'passage', 'document', 'text', 'content', 'body']):
                    has_document_text = True
                # æ£€æŸ¥queryå­—æ®µæ˜¯å¦å·²ç»æ˜¯æ–‡æœ¬ï¼ˆè€Œä¸æ˜¯IDï¼‰
                if 'query' in key_lower and len(value) > 20:
                    has_query_text = True

    # å°è¯•åŠ è½½queriesæ•°æ®é›†ï¼ˆåªæœ‰åœ¨éœ€è¦æ—¶ï¼‰
    queries = None
    if use_cache and not has_query_text:
        print("\næ­£åœ¨å°è¯•åŠ è½½queriesæ•°æ®é›†...")
        queries = load_queries_if_needed(dataset_name, use_cache)
        if queries:
            print("âœ… æˆåŠŸåŠ è½½queriesæ•°æ®é›†")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°queriesæ•°æ®é›†ï¼Œå°†å°è¯•ä»ä¸»æ•°æ®é›†ä¸­æå–")
    elif has_query_text:
        print("\nâœ… æ•°æ®é›†ä¸­å·²åŒ…å«æŸ¥è¯¢æ–‡æœ¬ï¼Œæ— éœ€åŠ è½½queriesæ•°æ®é›†")

    # åªæœ‰åœ¨éœ€è¦æ—¶æ‰åŠ è½½å’Œæ„å»ºcorpusæ˜ å°„
    doc_id_map = None
    if corpus and not has_document_text:
        print("\næ£€æµ‹åˆ°éœ€è¦ä»corpusè·å–æ–‡æ¡£ï¼Œæ­£åœ¨æ„å»ºæ˜ å°„...")
        doc_id_map = build_doc_id_map(corpus)
    elif has_document_text:
        print("\nâœ… æ•°æ®é›†ä¸­å·²åŒ…å«æ–‡æ¡£æ–‡æœ¬ï¼Œæ— éœ€åŠ è½½corpus")

    # é¢„å…ˆæ„å»ºæŸ¥è¯¢æ˜ å°„ï¼ˆå¦‚æœæœ‰queriesï¼‰
    query_id_map = None
    if queries:
        query_id_map = build_query_id_map(queries)

    # è½¬æ¢æ•°æ®ï¼šæ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ query å’Œ document
    print(f"\næ­£åœ¨è½¬æ¢æ•°æ®...")
    query_set = set()  # ä½¿ç”¨ set è‡ªåŠ¨å»é‡
    document_set = set()  # ä½¿ç”¨ set è‡ªåŠ¨å»é‡
    skipped = 0
    total = len(split_data)

    # ä½¿ç”¨æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
    batch_size = 1000
    for i in range(0, total, batch_size):
        batch_end = min(i + batch_size, total)
        batch = split_data.select(range(i, batch_end))

        for example in batch:
            result = extract_query_and_document(example, corpus, doc_id_map, queries, query_id_map)
            if result:
                query = result.get("query")
                document = result.get("document")
                if query:
                    query_set.add(query)
                if document:
                    document_set.add(document)
            else:
                skipped += 1

        # æ›´é¢‘ç¹çš„è¿›åº¦æ˜¾ç¤º
        processed = batch_end
        progress = (processed / total) * 100
        print(f"  è¿›åº¦: {processed}/{total} ({progress:.1f}%) - å”¯ä¸€query: {len(query_set)}, å”¯ä¸€document: {len(document_set)}, è·³è¿‡: {skipped}")

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åºï¼ˆä¿æŒé¡ºåºä¸€è‡´æ€§ï¼‰
    query_list = sorted(list(query_set))
    document_list = sorted(list(document_set))

    print(f"\nå»é‡ç»Ÿè®¡:")
    print(f"  åŸå§‹æ ·æœ¬æ•°: {total}")
    print(f"  å”¯ä¸€queryæ•°: {len(query_list)}")
    print(f"  å”¯ä¸€documentæ•°: {len(document_list)}")
    print(f"  è·³è¿‡æ ·æœ¬æ•°: {skipped}")

    # éªŒè¯è½¬æ¢ç»“æœï¼Œç¡®ä¿æ²¡æœ‰IDæ®‹ç•™
    print(f"\næ­£åœ¨éªŒè¯è½¬æ¢ç»“æœ...")
    validation_issues = []
    id_like_queries = []
    id_like_docs = []

    for idx, query in enumerate(query_list):
        # æ£€æŸ¥queryæ˜¯å¦è¿˜æ˜¯ID
        if is_likely_id(query):
            id_like_queries.append({
                "index": idx,
                "query": query[:50] if len(query) > 50 else query
            })

    for idx, document in enumerate(document_list):
        # æ£€æŸ¥documentæ˜¯å¦è¿˜æ˜¯ID
        if is_likely_id(document):
            id_like_docs.append({
                "index": idx,
                "document": document[:50] if len(document) > 50 else document
            })

    # æŠ¥å‘ŠéªŒè¯ç»“æœ
    if id_like_queries or id_like_docs:
        print(f"âš ï¸  è­¦å‘Šï¼šå‘ç°å¯èƒ½æœªè½¬æ¢çš„ID")
        if id_like_queries:
            print(f"  - å‘ç° {len(id_like_queries)} ä¸ªå¯èƒ½æœªè½¬æ¢çš„query ID")
            print(f"    å‰5ä¸ªç¤ºä¾‹:")
            for issue in id_like_queries[:5]:
                print(f"      ç´¢å¼• {issue['index']}: {issue['query']}")
        if id_like_docs:
            print(f"  - å‘ç° {len(id_like_docs)} ä¸ªå¯èƒ½æœªè½¬æ¢çš„document ID")
            print(f"    å‰5ä¸ªç¤ºä¾‹:")
            for issue in id_like_docs[:5]:
                print(f"      ç´¢å¼• {issue['index']}: {issue['document']}")

        validation_issues = {
            "query_ids": len(id_like_queries),
            "document_ids": len(id_like_docs),
            "total_issues": len(id_like_queries) + len(id_like_docs)
        }
    else:
        print("âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰IDéƒ½å·²è½¬æ¢ä¸ºå®é™…æ–‡æœ¬")
        validation_issues = {
            "query_ids": 0,
            "document_ids": 0,
            "total_issues": 0
        }

    # ä¿å­˜JSONæ–‡ä»¶
    print(f"\næ­£åœ¨ä¿å­˜JSONæ–‡ä»¶...")
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)

    # æ„å»ºè¾“å‡ºæ ¼å¼ï¼š{"query_list": [...], "document_list": [...]}
    output_data = {
        "query_list": query_list,
        "document_list": document_list
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "dataset": dataset_name,
        "split": split,
        "total_samples": len(split_data),
        "unique_queries": len(query_list),
        "unique_documents": len(document_list),
        "skipped_samples": skipped,
        "output_file": output_file,
        "file_size_mb": os.path.getsize(output_file) / (1024 * 1024),
        "validation": validation_issues
    }

    # è®¡ç®—å¹³å‡é•¿åº¦
    if query_list:
        avg_query_len = sum(len(q) for q in query_list) / len(query_list)
        stats["avg_query_length"] = round(avg_query_len, 2)

    if document_list:
        avg_doc_len = sum(len(d) for d in document_list) / len(document_list)
        stats["avg_document_length"] = round(avg_doc_len, 2)

    return stats

def print_dataset_list():
    """æ‰“å°å¯ç”¨çš„æ•°æ®é›†åˆ—è¡¨"""
    print("=" * 60)
    print("å¯ç”¨çš„æ£€ç´¢æ•°æ®é›†:")
    print("=" * 60)
    for key, info in SMALL_DATASETS.items():
        print(f"\n  {key}:")
        print(f"    - {info['description']}")
        print(f"    - å¤§å°: {info['size']}")
        print(f"    - æ•°æ®é›†å: {info['name']}")

if __name__ == "__main__":
    dataset_key = "nfcorpus"
    output_file = "output.json"
    split = "train"
    use_cache = True

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg in ["--list", "-l", "list"]:
            print_dataset_list()
            sys.exit(0)
        elif arg in ["--split", "-s"]:
            if i + 1 < len(sys.argv):
                split = sys.argv[i + 1]
                i += 2
            else:
                print("âŒ --split éœ€è¦æŒ‡å®šæ‹†åˆ†åç§°")
                sys.exit(1)
        elif arg in ["--force-download", "-f"]:
            use_cache = False
            i += 1
        elif arg.startswith("--"):
            print(f"âŒ æœªçŸ¥é€‰é¡¹: {arg}")
            sys.exit(1)
        else:
            if i == 1:
                dataset_key = arg.lower()
            elif i == 2:
                output_file = arg
            i += 1

    # è·å–æ•°æ®é›†ä¿¡æ¯
    if dataset_key in SMALL_DATASETS:
        # ä½¿ç”¨ç™½åå•ä¸­çš„æ•°æ®é›†ä¿¡æ¯
        dataset_info = SMALL_DATASETS[dataset_key]
        dataset_name = dataset_info["name"]
        print(f"\nğŸ“¦ ä½¿ç”¨æ•°æ®é›†: {dataset_key}")
        print(f"   {dataset_info['description']}")
        print(f"   å¤§å°: {dataset_info['size']}\n")
    else:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†åç§°ï¼ˆè‡ªåŠ¨æ·»åŠ  mteb/ å‰ç¼€å¦‚æœä¸å­˜åœ¨ï¼‰
        if dataset_key.startswith("mteb/"):
            dataset_name = dataset_key
        else:
            dataset_name = f"mteb/{dataset_key}"
        print(f"\nğŸ“¦ ä½¿ç”¨æ•°æ®é›†: {dataset_name}")
        print(f"   (ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†ï¼Œä¸åœ¨æ¨èåˆ—è¡¨ä¸­)\n")

    # è½¬æ¢æ•°æ®
    try:
        stats = convert_dataset_to_json(dataset_name, output_file, split, use_cache)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("è½¬æ¢å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 60)
        print(f"æ•°æ®é›†: {stats['dataset']}")
        print(f"æ‹†åˆ†: {stats['split']}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"å”¯ä¸€queryæ•°: {stats['unique_queries']}")
        print(f"å”¯ä¸€documentæ•°: {stats['unique_documents']}")
        print(f"è·³è¿‡æ ·æœ¬: {stats['skipped_samples']}")
        if 'avg_query_length' in stats:
            print(f"å¹³å‡æŸ¥è¯¢é•¿åº¦: {stats['avg_query_length']} å­—ç¬¦")
        if 'avg_document_length' in stats:
            print(f"å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['avg_document_length']} å­—ç¬¦")
        print(f"è¾“å‡ºæ–‡ä»¶: {stats['output_file']}")
        print(f"æ–‡ä»¶å¤§å°: {stats['file_size_mb']:.2f} MB")
        if 'validation' in stats:
            val = stats['validation']
            if val['total_issues'] > 0:
                print(f"\nâš ï¸  éªŒè¯è­¦å‘Š:")
                print(f"  å¯èƒ½æœªè½¬æ¢çš„query ID: {val['query_ids']}")
                print(f"  å¯èƒ½æœªè½¬æ¢çš„document ID: {val['document_ids']}")
            else:
                print(f"\nâœ… éªŒè¯é€šè¿‡: æ‰€æœ‰IDéƒ½å·²è½¬æ¢ä¸ºå®é™…æ–‡æœ¬")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

