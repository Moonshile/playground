"""
ä½¿ç”¨ datasets åº“æŸ¥çœ‹ mteb ä¸­çš„æ£€ç´¢æ•°æ®é›†å†…å®¹ç¤ºä¾‹
æ”¯æŒå¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¾ƒå°çš„æ›¿ä»£é€‰é¡¹

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨é»˜è®¤çš„å°æ•°æ®é›† (nfcorpus) - ä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜
    python vector/mteb/mteb_data_view.py

    # æŒ‡å®šæ•°æ®é›†
    python vector/mteb/mteb_data_view.py nfcorpus
    python vector/mteb/mteb_data_view.py scidocs

    # ä¿å­˜åˆ°æœ¬åœ°ä»¥ä¾¿ä¸‹æ¬¡æ›´å¿«åŠ è½½ï¼ˆæ¨èï¼‰
    python vector/mteb/mteb_data_view.py nfcorpus --save-local

    # å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    python vector/mteb/mteb_data_view.py nfcorpus --force-download

    # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†
    python vector/mteb/mteb_data_view.py --list

ç¼“å­˜è¯´æ˜:
    - datasets åº“é»˜è®¤ä¼šç¼“å­˜ä¸‹è½½çš„æ•°æ®é›†åˆ° ~/.cache/huggingface/datasets
    - é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ•°æ®é›†ï¼Œåç»­è¿è¡Œä¼šç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼Œä¸ä¼šé‡æ–°ä¸‹è½½
    - ä½¿ç”¨ --save-local å¯ä»¥å°†æ•°æ®é›†ä¿å­˜åˆ°é¡¹ç›®æœ¬åœ° .data/ ç›®å½•ï¼ŒåŠ è½½æ›´å¿«
    - ä½¿ç”¨ --force-download å¯ä»¥å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰

æ¨èçš„å°å‹æ•°æ®é›†:
    - nfcorpus: æœ€å°ï¼Œçº¦3,600ä¸ªæ–‡æ¡£å’Œ323ä¸ªæŸ¥è¯¢
    - scidocs: ç§‘å­¦æ–‡æ¡£æ£€ç´¢
    - scifact: ç§‘å­¦äº‹å®æ£€ç´¢
    - arguana: è®ºè¯æ£€ç´¢
"""
from datasets import load_dataset, load_from_disk
from typing import Dict, Any, Optional
import sys
import os
from pathlib import Path

# æ¨èçš„å°å‹æ£€ç´¢æ•°æ®é›†åˆ—è¡¨
SMALL_DATASETS = {
    "nfcorpus": {
        "name": "mteb/nfcorpus",
        "description": "NFCorpus - çº¦3,600ä¸ªæ–‡æ¡£å’Œ323ä¸ªæŸ¥è¯¢ï¼Œéå¸¸å°",
        "size": "å¾ˆå°"
    },
    "scidocs": {
        "name": "mteb/scidocs",
        "description": "SciDocs - ç§‘å­¦æ–‡æ¡£æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "scifact": {
        "name": "mteb/scifact",
        "description": "SciFact - ç§‘å­¦äº‹å®æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "arguana": {
        "name": "mteb/arguana",
        "description": "ArguAna - è®ºè¯æ£€ç´¢æ•°æ®é›†",
        "size": "å°"
    },
    "quora": {
        "name": "mteb/quora",
        "description": "Quora - Quoraé‡å¤é—®é¢˜æ£€æµ‹æ•°æ®é›†",
        "size": "ä¸­ç­‰"
    },
    "msmarco": {
        "name": "mteb/msmarco",
        "description": "MS MARCO - å¤§è§„æ¨¡æ£€ç´¢æ•°æ®é›†ï¼ˆè¾ƒå¤§ï¼‰",
        "size": "å¾ˆå¤§"
    }
}

def load_corpus_if_needed(dataset_name: str, use_cache: bool = True) -> Optional[Dict[str, Any]]:
    """å°è¯•åŠ è½½æ–‡æ¡£é›†åˆï¼ˆcorpusï¼‰"""
    # æ£€æŸ¥æœ¬åœ°ä¿å­˜çš„corpus
    corpus_local_path = f".data/{dataset_name.replace('/', '_')}_corpus"
    if os.path.exists(corpus_local_path) and use_cache:
        try:
            corpus = load_from_disk(corpus_local_path)
            print(f"âœ… ä»æœ¬åœ°åŠ è½½corpus: {corpus_local_path}")
            return corpus
        except:
            pass

    # æå–åŸºç¡€åç§°ï¼ˆå»æ‰mteb/å‰ç¼€ï¼‰
    base_name = dataset_name.replace("mteb/", "")

    # å°è¯•å¤šç§å¯èƒ½çš„corpusæ•°æ®é›†åç§°
    corpus_names = [
        f"{dataset_name}-corpus",
        f"mteb/{base_name}-corpus",
        base_name,
        f"{base_name}_corpus",
    ]

    download_mode = None if use_cache else "force_redownload"

    for name in corpus_names:
        try:
            corpus = load_dataset(name, download_mode=download_mode)
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£é›†åˆ: {name}")
            return corpus
        except Exception as e:
            continue

    # å°è¯•ä½¿ç”¨é…ç½®å
    try:
        corpus = load_dataset(dataset_name, "corpus", download_mode=download_mode)
        print("âœ… æˆåŠŸåŠ è½½corpusæ•°æ®é›†ï¼ˆä½¿ç”¨é…ç½®åï¼‰")
        return corpus
    except:
        pass

    # å°è¯•ç›´æ¥åŠ è½½å¯èƒ½åŒ…å«æ–‡æ¡£çš„æ•°æ®é›†
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–é…ç½®
        from datasets import get_dataset_config_names
        configs = get_dataset_config_names(dataset_name)
        print(f"ğŸ“‹ å¯ç”¨çš„é…ç½®: {configs}")
        for config in configs:
            if 'corpus' in config.lower() or 'passage' in config.lower() or 'doc' in config.lower():
                corpus = load_dataset(dataset_name, config, download_mode=download_mode)
                print(f"âœ… æˆåŠŸåŠ è½½é…ç½®: {config}")
                return corpus
    except:
        pass

    print("âš ï¸  æ— æ³•åŠ è½½corpusæ•°æ®é›†ï¼Œå°†åªæ˜¾ç¤ºID")
    return None

# ç¼“å­˜æ–‡æ¡£æŸ¥æ‰¾ç»“æœ
_doc_cache = {}

def get_document_text(corpus: Optional[Dict[str, Any]], doc_id: Any) -> Optional[str]:
    """æ ¹æ®æ–‡æ¡£IDè·å–æ–‡æ¡£æ–‡æœ¬"""
    if corpus is None:
        return None

    # ä½¿ç”¨ç¼“å­˜
    cache_key = str(doc_id)
    if cache_key in _doc_cache:
        return _doc_cache[cache_key]

    # å°è¯•åœ¨ä¸åŒæ‹†åˆ†ä¸­æŸ¥æ‰¾
    for split_name in corpus.keys():
        split_data = corpus[split_name]
        if len(split_data) == 0:
            continue

        # æ£€æŸ¥å­—æ®µç»“æ„
        first_item = split_data[0]
        id_field = None
        text_field = None

        # æŸ¥æ‰¾IDå­—æ®µå’Œæ–‡æœ¬å­—æ®µ
        for key in first_item.keys():
            key_lower = key.lower()
            if 'id' in key_lower and id_field is None:
                id_field = key
            if ('text' in key_lower or 'content' in key_lower or
                'passage' in key_lower or 'body' in key_lower or
                ('title' in key_lower and text_field is None)):
                if text_field is None or 'text' in key_lower or 'content' in key_lower:
                    text_field = key

        if not text_field:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ–‡æœ¬å­—æ®µï¼Œå°è¯•ä½¿ç”¨æœ€é•¿çš„å­—ç¬¦ä¸²å­—æ®µ
            for key in first_item.keys():
                value = first_item[key]
                if isinstance(value, str) and len(value) > 50:
                    text_field = key
                    break

        if text_field:
            # å°è¯•æŸ¥æ‰¾æ–‡æ¡£
            try:
                # å¦‚æœdoc_idæ˜¯æ•°å­—ï¼Œå°è¯•ç›´æ¥ç´¢å¼•
                if isinstance(doc_id, (int, str)) and str(doc_id).isdigit():
                    idx = int(doc_id)
                    if 0 <= idx < len(split_data):
                        result = split_data[idx].get(text_field)
                        if result:
                            _doc_cache[cache_key] = result
                            return result

                # å¦‚æœæœ‰IDå­—æ®µï¼Œé€šè¿‡IDæŸ¥æ‰¾
                if id_field:
                    for item in split_data:
                        if str(item.get(id_field)) == str(doc_id):
                            result = item.get(text_field)
                            if result:
                                _doc_cache[cache_key] = result
                                return result

                # å¦‚æœIDå­—æ®µå°±æ˜¯ç´¢å¼•ï¼Œç›´æ¥ä½¿ç”¨
                if id_field and split_data[0].get(id_field) == 0:
                    # å¯èƒ½æ˜¯ç´¢å¼•å­—æ®µ
                    try:
                        idx = int(doc_id)
                        if 0 <= idx < len(split_data):
                            result = split_data[idx].get(text_field)
                            if result:
                                _doc_cache[cache_key] = result
                                return result
                    except:
                        pass
            except Exception as e:
                pass

    return None

def get_cache_info():
    """è·å–ç¼“å­˜ç›®å½•ä¿¡æ¯"""
    cache_dir = os.environ.get("HF_HOME") or os.path.expanduser("~/.cache/huggingface")
    datasets_cache = os.path.join(cache_dir, "datasets")
    return datasets_cache

def view_dataset_data(dataset_name: str = "mteb/nfcorpus", use_cache: bool = True, save_local: bool = False):
    """åŠ è½½å¹¶æŸ¥çœ‹æŒ‡å®šæ•°æ®é›†çš„å†…å®¹

    Args:
        dataset_name: æ•°æ®é›†åç§°
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼Œä¼šä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®ï¼‰
        save_local: æ˜¯å¦ä¿å­˜åˆ°æœ¬åœ°ç›®å½•ä»¥ä¾¿æ›´å¿«åŠ è½½ï¼ˆé»˜è®¤Falseï¼‰
    """

    print("=" * 60)
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name}")
    print("=" * 60)

    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    cache_dir = get_cache_info()
    print(f"ğŸ“¦ ç¼“å­˜ç›®å½•: {cache_dir}")
    if use_cache:
        print("âœ… å°†ä½¿ç”¨ç¼“å­˜ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼Œä¸ä¼šé‡æ–°ä¸‹è½½ï¼‰")
    else:
        print("âš ï¸  å°†å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°ä¿å­˜çš„æ•°æ®é›†
    local_path = f".data/{dataset_name.replace('/', '_')}"
    dataset = None
    if os.path.exists(local_path) and use_cache:
        print(f"\nğŸ“‚ å‘ç°æœ¬åœ°ä¿å­˜çš„æ•°æ®é›†: {local_path}")
        print("   æ­£åœ¨ä»æœ¬åœ°åŠ è½½ï¼ˆæœ€å¿«ï¼‰...")
        try:
            dataset = load_from_disk(local_path)
            print("âœ… æˆåŠŸä»æœ¬åœ°åŠ è½½æ•°æ®é›†")
        except Exception as e:
            print(f"âš ï¸  æœ¬åœ°åŠ è½½å¤±è´¥: {e}ï¼Œå°†å°è¯•ä»ç½‘ç»œåŠ è½½")
            dataset = None

    # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥æˆ–æœªæ‰¾åˆ°ï¼Œä»ç½‘ç»œåŠ è½½
    if dataset is None:
        # åŠ è½½æ•°æ®é›† - å°è¯•ä¸åŒçš„é…ç½®
        print("\næ­£åœ¨ä»ç½‘ç»œåŠ è½½æ•°æ®é›†...")
        try:
            # å¦‚æœè®¾ç½®äº†use_cache=Falseï¼Œå¯ä»¥é€šè¿‡è®¾ç½®download_modeæ§åˆ¶
            download_mode = None if use_cache else "force_redownload"
            dataset = load_dataset(dataset_name, download_mode=download_mode)
            print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")

            # å¦‚æœå¯ç”¨ä¿å­˜æœ¬åœ°ï¼Œä¿å­˜æ•°æ®é›†
            if save_local:
                os.makedirs(local_path, exist_ok=True)
                print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°: {local_path}")
                dataset.save_to_disk(local_path)
                print("âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡è¿è¡Œå°†æ›´å¿«åŠ è½½")

        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥: {e}")
            # å°è¯•æŸ¥çœ‹å¯ç”¨é…ç½®
            try:
                from datasets import get_dataset_config_names
                configs = get_dataset_config_names(dataset_name)
                print(f"ğŸ“‹ å¯ç”¨çš„é…ç½®: {configs}")
                if configs:
                    download_mode = None if use_cache else "force_redownload"
                    dataset = load_dataset(dataset_name, configs[0], download_mode=download_mode)
                    print(f"âœ… ä½¿ç”¨é…ç½®: {configs[0]}")
                    if save_local:
                        os.makedirs(local_path, exist_ok=True)
                        dataset.save_to_disk(local_path)
            except:
                raise

    # å°è¯•åŠ è½½æ–‡æ¡£é›†åˆ
    print("\næ­£åœ¨å°è¯•åŠ è½½æ–‡æ¡£é›†åˆ...")
    corpus = load_corpus_if_needed(dataset_name, use_cache=use_cache)

    # æŸ¥çœ‹æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    print(dataset)

    # æŸ¥çœ‹å„ä¸ªæ‹†åˆ†çš„è¯¦ç»†ä¿¡æ¯
    print("\n" + "=" * 60)
    print("æ•°æ®é›†æ‹†åˆ†è¯¦æƒ…:")
    print("=" * 60)

    for split_name in dataset.keys():
        split_data = dataset[split_name]
        print(f"\n{split_name.upper()} æ‹†åˆ†:")
        print(f"  - æ ·æœ¬æ•°é‡: {len(split_data)}")
        print(f"  - ç‰¹å¾å­—æ®µ: {split_data.features}")

        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬æ¥æ£€æŸ¥å®é™…å­—æ®µå
        if len(split_data) > 0:
            first_item = split_data[0]
            print(f"  - å®é™…å­—æ®µå: {list(first_item.keys())}")

        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
        print(f"\n  å‰ 3 ä¸ªæ ·æœ¬ç¤ºä¾‹:")
        print("-" * 60)
        for i, example in enumerate(split_data.select(range(min(3, len(split_data))))):
            print(f"\n  æ ·æœ¬ {i+1}:")
            # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µï¼Œå¹¶å°è¯•è·å–æ–‡æ¡£å†…å®¹
            for key, value in example.items():
                # å¦‚æœå€¼çœ‹èµ·æ¥åƒIDï¼ˆæ•°å­—æˆ–çŸ­å­—ç¬¦ä¸²ï¼‰ä¸”å­—æ®µååŒ…å«id/passage/docï¼Œå°è¯•è·å–å®é™…æ–‡æœ¬
                is_id_field = ('id' in key.lower() or 'passage' in key.lower() or 'doc' in key.lower())
                is_id_value = (isinstance(value, (int, str)) and
                              (isinstance(value, int) or (isinstance(value, str) and len(str(value)) < 50 and not ' ' in str(value))))

                if corpus and is_id_field and is_id_value:
                    doc_text = get_document_text(corpus, value)
                    if doc_text:
                        print(f"    {key} (ID): {value}")
                        print(f"    {key}_text: {doc_text[:300]}..." if len(doc_text) > 300 else f"    {key}_text: {doc_text}")
                    else:
                        print(f"    {key}: {value} (æœªæ‰¾åˆ°å¯¹åº”æ–‡æ¡£)")
                elif isinstance(value, str):
                    # å¦‚æœå·²ç»æ˜¯æ–‡æœ¬å†…å®¹ï¼Œç›´æ¥æ˜¾ç¤º
                    if len(value) > 300:
                        print(f"    {key}: {value[:300]}...")
                    else:
                        print(f"    {key}: {value}")
                else:
                    print(f"    {key}: {value}")

    # æŸ¥çœ‹å®Œæ•´æ ·æœ¬ï¼ˆç¬¬ä¸€ä¸ªï¼‰
    print("\n" + "=" * 60)
    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ‹†åˆ†
    first_split_name = None
    first_split_data = None
    for split_name in dataset.keys():
        if len(dataset[split_name]) > 0:
            first_split_name = split_name
            first_split_data = dataset[split_name]
            break

    if first_split_name:
        print(f"å®Œæ•´æ ·æœ¬ç¤ºä¾‹ ({first_split_name} æ‹†åˆ†ç¬¬ä¸€ä¸ªæ ·æœ¬):")
        print("=" * 60)
        first_example = first_split_data[0]
        print(f"\nå®Œæ•´æ ·æœ¬å†…å®¹:")
        for key, value in first_example.items():
            print(f"\n{key}:")
            # å¦‚æœæ˜¯IDå­—æ®µï¼Œå°è¯•è·å–å®é™…æ–‡æœ¬
            if corpus and ('id' in key.lower() or 'passage' in key.lower() or 'doc' in key.lower()):
                doc_text = get_document_text(corpus, value)
                if doc_text:
                    print(f"  ID: {value}")
                    print(f"  å®é™…å†…å®¹: {doc_text}")
                else:
                    print(f"  {value}")
            else:
                # å¦‚æœæ˜¯é•¿æ–‡æœ¬ï¼Œæˆªæ–­æ˜¾ç¤º
                if isinstance(value, str) and len(value) > 500:
                    print(f"  {value[:500]}...")
                else:
                    print(f"  {value}")
    else:
        print("å®Œæ•´æ ·æœ¬ç¤ºä¾‹:")
        print("=" * 60)
        print("âš ï¸  æ•°æ®é›†ä¸­æ²¡æœ‰å¯ç”¨çš„æ‹†åˆ†æˆ–æ‹†åˆ†ä¸ºç©º")

    return dataset, corpus


def print_dataset_list():
    """æ‰“å°å¯ç”¨çš„æ•°æ®é›†åˆ—è¡¨"""
    print("=" * 60)
    print("å¯ç”¨çš„æ£€ç´¢æ•°æ®é›†:")
    print("=" * 60)
    for key, info in SMALL_DATASETS.items():
        print(f"\n  {key}:")
        print(f"    - {info['description']}")
        print(f"    - å¤§å°: {info['size']}")
        print(f"    - æ•°æ®é›†å: {info['name']}")

if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    dataset_key = "nfcorpus"  # é»˜è®¤ä½¿ç”¨æœ€å°çš„æ•°æ®é›†
    use_cache = True
    save_local = False
    force_download = False

    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if arg in ["--list", "-l", "list"]:
                print_dataset_list()
                sys.exit(0)
            elif arg in ["--save-local", "-s"]:
                save_local = True
            elif arg in ["--force-download", "-f"]:
                force_download = True
                use_cache = False
            elif arg in ["--no-cache"]:
                use_cache = False
            elif not arg.startswith("--") and not arg.startswith("-"):
                # å¦‚æœä¸æ˜¯é€‰é¡¹ï¼Œå¯èƒ½æ˜¯æ•°æ®é›†åç§°
                dataset_key = arg.lower()

    # è·å–æ•°æ®é›†ä¿¡æ¯
    if dataset_key in SMALL_DATASETS:
        # ä½¿ç”¨ç™½åå•ä¸­çš„æ•°æ®é›†ä¿¡æ¯
        dataset_info = SMALL_DATASETS[dataset_key]
        dataset_name = dataset_info["name"]
        print(f"\nğŸ“¦ ä½¿ç”¨æ•°æ®é›†: {dataset_key}")
        print(f"   {dataset_info['description']}")
        print(f"   å¤§å°: {dataset_info['size']}")
    else:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†åç§°ï¼ˆè‡ªåŠ¨æ·»åŠ  mteb/ å‰ç¼€å¦‚æœä¸å­˜åœ¨ï¼‰
        if dataset_key.startswith("mteb/"):
            dataset_name = dataset_key
        else:
            dataset_name = f"mteb/{dataset_key}"
        print(f"\nğŸ“¦ ä½¿ç”¨æ•°æ®é›†: {dataset_name}")
        print(f"   (ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†ï¼Œä¸åœ¨æ¨èåˆ—è¡¨ä¸­)")
    if save_local:
        print("   ğŸ’¾ å°†ä¿å­˜åˆ°æœ¬åœ°ä»¥ä¾¿ä¸‹æ¬¡å¿«é€ŸåŠ è½½")
    if force_download:
        print("   âš ï¸  å°†å¼ºåˆ¶é‡æ–°ä¸‹è½½")
    print()

    dataset, corpus = view_dataset_data(dataset_name, use_cache=use_cache, save_local=save_local)

    # å¯é€‰ï¼šè¿›ä¸€æ­¥åˆ†æ
    print("\n" + "=" * 60)
    print("æ•°æ®ç»Ÿè®¡:")
    print("=" * 60)

    # æ˜¾ç¤ºæ‰€æœ‰æ‹†åˆ†çš„ç»Ÿè®¡ä¿¡æ¯
    for split_name in dataset.keys():
        split_data = dataset[split_name]
        if len(split_data) == 0:
            continue

        print(f"\n{split_name.upper()} æ‹†åˆ†ç»Ÿè®¡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {len(split_data)}")

        # è®¡ç®—å¹³å‡æŸ¥è¯¢é•¿åº¦ï¼ˆå¦‚æœå­˜åœ¨ query å­—æ®µï¼‰
        if len(split_data) > 0:
            first_item = split_data[0]
            # å°è¯•æ‰¾åˆ°åŒ…å«æŸ¥è¯¢æ–‡æœ¬çš„å­—æ®µ
            query_field = None
            for key in first_item.keys():
                if 'query' in key.lower() or 'text' in key.lower():
                    query_field = key
                    break

            if query_field:
                sample_size = min(1000, len(split_data))
                query_lengths = [len(str(item[query_field])) for item in split_data.select(range(sample_size))]
                avg_query_len = sum(query_lengths) / len(query_lengths) if query_lengths else 0
                print(f"  - å¹³å‡{query_field}é•¿åº¦ (å‰{sample_size}ä¸ªæ ·æœ¬): {avg_query_len:.1f} å­—ç¬¦")

