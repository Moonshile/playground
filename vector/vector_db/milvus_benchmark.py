"""
Milvuså‘é‡æ•°æ®åº“è¯„æµ‹è„šæœ¬

æ ¹æ®spec.mdçš„è¦æ±‚ï¼Œå¯¹Milvusè¿›è¡Œå‘é‡æ£€ç´¢è¯„æµ‹ï¼š
- åŸºäºå‘é‡åŒ–è¯„æµ‹ç”Ÿæˆçš„q-då‘é‡
- æ‰€æœ‰documentå‘é‡å…¨éƒ¨å…¥åº“åå†å¼€å§‹è¯„æµ‹
- è¯„æµ‹åŸºå‡†ï¼šåŸºäºæš´åŠ›æ³•ç®—å‡ºæ¥çš„top-Næœ€è¿‘é‚»å‘é‡
- è¯„æµ‹æŒ‡æ ‡ï¼š
  - æ£€ç´¢èƒ½åŠ›ï¼šæ¯ä¸ªè¯·æ±‚åˆ†åˆ«è®¡ç®—å‡†ç¡®ç‡å’Œå¬å›ç‡ï¼Œæœ€åè®¡ç®—æ‰€æœ‰è¯·æ±‚çš„å¹³å‡å€¼
  - å¤„ç†èƒ½åŠ›ï¼šè®°å½•æ¯ä¸ªè¯·æ±‚çš„æ—¶é—´ï¼Œå¹¶æŠ¥å‘Šæœ€ç»ˆçš„æ—¶é—´åˆ†å¸ƒï¼ˆæœ€å¤§æœ€å°ã€å¹³å‡ã€åˆ†ä½æ•°ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_openai_vectors.json
    python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_gemini_vec.json -n 10
"""
import os
import json
import time
import argparse
import hashlib
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
from pymilvus import (
    connections,
    utility,
    FieldSchema,
    CollectionSchema,
    DataType,
    Collection,
    MilvusException
)


# ==================== å·¥å…·å‡½æ•° ====================

def get_milvus_client():
    """è·å–Milvuså®¢æˆ·ç«¯è¿æ¥"""
    cluster_endpoint = os.getenv("MILVUS_CLUSTER_ENDPOINT")
    token = os.getenv("MILUVS_TOKEN") or os.getenv("MILVUS_TOKEN")  # å…¼å®¹æ‹¼å†™é”™è¯¯

    if not cluster_endpoint:
        raise ValueError("è¯·è®¾ç½® MILVUS_CLUSTER_ENDPOINT ç¯å¢ƒå˜é‡")
    if not token:
        raise ValueError("è¯·è®¾ç½® MILVUS_TOKEN æˆ– MILUVS_TOKEN ç¯å¢ƒå˜é‡")

    connections.connect(
        alias="default",
        uri=cluster_endpoint,
        token=token
    )
    return connections.get_connection_addr("default")


def compute_sha2048(text: str) -> str:
    """
    è®¡ç®—æ–‡æœ¬çš„SHA-2048å“ˆå¸Œå€¼
    æ³¨æ„ï¼šSHA-2048å®é™…ä¸Šæ˜¯æŒ‡SHA-512ç®—æ³•ï¼ˆäº§ç”Ÿ512ä½=64å­—èŠ‚çš„å“ˆå¸Œå€¼ï¼‰
    è¿™é‡Œä½¿ç”¨SHA-512ç®—æ³•
    """
    return hashlib.sha512(text.encode('utf-8')).hexdigest()


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return float(dot_product / (norm1 * norm2))


def brute_force_top_n(
    query_vec: List[float],
    doc_vectors: List[List[float]],
    doc_ids: List[str],
    top_n: int
) -> List[Tuple[str, float]]:
    """
    ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—top-Næœ€è¿‘é‚»å‘é‡

    Returns:
        List of (doc_id, similarity_score) tuples, sorted by similarity descending
    """
    similarities = []
    for doc_id, doc_vec in zip(doc_ids, doc_vectors):
        sim = cosine_similarity(query_vec, doc_vec)
        similarities.append((doc_id, sim))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_n]


# ==================== æ•°æ®åŠ è½½ ====================

def load_vector_data(vector_file: str) -> Tuple[Dict[str, Any], str]:
    """
    åŠ è½½å‘é‡æ•°æ®æ–‡ä»¶

    Returns:
        (æ•°æ®å­—å…¸ï¼ˆåŒ…å«resultså’Œmetadataï¼‰, æ¨¡å‹åç§°)
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½å‘é‡æ•°æ®: {vector_file}")
    with open(vector_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # è·å–æ¨¡å‹åç§°
    model_name = data.get('metadata', {}).get('model', 'unknown')
    results = data.get('results', [])

    print(f"âœ… å·²åŠ è½½ {len(results)} æ¡æ•°æ®")
    print(f"   æ¨¡å‹: {model_name}")

    return data, model_name


def load_original_data(original_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½åŸå§‹QAæ•°æ®æ–‡ä»¶"""
    if not os.path.exists(original_file):
        return []

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: {original_file}")
    with open(original_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(data)} æ¡åŸå§‹æ•°æ®")
    return data


def extract_query_document_vectors(
    data: List[Dict[str, Any]],
    original_data_file: Optional[str] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ä»æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡

    Args:
        data: å‘é‡æ•°æ®åˆ—è¡¨
        original_data_file: åŸå§‹QAæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåŒ¹é…queryå’Œdocumentï¼‰

    Returns:
        (queryåˆ—è¡¨, documentåˆ—è¡¨)
    """
    queries = []
    documents = []

    # é¦–å…ˆå°è¯•ç›´æ¥æå–query_vectorå’Œdocument_vector
    for item in data:
        if 'query_vector' in item and item['query_vector'] is not None:
            queries.append({
                'query': item.get('query', ''),
                'vector': item['query_vector'],
                'document': item.get('document', ''),
                'score': item.get('score')
            })

        if 'document_vector' in item and item['document_vector'] is not None:
            doc_text = item.get('document', '')
            doc_hash = compute_sha2048(doc_text)
            documents.append({
                'document': doc_text,
                'vector': item['document_vector'],
                'hash': doc_hash
            })

    # å¦‚æœæ•°æ®æ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚åªæœ‰text_embeddingï¼‰ï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…
    if not queries and not documents:
        print("âš ï¸  æœªæ‰¾åˆ°query_vector/document_vectorå­—æ®µï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…...")

        # åŠ è½½åŸå§‹æ•°æ®
        original_data = []
        if original_data_file:
            if os.path.exists(original_data_file):
                original_data = load_original_data(original_data_file)
            else:
                # å°è¯•ç›¸å¯¹è·¯å¾„
                possible_paths = [
                    original_data_file,
                    os.path.join('.data', 'mteb', os.path.basename(original_data_file)),
                    os.path.join('.data/mteb', os.path.basename(original_data_file))
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        original_data = load_original_data(path)
                        break

        if not original_data:
            print("âŒ æ— æ³•æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œæ— æ³•åŒ¹é…queryå’Œdocument")
            return queries, documents

        # æ„å»ºæ–‡æœ¬åˆ°å‘é‡çš„æ˜ å°„
        text_to_vector = {}
        for item in data:
            text = item.get('text', '')
            vector = item.get('text_embedding') or item.get('vector')
            if text and vector:
                text_to_vector[text] = vector

        # ä»åŸå§‹æ•°æ®ä¸­åŒ¹é…queryå’Œdocument
        query_texts_seen = set()
        doc_texts_seen = set()

        for orig_item in original_data:
            query_text = orig_item.get('query', '')
            doc_text = orig_item.get('document', '')

            # åŒ¹é…queryå‘é‡
            if query_text and query_text in text_to_vector and query_text not in query_texts_seen:
                queries.append({
                    'query': query_text,
                    'vector': text_to_vector[query_text],
                    'document': doc_text,
                    'score': orig_item.get('score')
                })
                query_texts_seen.add(query_text)

            # åŒ¹é…documentå‘é‡
            if doc_text and doc_text in text_to_vector and doc_text not in doc_texts_seen:
                doc_hash = compute_sha2048(doc_text)
                documents.append({
                    'document': doc_text,
                    'vector': text_to_vector[doc_text],
                    'hash': doc_hash
                })
                doc_texts_seen.add(doc_text)

    print(f"ğŸ“Š æå–ç»“æœ:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")

    # æ£€æŸ¥æ•°é‡æ˜¯å¦åˆç†ï¼ˆé€šå¸¸queryæ•°é‡åº”è¯¥å°‘äºdocumentæ•°é‡ï¼‰
    if len(queries) > 0 and len(documents) > 0:
        if len(queries) > len(documents) * 2:
            print(f"âš ï¸  è­¦å‘Š: Queryæ•°é‡ ({len(queries)}) æ˜æ˜¾å¤§äºDocumentæ•°é‡ ({len(documents)})")
            print(f"   è¿™å¯èƒ½æ˜¯å¼‚å¸¸çš„ï¼Œè¯·æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®")
        elif len(documents) > len(queries) * 10:
            print(f"â„¹ï¸  ä¿¡æ¯: Documentæ•°é‡ ({len(documents)}) è¿œå¤§äºQueryæ•°é‡ ({len(queries)})ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
    elif len(queries) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•queryå‘é‡")
    elif len(documents) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•documentå‘é‡")
        print(f"   è¿™å¯èƒ½æ˜¯å› ä¸ºå‘é‡æ•°æ®æ–‡ä»¶ä¸å®Œæ•´ï¼ŒåªåŒ…å«äº†queryå‘é‡è€Œæ²¡æœ‰documentå‘é‡")

    return queries, documents


# ==================== Milvusæ“ä½œ ====================

def create_collection(collection_name: str, dimension: int) -> Collection:
    """
    åˆ›å»ºMilvus collection

    Args:
        collection_name: collectionåç§°
        dimension: å‘é‡ç»´åº¦

    Returns:
        Collectionå¯¹è±¡
    """
    # æ£€æŸ¥collectionæ˜¯å¦å·²å­˜åœ¨
    if utility.has_collection(collection_name):
        print(f"âš ï¸  Collection '{collection_name}' å·²å­˜åœ¨ï¼Œåˆ é™¤æ—§collection...")
        collection = Collection(collection_name)
        collection.drop()

    # å®šä¹‰schema
    fields = [
        FieldSchema(name="primary_key", dtype=DataType.VARCHAR, max_length=1024, is_primary=True),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dimension)
    ]

    schema = CollectionSchema(fields, description=f"Vector collection for {collection_name}")

    # åˆ›å»ºcollection
    collection = Collection(collection_name, schema)

    # åˆ›å»ºç´¢å¼•
    index_params = {
        "metric_type": "COSINE",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024}
    }
    collection.create_index("vector", index_params)

    print(f"âœ… å·²åˆ›å»ºcollection: {collection_name}")
    print(f"   ç»´åº¦: {dimension}")
    print(f"   ç´¢å¼•ç±»å‹: IVF_FLAT")

    return collection


def insert_documents(collection: Collection, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """
    æ’å…¥documentå‘é‡åˆ°Milvus

    Args:
        collection: Milvus collectionå¯¹è±¡
        documents: documentåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«hashå’Œvector
        batch_size: æ‰¹é‡æ’å…¥å¤§å°
    """
    print(f"ğŸ“¥ å¼€å§‹æ’å…¥ {len(documents)} ä¸ªdocumentå‘é‡...")

    primary_keys = [doc['hash'] for doc in documents]
    vectors = [doc['vector'] for doc in documents]

    # æ‰¹é‡æ’å…¥
    total_batches = (len(documents) + batch_size - 1) // batch_size
    for i in range(0, len(documents), batch_size):
        batch_keys = primary_keys[i:i + batch_size]
        batch_vectors = vectors[i:i + batch_size]

        batch_num = i // batch_size + 1
        print(f"   æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_keys)} æ¡)...", end='\r')

        collection.insert([batch_keys, batch_vectors])

    # åˆ·æ–°æ•°æ®ï¼Œç¡®ä¿å¯æœç´¢
    collection.flush()
    print(f"\nâœ… å·²æ’å…¥ {len(documents)} ä¸ªdocumentå‘é‡")

    # åŠ è½½collectionåˆ°å†…å­˜
    collection.load()
    print("âœ… Collectionå·²åŠ è½½åˆ°å†…å­˜")


def search_vectors(
    collection: Collection,
    query_vectors: List[List[float]],
    top_k: int
) -> List[List[Dict[str, Any]]]:
    """
    åœ¨Milvusä¸­æœç´¢å‘é‡

    Args:
        collection: Milvus collectionå¯¹è±¡
        query_vectors: æŸ¥è¯¢å‘é‡åˆ—è¡¨
        top_k: è¿”å›top-kç»“æœ

    Returns:
        æ¯ä¸ªqueryçš„æœç´¢ç»“æœåˆ—è¡¨
    """
    search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}

    results = collection.search(
        data=query_vectors,
        anns_field="vector",
        param=search_params,
        limit=top_k,
        output_fields=[]
    )

    # è½¬æ¢ç»“æœæ ¼å¼
    formatted_results = []
    for result in results:
        hits = []
        for hit in result:
            hits.append({
                'id': hit.id,
                'distance': hit.distance,
                'score': hit.score if hasattr(hit, 'score') else hit.distance
            })
        formatted_results.append(hits)

    return formatted_results


# ==================== è¯„æµ‹æŒ‡æ ‡ ====================

def calculate_metrics(
    retrieved_ids: List[str],
    ground_truth_ids: List[str]
) -> Tuple[float, float]:
    """
    è®¡ç®—å‡†ç¡®ç‡å’Œå¬å›ç‡

    Args:
        retrieved_ids: æ£€ç´¢åˆ°çš„IDåˆ—è¡¨
        ground_truth_ids: çœŸå®top-Nçš„IDåˆ—è¡¨

    Returns:
        (å‡†ç¡®ç‡, å¬å›ç‡)
    """
    retrieved_set = set(retrieved_ids)
    ground_truth_set = set(ground_truth_ids)

    # äº¤é›†
    intersection = retrieved_set & ground_truth_set

    # å‡†ç¡®ç‡ = æ£€ç´¢ç»“æœä¸­æ­£ç¡®çš„æ•°é‡ / æ£€ç´¢ç»“æœæ€»æ•°
    precision = len(intersection) / len(retrieved_ids) if retrieved_ids else 0.0

    # å¬å›ç‡ = æ£€ç´¢ç»“æœä¸­æ­£ç¡®çš„æ•°é‡ / çœŸå®ç»“æœæ€»æ•°
    recall = len(intersection) / len(ground_truth_ids) if ground_truth_ids else 0.0

    return precision, recall


def calculate_time_statistics(times: List[float]) -> Dict[str, float]:
    """è®¡ç®—æ—¶é—´ç»Ÿè®¡ä¿¡æ¯"""
    if not times:
        return {}

    times_array = np.array(times)
    return {
        'min': float(np.min(times_array)),
        'max': float(np.max(times_array)),
        'mean': float(np.mean(times_array)),
        'median': float(np.median(times_array)),
        'p25': float(np.percentile(times_array, 25)),
        'p75': float(np.percentile(times_array, 75)),
        'p95': float(np.percentile(times_array, 95)),
        'p99': float(np.percentile(times_array, 99))
    }


# ==================== ä¸»è¯„æµ‹æµç¨‹ ====================

def load_ground_truth(ground_truth_file: str) -> Optional[List[List[str]]]:
    """åŠ è½½é¢„è®¡ç®—çš„åŸºå‡†ç»“æœ"""
    if not os.path.exists(ground_truth_file):
        return None

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸºå‡†ç»“æœ: {ground_truth_file}")
    with open(ground_truth_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    ground_truth = data.get('ground_truth', [])
    if ground_truth:
        print(f"âœ… å·²åŠ è½½ {len(ground_truth)} ä¸ªqueryçš„åŸºå‡†ç»“æœ")
        return ground_truth

    return None


def run_benchmark(
    vector_file: str,
    top_n: int = 10,
    collection_name: Optional[str] = None,
    ground_truth_file: Optional[str] = None
):
    """
    è¿è¡ŒMilvusè¯„æµ‹

    Args:
        vector_file: å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„
        top_n: top-Næ£€ç´¢æ•°é‡
        collection_name: collectionåç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼‰
    """
    print("=" * 80)
    print("ğŸš€ Milvuså‘é‡æ•°æ®åº“è¯„æµ‹")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    vector_data, model_name = load_vector_data(vector_file)
    results = vector_data.get('results', [])
    metadata = vector_data.get('metadata', {})

    # å°è¯•è·å–åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    original_file = metadata.get('input_file', '')
    queries, documents = extract_query_document_vectors(results, original_file)

    if not queries:
        print("âŒ æœªæ‰¾åˆ°queryå‘é‡ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")
        return

    if not documents:
        print("âŒ æœªæ‰¾åˆ°documentå‘é‡ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")
        return

    # å»é‡documentsï¼ˆåŸºäºhashï¼‰
    unique_docs = {}
    for doc in documents:
        doc_hash = doc['hash']
        if doc_hash not in unique_docs:
            unique_docs[doc_hash] = doc

    documents = list(unique_docs.values())
    print(f"ğŸ“Š å»é‡åDocumentæ•°é‡: {len(documents)}")

    # 2. è¿æ¥Milvus
    print("\n" + "=" * 80)
    print("ğŸ”Œ è¿æ¥Milvus...")
    try:
        get_milvus_client()
        print("âœ… Milvusè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Milvusè¿æ¥å¤±è´¥: {e}")
        return

    # 3. åˆ›å»ºcollection
    print("\n" + "=" * 80)
    print("ğŸ“¦ åˆ›å»ºCollection...")
    if collection_name is None:
        # æ ¹æ®æ¨¡å‹åç§°ç”Ÿæˆcollectionåç§°
        model_safe = model_name.replace('@', '_').replace('/', '_').replace('-', '_')
        collection_name = f"benchmark_{model_safe}"

    # è·å–å‘é‡ç»´åº¦
    dimension = len(documents[0]['vector'])
    collection = create_collection(collection_name, dimension)

    # 4. æ’å…¥æ‰€æœ‰documentå‘é‡
    print("\n" + "=" * 80)
    print("ğŸ“¥ æ’å…¥Documentå‘é‡...")
    insert_documents(collection, documents)

    # 5. è®¡ç®—æˆ–åŠ è½½åŸºå‡†ï¼ˆæš´åŠ›æ³•ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ” è®¡ç®—åŸºå‡†ï¼ˆæš´åŠ›æ³•ï¼‰...")

    ground_truth = None

    # å°è¯•åŠ è½½é¢„è®¡ç®—çš„åŸºå‡†
    if ground_truth_file:
        ground_truth = load_ground_truth(ground_truth_file)

    # å¦‚æœæ²¡æœ‰æä¾›åŸºå‡†æ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
    if ground_truth is None:
        possible_benchmark_file = vector_file.replace('.json', '_brute_force_benchmark.json')
        if os.path.exists(possible_benchmark_file):
            print(f"ğŸ“– å‘ç°åŸºå‡†æ–‡ä»¶: {possible_benchmark_file}")
            ground_truth = load_ground_truth(possible_benchmark_file)

    # å¦‚æœä»ç„¶æ²¡æœ‰åŸºå‡†ï¼Œåˆ™è®¡ç®—
    if ground_truth is None:
        print(f"   æ­£åœ¨ä¸º {len(queries)} ä¸ªqueryè®¡ç®—top-{top_n}åŸºå‡†...")

        doc_vectors = [doc['vector'] for doc in documents]
        doc_ids = [doc['hash'] for doc in documents]

        ground_truth = []
        for i, query in enumerate(queries):
            if (i + 1) % 100 == 0:
                print(f"   è¿›åº¦: {i + 1}/{len(queries)}", end='\r')

            query_vec = query['vector']
            top_n_results = brute_force_top_n(query_vec, doc_vectors, doc_ids, top_n)
            ground_truth.append([doc_id for doc_id, _ in top_n_results])

        print(f"\nâœ… åŸºå‡†è®¡ç®—å®Œæˆ")
    else:
        # éªŒè¯åŸºå‡†æ•°é‡æ˜¯å¦åŒ¹é…
        if len(ground_truth) != len(queries):
            print(f"âš ï¸  è­¦å‘Š: åŸºå‡†ç»“æœæ•°é‡ ({len(ground_truth)}) ä¸queryæ•°é‡ ({len(queries)}) ä¸åŒ¹é…")
            print(f"   å°†ä½¿ç”¨å‰ {min(len(ground_truth), len(queries))} ä¸ªç»“æœ")
            ground_truth = ground_truth[:len(queries)]

    # 6. æ‰§è¡Œæ£€ç´¢è¯„æµ‹
    print("\n" + "=" * 80)
    print("ğŸ” æ‰§è¡Œæ£€ç´¢è¯„æµ‹...")

    precisions = []
    recalls = []
    search_times = []

    # æ‰¹é‡æ£€ç´¢ä»¥æé«˜æ•ˆç‡
    batch_size = 10
    for i in range(0, len(queries), batch_size):
        batch_queries = queries[i:i + batch_size]
        batch_query_vectors = [q['vector'] for q in batch_queries]
        batch_ground_truth = ground_truth[i:i + batch_size]

        # æ‰§è¡Œæœç´¢
        search_start = time.time()
        batch_results = search_vectors(collection, batch_query_vectors, top_n)
        search_time = time.time() - search_start

        # å¤„ç†æ¯ä¸ªqueryçš„ç»“æœ
        for j, (result, gt_ids) in enumerate(zip(batch_results, batch_ground_truth)):
            retrieved_ids = [hit['id'] for hit in result]

            # è®¡ç®—æŒ‡æ ‡
            precision, recall = calculate_metrics(retrieved_ids, gt_ids)
            precisions.append(precision)
            recalls.append(recall)

        # å¹³å‡æ¯ä¸ªqueryçš„æœç´¢æ—¶é—´
        avg_time_per_query = search_time / len(batch_queries)
        search_times.extend([avg_time_per_query] * len(batch_queries))

        if (i + batch_size) % 100 == 0 or i + batch_size >= len(queries):
            print(f"   è¿›åº¦: {min(i + batch_size, len(queries))}/{len(queries)}", end='\r')

    print(f"\nâœ… æ£€ç´¢è¯„æµ‹å®Œæˆ")

    # 7. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„æµ‹ç»“æœ")
    print("=" * 80)

    avg_precision = np.mean(precisions) if precisions else 0.0
    avg_recall = np.mean(recalls) if recalls else 0.0
    time_stats = calculate_time_statistics(search_times)

    print(f"\nğŸ” æ£€ç´¢èƒ½åŠ›:")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {avg_precision:.4f}")
    print(f"   å¹³å‡å¬å›ç‡: {avg_recall:.4f}")

    print(f"\nâ±ï¸  å¤„ç†èƒ½åŠ›ï¼ˆæ¯ä¸ªè¯·æ±‚çš„æ—¶é—´ï¼Œå•ä½ï¼šç§’ï¼‰:")
    if time_stats:
        print(f"   æœ€å°å€¼: {time_stats['min']:.6f}")
        print(f"   æœ€å¤§å€¼: {time_stats['max']:.6f}")
        print(f"   å¹³å‡å€¼: {time_stats['mean']:.6f}")
        print(f"   ä¸­ä½æ•°: {time_stats['median']:.6f}")
        print(f"   P25: {time_stats['p25']:.6f}")
        print(f"   P75: {time_stats['p75']:.6f}")
        print(f"   P95: {time_stats['p95']:.6f}")
        print(f"   P99: {time_stats['p99']:.6f}")

    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")
    print(f"   Top-N: {top_n}")
    print(f"   Collection: {collection_name}")

    # ä¿å­˜ç»“æœ
    results = {
        'vector_file': vector_file,
        'model': model_name,
        'collection_name': collection_name,
        'query_count': len(queries),
        'document_count': len(documents),
        'top_n': top_n,
        'metrics': {
            'average_precision': float(avg_precision),
            'average_recall': float(avg_recall),
            'precision_list': [float(p) for p in precisions],
            'recall_list': [float(r) for r in recalls]
        },
        'time_statistics': time_stats,
        'search_times': [float(t) for t in search_times]
    }

    output_file = vector_file.replace('.json', '_milvus_benchmark_results.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Milvuså‘é‡æ•°æ®åº“è¯„æµ‹è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨ï¼ˆä¼šè‡ªåŠ¨è®¡ç®—åŸºå‡†ï¼‰
  python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_openai_vectors.json

  # ä½¿ç”¨é¢„è®¡ç®—çš„åŸºå‡†æ–‡ä»¶ï¼ˆæ¨èï¼šå…ˆè¿è¡Œbrute_force_benchmark.pyç”ŸæˆåŸºå‡†ï¼‰
  python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_openai_vectors.json -g benchmark_results.json

  # æŒ‡å®štop-N
  python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_gemini_vec.json -n 20

  # æŒ‡å®šcollectionåç§°
  python vector/vector_db/milvus_benchmark.py -i .data/vectors/nfcorpus_gemini_multimodal_vec.json -c my_collection
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰'
    )

    parser.add_argument(
        '-n', '--top-n',
        type=int,
        default=10,
        help='Top-Næ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )

    parser.add_argument(
        '-c', '--collection',
        type=str,
        default=None,
        help='Collectionåç§°ï¼ˆé»˜è®¤: æ ¹æ®æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼‰'
    )

    parser.add_argument(
        '-g', '--ground-truth',
        type=str,
        default=None,
        help='é¢„è®¡ç®—çš„åŸºå‡†ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†è·³è¿‡åŸºå‡†è®¡ç®—ï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    try:
        run_benchmark(
            vector_file=args.input,
            top_n=args.top_n,
            collection_name=args.collection,
            ground_truth_file=args.ground_truth
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

