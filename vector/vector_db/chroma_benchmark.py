"""
Chromaå‘é‡æ•°æ®åº“è¯„æµ‹è„šæœ¬

æ ¹æ®spec.mdçš„è¦æ±‚ï¼Œå¯¹Chromaè¿›è¡Œå‘é‡æ£€ç´¢è¯„æµ‹ï¼š
- åŸºäºå‘é‡åŒ–è¯„æµ‹ç”Ÿæˆçš„q-då‘é‡
- æ‰€æœ‰documentå‘é‡å…¨éƒ¨å…¥åº“åå†å¼€å§‹è¯„æµ‹
- è¯„æµ‹åŸºå‡†ï¼šåŸºäºæš´åŠ›æ³•ç®—å‡ºæ¥çš„top-Næœ€è¿‘é‚»å‘é‡
- è¯„æµ‹æŒ‡æ ‡ï¼š
  - æ£€ç´¢èƒ½åŠ›ï¼šæ¯ä¸ªè¯·æ±‚åˆ†åˆ«è®¡ç®—å‡†ç¡®ç‡å’Œå¬å›ç‡ï¼Œæœ€åè®¡ç®—æ‰€æœ‰è¯·æ±‚çš„å¹³å‡å€¼
  - å¤„ç†èƒ½åŠ›ï¼šè®°å½•æ¯ä¸ªè¯·æ±‚çš„æ—¶é—´ï¼Œå¹¶æŠ¥å‘Šæœ€ç»ˆçš„æ—¶é—´åˆ†å¸ƒï¼ˆæœ€å¤§æœ€å°ã€å¹³å‡ã€åˆ†ä½æ•°ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_openai_vectors.json
    python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json -n 10
"""
import os
import json
import time
import argparse
import hashlib
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from chromadb import CloudClient


# ==================== å·¥å…·å‡½æ•° ====================

def get_chroma_client():
    """è·å–Chromaå®¢æˆ·ç«¯è¿æ¥ï¼ˆCloudæ¨¡å¼ï¼‰"""
    api_key = os.getenv("CHROMA_API_KEY")
    tenant = os.getenv("CHROMA_TENANT")
    database = os.getenv("CHROMA_DATABASE")

    if not api_key:
        raise ValueError("è¯·è®¾ç½® CHROMA_API_KEY ç¯å¢ƒå˜é‡")
    if not tenant:
        raise ValueError("è¯·è®¾ç½® CHROMA_TENANT ç¯å¢ƒå˜é‡")
    if not database:
        raise ValueError("è¯·è®¾ç½® CHROMA_DATABASE ç¯å¢ƒå˜é‡")

    # åˆ›å»ºChroma Cloudå®¢æˆ·ç«¯
    # æ ¹æ®Chroma Cloudå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.trychroma.com/docs/run-chroma/cloud-client
    # ä½¿ç”¨CloudClientåˆ›å»ºå®¢æˆ·ç«¯
    try:
        client = CloudClient(
            tenant=tenant,
            database=database,
            api_key=api_key
        )
        print(f"âœ… Chroma Cloudå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"   Tenant: {tenant}")
        print(f"   Database: {database}")
    except Exception as e:
        error_msg = str(e)
        if "Permission denied" in error_msg or "permission" in error_msg.lower():
            raise ValueError(
                f"Chroma Cloudè¿æ¥æƒé™è¢«æ‹’ç»ã€‚è¯·æ£€æŸ¥ï¼š\n"
                f"  1. APIå¯†é’¥æ˜¯å¦æ­£ç¡® (CHROMA_API_KEY)\n"
                f"  2. Tenantæ˜¯å¦æ­£ç¡® (CHROMA_TENANT={tenant})\n"
                f"  3. Databaseæ˜¯å¦æ­£ç¡® (CHROMA_DATABASE={database})\n"
                f"  4. APIå¯†é’¥æ˜¯å¦æœ‰è®¿é—®è¯¥tenantå’Œdatabaseçš„æƒé™\n"
                f"åŸå§‹é”™è¯¯: {e}"
            )
        else:
            raise ValueError(f"åˆ›å»ºChroma Cloudå®¢æˆ·ç«¯å¤±è´¥: {e}")

    return client


def compute_sha512_hex(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬çš„SHA-512å“ˆå¸Œå€¼ï¼ˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰"""
    return hashlib.sha512(text.encode('utf-8')).hexdigest()


# ==================== æ•°æ®åŠ è½½ ====================

def load_vector_data(vector_file: str) -> Tuple[Dict[str, Any], str]:
    """
    åŠ è½½å‘é‡æ•°æ®æ–‡ä»¶

    Returns:
        (æ•°æ®å­—å…¸, æ¨¡å‹åç§°)
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½å‘é‡æ•°æ®: {vector_file}")
    with open(vector_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # è·å–æ¨¡å‹åç§°
    model_name = data.get('metadata', {}).get('model', 'unknown')

    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if 'query_vectors' in data and 'document_vectors' in data:
        # æ–°æ ¼å¼ï¼šåˆ†ç¦»çš„query_vectorså’Œdocument_vectors
        query_count = len(data.get('query_vectors', []))
        doc_count = len(data.get('document_vectors', []))
        print(f"âœ… å·²åŠ è½½æ•°æ®ï¼ˆæ–°æ ¼å¼ï¼‰")
        print(f"   Queryå‘é‡æ•°é‡: {query_count}")
        print(f"   Documentå‘é‡æ•°é‡: {doc_count}")
        print(f"   æ¨¡å‹: {model_name}")
    elif 'results' in data:
        # æ—§æ ¼å¼ï¼šresultsåˆ—è¡¨
        results = data.get('results', [])
        print(f"âœ… å·²åŠ è½½ {len(results)} æ¡æ•°æ®ï¼ˆæ—§æ ¼å¼ï¼‰")
        print(f"   æ¨¡å‹: {model_name}")
    else:
        print(f"âš ï¸  æœªçŸ¥çš„æ•°æ®æ ¼å¼")

    return data, model_name


def load_original_data(original_file: str):
    """åŠ è½½åŸå§‹QAæ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒæ–°æ ¼å¼å’Œæ—§æ ¼å¼ï¼‰"""
    if not os.path.exists(original_file):
        return None

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: {original_file}")
    with open(original_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if isinstance(data, dict):
        # æ–°æ ¼å¼
        print(f"âœ… å·²åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ–°æ ¼å¼ï¼‰")
    elif isinstance(data, list):
        # æ—§æ ¼å¼
        print(f"âœ… å·²åŠ è½½ {len(data)} æ¡åŸå§‹æ•°æ®ï¼ˆæ—§æ ¼å¼ï¼‰")
    else:
        print(f"âš ï¸  åŸå§‹æ•°æ®æ ¼å¼æœªçŸ¥")

    return data


def extract_query_document_vectors_new_format(
    vector_data: Dict[str, Any],
    original_data_file: str
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ä»æ–°æ ¼å¼çš„å‘é‡æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡

    Args:
        vector_data: å‘é‡æ•°æ®å­—å…¸ï¼ˆåŒ…å«query_vectorså’Œdocument_vectorsï¼‰
        original_data_file: åŸå§‹QAæ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
        (queryåˆ—è¡¨, documentåˆ—è¡¨)
    """
    queries = []
    documents = []

    query_vectors = vector_data.get('query_vectors', [])
    document_vectors = vector_data.get('document_vectors', [])

    # åŠ è½½åŸå§‹æ•°æ®
    original_data = load_original_data(original_data_file)
    if not original_data:
        print("âŒ æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶")
        return queries, documents

    # æ£€æŸ¥åŸå§‹æ•°æ®æ ¼å¼
    if isinstance(original_data, dict):
        # æ–°æ ¼å¼ï¼šåŒ…å«query_listå’Œdocument_list
        if 'query_list' in original_data and 'document_list' in original_data:
            query_list = original_data['query_list']
            document_list = original_data['document_list']
        else:
            print("âŒ åŸå§‹æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼ˆæ–°æ ¼å¼åº”åŒ…å«query_listå’Œdocument_listï¼‰")
            return queries, documents
    elif isinstance(original_data, list):
        # æ—§æ ¼å¼ï¼šåˆ—è¡¨ï¼Œæ¯ä¸ªitemåŒ…å«queryå’Œdocument
        # æå–å”¯ä¸€çš„queryå’Œdocumentåˆ—è¡¨ï¼ˆæŒ‰é¦–æ¬¡å‡ºç°çš„é¡ºåºï¼‰
        query_list = []
        document_list = []
        query_seen = set()
        doc_seen = set()

        for item in original_data:
            query_text = item.get('query', '')
            doc_text = item.get('document', '')

            if query_text and query_text not in query_seen:
                query_list.append(query_text)
                query_seen.add(query_text)

            if doc_text:
                doc_hash = compute_sha512_hex(doc_text)
                if doc_hash not in doc_seen:
                    document_list.append(doc_text)
                    doc_seen.add(doc_hash)
    else:
        print("âŒ åŸå§‹æ•°æ®æ ¼å¼æœªçŸ¥")
        return queries, documents

    # åŒ¹é…å‘é‡å’Œæ–‡æœ¬
    print(f"ğŸ“Š åŒ¹é…å‘é‡å’Œæ–‡æœ¬...")
    print(f"   å”¯ä¸€Query: {len(query_list)}")
    print(f"   å”¯ä¸€Document: {len(document_list)}")
    print(f"   Queryå‘é‡: {len(query_vectors)}")
    print(f"   Documentå‘é‡: {len(document_vectors)}")

    # éªŒè¯æ•°é‡
    if len(query_vectors) != len(query_list):
        print(f"âš ï¸  è­¦å‘Š: Queryå‘é‡æ•°é‡ ({len(query_vectors)}) ä¸å”¯ä¸€Queryæ•°é‡ ({len(query_list)}) ä¸åŒ¹é…")

    if len(document_vectors) != len(document_list):
        print(f"âš ï¸  è­¦å‘Š: Documentå‘é‡æ•°é‡ ({len(document_vectors)}) ä¸å”¯ä¸€Documentæ•°é‡ ({len(document_list)}) ä¸åŒ¹é…")

    # åŒ¹é…queryå‘é‡ï¼ˆæŒ‰é¡ºåºï¼‰
    for i, query_text in enumerate(query_list):
        if i < len(query_vectors):
            queries.append({
                'query': query_text,
                'vector': query_vectors[i],
                'document': '',  # æ–°æ ¼å¼ä¸åŒ…å«documentå…³è”
                'score': None
            })

    # åŒ¹é…documentå‘é‡ï¼ˆæŒ‰é¡ºåºï¼‰
    for i, doc_text in enumerate(document_list):
        if i < len(document_vectors):
            doc_hash = compute_sha512_hex(doc_text)
            documents.append({
                'document': doc_text,
                'vector': document_vectors[i],
                'hash': doc_hash
            })

    return queries, documents


def extract_query_document_vectors(
    data: List[Dict[str, Any]],
    original_data_file: Optional[str] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ä»æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡

    Args:
        data: å‘é‡æ•°æ®åˆ—è¡¨
        original_data_file: åŸå§‹QAæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåŒ¹é…queryå’Œdocumentï¼‰

    Returns:
        (queryåˆ—è¡¨, documentåˆ—è¡¨)
    """
    queries = []
    documents = []

    # é¦–å…ˆå°è¯•ç›´æ¥æå–query_vectorå’Œdocument_vector
    for item in data:
        if 'query_vector' in item and item['query_vector'] is not None:
            queries.append({
                'query': item.get('query', ''),
                'vector': item['query_vector'],
                'document': item.get('document', ''),
                'score': item.get('score')
            })

        if 'document_vector' in item and item['document_vector'] is not None:
            doc_text = item.get('document', '')
            doc_hash = compute_sha512_hex(doc_text)
            documents.append({
                'document': doc_text,
                'vector': item['document_vector'],
                'hash': doc_hash
            })

    # å¦‚æœæ•°æ®æ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚åªæœ‰text_embeddingï¼‰ï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…
    if not queries and not documents:
        print("âš ï¸  æœªæ‰¾åˆ°query_vector/document_vectorå­—æ®µï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…...")

        # åŠ è½½åŸå§‹æ•°æ®
        original_data = []
        if original_data_file:
            if os.path.exists(original_data_file):
                original_data = load_original_data(original_data_file)
            else:
                # å°è¯•ç›¸å¯¹è·¯å¾„
                possible_paths = [
                    original_data_file,
                    os.path.join('.data', 'mteb', os.path.basename(original_data_file)),
                    os.path.join('.data/mteb', os.path.basename(original_data_file))
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        original_data = load_original_data(path)
                        break

        if not original_data:
            print("âŒ æ— æ³•æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œæ— æ³•åŒ¹é…queryå’Œdocument")
            return queries, documents

        # æ„å»ºæ–‡æœ¬åˆ°å‘é‡çš„æ˜ å°„
        text_to_vector = {}
        for item in data:
            text = item.get('text', '')
            vector = item.get('text_embedding') or item.get('vector')
            if text and vector:
                text_to_vector[text] = vector

        # ä»åŸå§‹æ•°æ®ä¸­åŒ¹é…queryå’Œdocument
        query_texts_seen = set()
        doc_texts_seen = set()

        for orig_item in original_data:
            query_text = orig_item.get('query', '')
            doc_text = orig_item.get('document', '')

            # åŒ¹é…queryå‘é‡
            if query_text and query_text in text_to_vector and query_text not in query_texts_seen:
                queries.append({
                    'query': query_text,
                    'vector': text_to_vector[query_text],
                    'document': doc_text,
                    'score': orig_item.get('score')
                })
                query_texts_seen.add(query_text)

            # åŒ¹é…documentå‘é‡
            if doc_text and doc_text in text_to_vector and doc_text not in doc_texts_seen:
                doc_hash = compute_sha512_hex(doc_text)
                documents.append({
                    'document': doc_text,
                    'vector': text_to_vector[doc_text],
                    'hash': doc_hash
                })
                doc_texts_seen.add(doc_text)

    print(f"ğŸ“Š æå–ç»“æœ:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")

    # æ£€æŸ¥æ•°é‡æ˜¯å¦åˆç†ï¼ˆé€šå¸¸queryæ•°é‡åº”è¯¥å°‘äºdocumentæ•°é‡ï¼‰
    if len(queries) > 0 and len(documents) > 0:
        if len(queries) > len(documents) * 2:
            print(f"âš ï¸  è­¦å‘Š: Queryæ•°é‡ ({len(queries)}) æ˜æ˜¾å¤§äºDocumentæ•°é‡ ({len(documents)})")
            print(f"   è¿™å¯èƒ½æ˜¯å¼‚å¸¸çš„ï¼Œè¯·æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®")
        elif len(documents) > len(queries) * 10:
            print(f"â„¹ï¸  ä¿¡æ¯: Documentæ•°é‡ ({len(documents)}) è¿œå¤§äºQueryæ•°é‡ ({len(queries)})ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
    elif len(queries) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•queryå‘é‡")
    elif len(documents) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•documentå‘é‡")
        print(f"   è¿™å¯èƒ½æ˜¯å› ä¸ºå‘é‡æ•°æ®æ–‡ä»¶ä¸å®Œæ•´ï¼ŒåªåŒ…å«äº†queryå‘é‡è€Œæ²¡æœ‰documentå‘é‡")

    return queries, documents


# ==================== Chromaæ“ä½œ ====================

def create_collection(client, collection_name: str, dimension: int, force_recreate: bool = False):
    """
    åˆ›å»ºæˆ–è·å–Chroma collection

    Args:
        client: Chromaå®¢æˆ·ç«¯
        collection_name: collectionåç§°
        dimension: å‘é‡ç»´åº¦ï¼ˆChromaä¼šè‡ªåŠ¨æ¨æ–­ï¼Œä½†æˆ‘ä»¬å¯ä»¥éªŒè¯ï¼‰
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼ˆåˆ é™¤å·²å­˜åœ¨çš„collectionï¼‰

    Returns:
        (Collectionå¯¹è±¡, æ˜¯å¦æ˜¯æ–°åˆ›å»ºçš„)
    """
    # æ£€æŸ¥collectionæ˜¯å¦å·²å­˜åœ¨
    try:
        existing_collection = client.get_collection(collection_name)
        if force_recreate:
            print(f"âš ï¸  Collection '{collection_name}' å·²å­˜åœ¨ï¼Œå¼ºåˆ¶åˆ é™¤æ—§collection...")
            client.delete_collection(collection_name)
        else:
            print(f"âœ… Collection '{collection_name}' å·²å­˜åœ¨ï¼Œå¤ç”¨ç°æœ‰collection")
            # éªŒè¯ç»´åº¦
            metadata = existing_collection.metadata or {}
            existing_dim = metadata.get('dimension')
            if existing_dim and existing_dim != dimension:
                print(f"âš ï¸  è­¦å‘Š: ç°æœ‰collectionçš„ç»´åº¦ ({existing_dim}) ä¸é¢„æœŸç»´åº¦ ({dimension}) ä¸åŒ¹é…")
            return existing_collection, False
    except Exception:
        # Collectionä¸å­˜åœ¨ï¼Œç»§ç»­åˆ›å»º
        pass

    # åˆ›å»ºæ–°collection
    # Chromaä¼šè‡ªåŠ¨æ¨æ–­å‘é‡ç»´åº¦ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨metadataä¸­å­˜å‚¨
    collection = client.create_collection(
        name=collection_name,
        metadata={"dimension": dimension}
    )

    print(f"âœ… å·²åˆ›å»ºcollection: {collection_name}")
    print(f"   ç»´åº¦: {dimension}")

    return collection, True


def check_collection_data(collection, expected_count: int) -> bool:
    """
    æ£€æŸ¥collectionä¸­çš„æ•°æ®é‡æ˜¯å¦åŒ¹é…é¢„æœŸ

    Args:
        collection: Chroma collectionå¯¹è±¡
        expected_count: é¢„æœŸçš„æ•°æ®é‡

    Returns:
        æ˜¯å¦åŒ¹é…
    """
    try:
        # è·å–collectionçš„count
        count = collection.count()
        return count == expected_count
    except Exception as e:
        print(f"   âš ï¸  æ£€æŸ¥collectionæ•°æ®é‡æ—¶å‡ºé”™: {e}")
        return False


def insert_documents(collection, documents: List[Dict[str, Any]], batch_size: int = 100, skip_if_exists: bool = True):
    """
    æ’å…¥documentå‘é‡åˆ°Chroma

    Args:
        collection: Chroma collectionå¯¹è±¡
        documents: documentåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«hashå’Œvector
        batch_size: æ‰¹é‡æ’å…¥å¤§å°
        skip_if_exists: å¦‚æœcollectionä¸­å·²æœ‰æ•°æ®ï¼Œæ˜¯å¦è·³è¿‡æ’å…¥
    """
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    if skip_if_exists:
        try:
            current_count = collection.count()
            expected_count = len(documents)
            print(f"ğŸ“Š Collectionå½“å‰æ•°æ®é‡: {current_count}ï¼Œé¢„æœŸæ’å…¥: {expected_count}")

            if current_count >= expected_count:
                print(f"âœ… Collectionä¸­å·²æœ‰ {current_count} æ¡æ•°æ®ï¼ˆé¢„æœŸ {expected_count} æ¡ï¼‰ï¼Œè·³è¿‡æ’å…¥")
                return
            elif current_count > 0:
                print(f"âš ï¸  Collectionä¸­å·²æœ‰ {current_count} æ¡æ•°æ®ï¼Œä½†é¢„æœŸ {expected_count} æ¡")
                print(f"   æ£€æŸ¥å“ªäº›æ•°æ®å·²å­˜åœ¨...")

                # åˆ†æ‰¹æ£€æŸ¥å·²å­˜åœ¨çš„IDï¼Œé¿å…ä¸€æ¬¡æ€§æŸ¥è¯¢å¤ªå¤š
                all_ids = [doc['hash'] for doc in documents]
                existing_ids = set()
                check_batch_size = 100  # æ¯æ¬¡æ£€æŸ¥100ä¸ªID

                for i in range(0, len(all_ids), check_batch_size):
                    batch_ids = all_ids[i:i + check_batch_size]
                    try:
                        existing_results = collection.get(ids=batch_ids)
                        batch_existing = existing_results.get('ids', [])
                        existing_ids.update(batch_existing)
                    except Exception as e:
                        # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼šå‡è®¾è¿™äº›IDå·²å­˜åœ¨ï¼Œé¿å…é‡å¤æ’å…¥
                        print(f"   âš ï¸  æ£€æŸ¥æ‰¹æ¬¡ {i//check_batch_size + 1} æ—¶å‡ºé”™: {e}")
                        print(f"   ä¿å®ˆå¤„ç†ï¼šå‡è®¾è¿™äº›IDå·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                        existing_ids.update(batch_ids)

                # è¿‡æ»¤å‡ºéœ€è¦æ’å…¥çš„æ•°æ®
                documents = [doc for doc in documents if doc['hash'] not in existing_ids]

                if not documents:
                    print(f"âœ… æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                    return

                print(f"   å®é™…éœ€è¦æ’å…¥ {len(documents)} æ¡æ–°æ•°æ®ï¼ˆå·²å­˜åœ¨ {len(existing_ids)} æ¡ï¼‰")
        except Exception as e:
            print(f"   âš ï¸  æ£€æŸ¥collectionæ•°æ®é‡æ—¶å‡ºé”™: {e}")
            print(f"   âš ï¸  ä¸ºé¿å…é…é¢è¶…é™ï¼Œå°†è·³è¿‡æ’å…¥ã€‚å¦‚éœ€å¼ºåˆ¶æ’å…¥ï¼Œè¯·ä½¿ç”¨ --force-recreate")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œä¸æ’å…¥æ•°æ®
            raise ValueError(
                f"æ— æ³•æ£€æŸ¥collectionæ•°æ®çŠ¶æ€ï¼Œä¸ºé¿å…é…é¢è¶…é™å·²è·³è¿‡æ’å…¥ã€‚\n"
                f"å¦‚æœç¡®å®šéœ€è¦æ’å…¥ï¼Œè¯·ä½¿ç”¨ --force-recreate å‚æ•°å¼ºåˆ¶é‡å»ºcollectionã€‚\n"
                f"åŸå§‹é”™è¯¯: {e}"
            )

    print(f"ğŸ“¥ å¼€å§‹æ’å…¥ {len(documents)} ä¸ªdocumentå‘é‡...")

    ids = [doc['hash'] for doc in documents]
    embeddings = [doc['vector'] for doc in documents]
    # Chromaéœ€è¦metadatasï¼Œæˆ‘ä»¬å¯ä»¥å­˜å‚¨documentæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
    metadatas = [{"text": doc['document'][:1000]} for doc in documents]  # é™åˆ¶é•¿åº¦

    # æ‰¹é‡æ’å…¥
    total_batches = (len(documents) + batch_size - 1) // batch_size
    inserted_count = 0

    for i in range(0, len(documents), batch_size):
        batch_ids = ids[i:i + batch_size]
        batch_embeddings = embeddings[i:i + batch_size]
        batch_metadatas = metadatas[i:i + batch_size]

        batch_num = i // batch_size + 1
        print(f"   æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_ids)} æ¡)...", end='\r')

        try:
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings,
                metadatas=batch_metadatas
            )
            inserted_count += len(batch_ids)
        except Exception as e:
            error_msg = str(e)
            if "Quota exceeded" in error_msg or "quota" in error_msg.lower():
                print(f"\nâŒ Chroma Cloudé…é¢è¶…é™é”™è¯¯:")
                print(f"   {error_msg}")
                print(f"\nğŸ“Š æ’å…¥è¿›åº¦: å·²æˆåŠŸæ’å…¥ {inserted_count}/{len(documents)} æ¡æ•°æ®")
                print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print(f"   1. å½“å‰collectionå¯èƒ½å·²æœ‰æ•°æ®ï¼Œè¯·æ£€æŸ¥å¹¶è€ƒè™‘ä½¿ç”¨ --force-recreate å¼ºåˆ¶é‡å»º")
                print(f"   2. è”ç³»Chroma Cloudç”³è¯·å¢åŠ é…é¢ï¼ˆé”™è¯¯ä¿¡æ¯ä¸­åŒ…å«ç”³è¯·é“¾æ¥ï¼‰")
                print(f"   3. ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•")
                print(f"   4. å¦‚æœæ•°æ®å·²éƒ¨åˆ†æ’å…¥ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œè„šæœ¬ï¼Œè„šæœ¬ä¼šè·³è¿‡å·²å­˜åœ¨çš„æ•°æ®")
                raise ValueError(f"Chroma Cloudé…é¢è¶…é™: {error_msg}")
            else:
                # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
                raise

    print(f"\nâœ… å·²æ’å…¥ {inserted_count} ä¸ªdocumentå‘é‡")


def search_vectors(
    collection,
    query_vectors: List[List[float]],
    top_k: int
) -> List[List[Dict[str, Any]]]:
    """
    åœ¨Chromaä¸­æœç´¢å‘é‡

    Args:
        collection: Chroma collectionå¯¹è±¡
        query_vectors: æŸ¥è¯¢å‘é‡åˆ—è¡¨
        top_k: è¿”å›top-kç»“æœ

    Returns:
        æ¯ä¸ªqueryçš„æœç´¢ç»“æœåˆ—è¡¨
    """
    # Chromaæ”¯æŒæ‰¹é‡æŸ¥è¯¢
    results = collection.query(
        query_embeddings=query_vectors,
        n_results=top_k
    )

    # è½¬æ¢ç»“æœæ ¼å¼
    formatted_results = []
    # resultsçš„ç»“æ„: {'ids': [[id1, id2, ...], ...], 'distances': [[dist1, dist2, ...], ...], ...}
    num_queries = len(query_vectors)
    for i in range(num_queries):
        hits = []
        query_ids = results['ids'][i] if i < len(results['ids']) else []
        query_distances = results['distances'][i] if i < len(results['distances']) else []

        for doc_id, distance in zip(query_ids, query_distances):
            hits.append({
                'id': doc_id,
                'distance': float(distance),
                'score': float(distance)  # Chromaä½¿ç”¨è·ç¦»ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            })
        formatted_results.append(hits)

    return formatted_results


# ==================== è¯„æµ‹æŒ‡æ ‡ ====================

def calculate_metrics(
    retrieved_ids: List[str],
    ground_truth_ids: List[str]
) -> Tuple[float, float]:
    """
    è®¡ç®—å‡†ç¡®ç‡å’Œå¬å›ç‡

    Args:
        retrieved_ids: æ£€ç´¢åˆ°çš„IDåˆ—è¡¨
        ground_truth_ids: çœŸå®top-Nçš„IDåˆ—è¡¨

    Returns:
        (å‡†ç¡®ç‡, å¬å›ç‡)
    """
    retrieved_set = set(retrieved_ids)
    ground_truth_set = set(ground_truth_ids)

    # äº¤é›†
    intersection = retrieved_set & ground_truth_set

    # å‡†ç¡®ç‡ = æ£€ç´¢ç»“æœä¸­æ­£ç¡®çš„æ•°é‡ / æ£€ç´¢ç»“æœæ€»æ•°
    precision = len(intersection) / len(retrieved_ids) if retrieved_ids else 0.0

    # å¬å›ç‡ = æ£€ç´¢ç»“æœä¸­æ­£ç¡®çš„æ•°é‡ / çœŸå®ç»“æœæ€»æ•°
    recall = len(intersection) / len(ground_truth_ids) if ground_truth_ids else 0.0

    return precision, recall


def calculate_time_statistics(times: List[float]) -> Dict[str, float]:
    """è®¡ç®—æ—¶é—´ç»Ÿè®¡ä¿¡æ¯"""
    if not times:
        return {}

    times_array = np.array(times)
    return {
        'min': float(np.min(times_array)),
        'max': float(np.max(times_array)),
        'mean': float(np.mean(times_array)),
        'median': float(np.median(times_array)),
        'p25': float(np.percentile(times_array, 25)),
        'p75': float(np.percentile(times_array, 75)),
        'p95': float(np.percentile(times_array, 95)),
        'p99': float(np.percentile(times_array, 99))
    }


# ==================== ä¸»è¯„æµ‹æµç¨‹ ====================

def find_benchmark_file(vector_file: str) -> Optional[str]:
    """
    æŸ¥æ‰¾åŸºå‡†æ–‡ä»¶ï¼ˆä¼˜å…ˆä».data/vector_searchç›®å½•ï¼‰

    Args:
        vector_file: å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
        åŸºå‡†æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    # ä»å‘é‡æ–‡ä»¶åç”ŸæˆåŸºå‡†æ–‡ä»¶å
    base_name = os.path.basename(vector_file)
    benchmark_name = base_name.replace('.json', '_brute_force_benchmark.json')

    # ä¼˜å…ˆæŸ¥æ‰¾è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    possible_paths = [
        # 1. .data/vector_searchç›®å½•ï¼ˆä¼˜å…ˆï¼‰
        os.path.join('.data', 'vector_search', benchmark_name),
        # 2. ä¸å‘é‡æ–‡ä»¶åŒç›®å½•
        vector_file.replace('.json', '_brute_force_benchmark.json'),
        # 3. å‘é‡æ–‡ä»¶æ‰€åœ¨ç›®å½•
        os.path.join(os.path.dirname(vector_file), benchmark_name),
    ]

    for path in possible_paths:
        if os.path.exists(path):
            return path

    return None


def load_ground_truth(ground_truth_file: str) -> Optional[List[List[str]]]:
    """åŠ è½½é¢„è®¡ç®—çš„åŸºå‡†ç»“æœ"""
    if not os.path.exists(ground_truth_file):
        return None

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸºå‡†ç»“æœ: {ground_truth_file}")
    with open(ground_truth_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    ground_truth = data.get('ground_truth', [])
    if ground_truth:
        print(f"âœ… å·²åŠ è½½ {len(ground_truth)} ä¸ªqueryçš„åŸºå‡†ç»“æœ")
        # éªŒè¯åŸºå‡†æ–‡ä»¶ä¿¡æ¯
        benchmark_top_n = data.get('top_n', 0)
        benchmark_query_count = data.get('query_count', 0)
        print(f"   åŸºå‡†ä¿¡æ¯: top_n={benchmark_top_n}, query_count={benchmark_query_count}")
        return ground_truth

    return None


def run_benchmark(
    vector_file: str,
    top_n: int = 10,
    collection_name: Optional[str] = None,
    ground_truth_file: Optional[str] = None,
    force_recreate: bool = False
):
    """
    è¿è¡ŒChromaè¯„æµ‹

    Args:
        vector_file: å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„
        top_n: top-Næ£€ç´¢æ•°é‡
        collection_name: collectionåç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼‰
        ground_truth_file: é¢„è®¡ç®—çš„åŸºå‡†ç»“æœæ–‡ä»¶è·¯å¾„
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»ºcollection
    """
    print("=" * 80)
    print("ğŸš€ Chromaå‘é‡æ•°æ®åº“è¯„æµ‹")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    vector_data, model_name = load_vector_data(vector_file)
    metadata = vector_data.get('metadata', {})
    original_file = metadata.get('input_file', '')

    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if 'query_vectors' in vector_data and 'document_vectors' in vector_data:
        # æ–°æ ¼å¼ï¼šåˆ†ç¦»çš„query_vectorså’Œdocument_vectors
        print(f"\næ£€æµ‹åˆ°æ–°æ ¼å¼æ•°æ®ï¼Œä½¿ç”¨æ–°æ ¼å¼è§£æ...")
        if not original_file:
            print("âŒ æ–°æ ¼å¼éœ€è¦åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œä½†metadataä¸­æœªæ‰¾åˆ°input_file")
            return

        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            original_file,
            os.path.join('.data', 'mteb', os.path.basename(original_file)),
            os.path.join('.data/mteb', os.path.basename(original_file))
        ]

        found_original_file = None
        for path in possible_paths:
            if os.path.exists(path):
                found_original_file = path
                break

        if not found_original_file:
            print(f"âŒ æ— æ³•æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {original_file}")
            return

        queries, documents = extract_query_document_vectors_new_format(vector_data, found_original_file)
    else:
        # æ—§æ ¼å¼ï¼šresultsåˆ—è¡¨
        results = vector_data.get('results', [])
        queries, documents = extract_query_document_vectors(results, original_file)

    if not queries:
        print("âŒ æœªæ‰¾åˆ°queryå‘é‡ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")
        return

    if not documents:
        print("âŒ æœªæ‰¾åˆ°documentå‘é‡ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")
        return

    # å»é‡documentsï¼ˆåŸºäºhashï¼‰
    unique_docs = {}
    for doc in documents:
        doc_hash = doc['hash']
        if doc_hash not in unique_docs:
            unique_docs[doc_hash] = doc

    documents = list(unique_docs.values())
    print(f"ğŸ“Š å»é‡åDocumentæ•°é‡: {len(documents)}")

    # 2. è¿æ¥Chroma
    print("\n" + "=" * 80)
    print("ğŸ”Œ è¿æ¥Chroma...")
    try:
        client = get_chroma_client()
        print("âœ… Chromaè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Chromaè¿æ¥å¤±è´¥: {e}")
        return

    # 3. åˆ›å»ºcollection
    print("\n" + "=" * 80)
    print("ğŸ“¦ åˆ›å»ºCollection...")
    if collection_name is None:
        # æ ¹æ®æ¨¡å‹åç§°ç”Ÿæˆcollectionåç§°
        model_safe = model_name.replace('@', '_').replace('/', '_').replace('-', '_')
        collection_name = f"benchmark_{model_safe}"

    # è·å–å‘é‡ç»´åº¦
    dimension = len(documents[0]['vector'])
    collection, is_new_collection = create_collection(client, collection_name, dimension, force_recreate)

    # 4. æ’å…¥æ‰€æœ‰documentå‘é‡ï¼ˆå¦‚æœæ˜¯æ–°collectionæˆ–å¼ºåˆ¶é‡å»ºï¼Œåˆ™æ’å…¥ï¼›å¦åˆ™æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ“¥ æ’å…¥Documentå‘é‡...")
    insert_documents(collection, documents, skip_if_exists=not force_recreate)

    # 5. åŠ è½½åŸºå‡†ï¼ˆä¼˜å…ˆä».data/vector_searchç›®å½•ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ” åŠ è½½åŸºå‡†ç»“æœ...")

    ground_truth = None
    benchmark_file_used = None

    # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åŸºå‡†æ–‡ä»¶
    if ground_truth_file:
        if os.path.exists(ground_truth_file):
            ground_truth = load_ground_truth(ground_truth_file)
            benchmark_file_used = ground_truth_file
        else:
            print(f"âš ï¸  æŒ‡å®šçš„åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {ground_truth_file}")

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨æŸ¥æ‰¾åŸºå‡†æ–‡ä»¶
    if ground_truth is None:
        benchmark_file = find_benchmark_file(vector_file)
        if benchmark_file:
            print(f"ğŸ“– è‡ªåŠ¨å‘ç°åŸºå‡†æ–‡ä»¶: {benchmark_file}")
            ground_truth = load_ground_truth(benchmark_file)
            benchmark_file_used = benchmark_file

    # å¦‚æœä»ç„¶æ²¡æœ‰åŸºå‡†ï¼ŒæŠ¥é”™ï¼ˆä¸å†è‡ªåŠ¨è®¡ç®—ï¼‰
    if ground_truth is None:
        print(f"âŒ æœªæ‰¾åˆ°åŸºå‡†æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")
        print(f"   è¯·å…ˆè¿è¡Œ brute_force_benchmark.py ç”ŸæˆåŸºå‡†æ–‡ä»¶")
        print(f"   æˆ–ä½¿ç”¨ -g å‚æ•°æŒ‡å®šåŸºå‡†æ–‡ä»¶è·¯å¾„")
        print(f"   é¢„æœŸåŸºå‡†æ–‡ä»¶ä½ç½®:")
        print(f"     - .data/vector_search/{os.path.basename(vector_file).replace('.json', '_brute_force_benchmark.json')}")
        print(f"     - {vector_file.replace('.json', '_brute_force_benchmark.json')}")
        return

    # éªŒè¯åŸºå‡†æ–‡ä»¶ä¿¡æ¯
    if benchmark_file_used:
        with open(benchmark_file_used, 'r', encoding='utf-8') as f:
            benchmark_data = json.load(f)
        benchmark_top_n = benchmark_data.get('top_n', 0)
        benchmark_query_count = benchmark_data.get('query_count', 0)

        # éªŒè¯top_næ˜¯å¦åŒ¹é…
        if benchmark_top_n != top_n:
            print(f"âš ï¸  è­¦å‘Š: åŸºå‡†æ–‡ä»¶çš„top_n ({benchmark_top_n}) ä¸æŒ‡å®šçš„top_n ({top_n}) ä¸åŒ¹é…")
            print(f"   å°†ä½¿ç”¨åŸºå‡†æ–‡ä»¶çš„top_n: {benchmark_top_n}")
            top_n = benchmark_top_n

        # éªŒè¯queryæ•°é‡æ˜¯å¦åŒ¹é…
        if len(ground_truth) != len(queries):
            print(f"âš ï¸  è­¦å‘Š: åŸºå‡†ç»“æœæ•°é‡ ({len(ground_truth)}) ä¸queryæ•°é‡ ({len(queries)}) ä¸åŒ¹é…")
            if len(ground_truth) < len(queries):
                print(f"   åŸºå‡†ç»“æœä¸è¶³ï¼Œæ— æ³•å®Œæˆè¯„æµ‹")
                return
            else:
                print(f"   å°†ä½¿ç”¨å‰ {len(queries)} ä¸ªç»“æœ")
                ground_truth = ground_truth[:len(queries)]

    print(f"âœ… åŸºå‡†åŠ è½½å®Œæˆï¼Œä½¿ç”¨æ–‡ä»¶: {benchmark_file_used}")
    print(f"   Top-N: {top_n}, Queryæ•°é‡: {len(queries)}")

    # 6. æ‰§è¡Œæ£€ç´¢è¯„æµ‹
    print("\n" + "=" * 80)
    print("ğŸ” æ‰§è¡Œæ£€ç´¢è¯„æµ‹...")

    precisions = []
    recalls = []
    search_times = []

    # æ‰¹é‡æ£€ç´¢ä»¥æé«˜æ•ˆç‡
    batch_size = 10
    for i in range(0, len(queries), batch_size):
        batch_queries = queries[i:i + batch_size]
        batch_query_vectors = [q['vector'] for q in batch_queries]
        batch_ground_truth = ground_truth[i:i + batch_size]

        # æ‰§è¡Œæœç´¢
        search_start = time.time()
        batch_results = search_vectors(collection, batch_query_vectors, top_n)
        search_time = time.time() - search_start

        # å¤„ç†æ¯ä¸ªqueryçš„ç»“æœ
        for j, (result, gt_ids) in enumerate(zip(batch_results, batch_ground_truth)):
            retrieved_ids = [hit['id'] for hit in result]

            # è®¡ç®—æŒ‡æ ‡
            precision, recall = calculate_metrics(retrieved_ids, gt_ids)
            precisions.append(precision)
            recalls.append(recall)

        # å¹³å‡æ¯ä¸ªqueryçš„æœç´¢æ—¶é—´
        avg_time_per_query = search_time / len(batch_queries)
        search_times.extend([avg_time_per_query] * len(batch_queries))

        if (i + batch_size) % 100 == 0 or i + batch_size >= len(queries):
            print(f"   è¿›åº¦: {min(i + batch_size, len(queries))}/{len(queries)}", end='\r')

    print(f"\nâœ… æ£€ç´¢è¯„æµ‹å®Œæˆ")

    # 7. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„æµ‹ç»“æœ")
    print("=" * 80)

    avg_precision = np.mean(precisions) if precisions else 0.0
    avg_recall = np.mean(recalls) if recalls else 0.0
    time_stats = calculate_time_statistics(search_times)

    print(f"\nğŸ” æ£€ç´¢èƒ½åŠ›:")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {avg_precision:.4f}")
    print(f"   å¹³å‡å¬å›ç‡: {avg_recall:.4f}")

    print(f"\nâ±ï¸  å¤„ç†èƒ½åŠ›ï¼ˆæ¯ä¸ªè¯·æ±‚çš„æ—¶é—´ï¼Œå•ä½ï¼šç§’ï¼‰:")
    if time_stats:
        print(f"   æœ€å°å€¼: {time_stats['min']:.6f}")
        print(f"   æœ€å¤§å€¼: {time_stats['max']:.6f}")
        print(f"   å¹³å‡å€¼: {time_stats['mean']:.6f}")
        print(f"   ä¸­ä½æ•°: {time_stats['median']:.6f}")
        print(f"   P25: {time_stats['p25']:.6f}")
        print(f"   P75: {time_stats['p75']:.6f}")
        print(f"   P95: {time_stats['p95']:.6f}")
        print(f"   P99: {time_stats['p99']:.6f}")

    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")
    print(f"   Top-N: {top_n}")
    print(f"   Collection: {collection_name}")

    # ä¿å­˜ç»“æœ
    results = {
        'vector_file': vector_file,
        'model': model_name,
        'collection_name': collection_name,
        'query_count': len(queries),
        'document_count': len(documents),
        'top_n': top_n,
        'metrics': {
            'average_precision': float(avg_precision),
            'average_recall': float(avg_recall),
            'precision_list': [float(p) for p in precisions],
            'recall_list': [float(r) for r in recalls]
        },
        'time_statistics': time_stats,
        'search_times': [float(t) for t in search_times]
    }

    output_file = vector_file.replace('.json', '_chroma_benchmark_results.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Chromaå‘é‡æ•°æ®åº“è¯„æµ‹è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªåŠ¨ä».data/vector_searchç›®å½•åŠ è½½åŸºå‡†æ–‡ä»¶ï¼‰
  python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_openai_vectors.json

  # æŒ‡å®šåŸºå‡†æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸åœ¨é»˜è®¤ä½ç½®ï¼‰
  python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_openai_vectors.json -g path/to/benchmark.json

  # æŒ‡å®štop-Nï¼ˆä¼šè‡ªåŠ¨ä»åŸºå‡†æ–‡ä»¶è¯»å–ï¼Œå¦‚æœåŸºå‡†æ–‡ä»¶çš„top_nä¸åŒä¼šç»™å‡ºè­¦å‘Šï¼‰
  python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json -n 20

  # æŒ‡å®šcollectionåç§°
  python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_openai_vectors.json -c my_collection

  # å¼ºåˆ¶é‡æ–°åˆ›å»ºcollectionï¼ˆåˆ é™¤å·²å­˜åœ¨çš„å¹¶é‡æ–°æ’å…¥æ•°æ®ï¼‰
  python vector/vector_db/chroma_benchmark.py -i .data/vectors/scidocs_openai_vectors.json --force-recreate

ç¯å¢ƒå˜é‡:
  - CHROMA_API_KEY: Chroma Cloud APIå¯†é’¥
  - CHROMA_TENANT: Chroma Cloudç§Ÿæˆ·åç§°
  - CHROMA_DATABASE: Chroma Cloudæ•°æ®åº“åç§°

æ³¨æ„:
  - åŸºå‡†æ–‡ä»¶ä¼šè‡ªåŠ¨ä».data/vector_searchç›®å½•æŸ¥æ‰¾ï¼ˆä¼˜å…ˆï¼‰
  - å¦‚æœæ‰¾ä¸åˆ°åŸºå‡†æ–‡ä»¶ï¼Œè¯„æµ‹ä¼šå¤±è´¥å¹¶æç¤ºéœ€è¦å…ˆè¿è¡Œbrute_force_benchmark.py
  - åŸºå‡†æ–‡ä»¶åº”åŒ…å«ground_truthå­—æ®µï¼ˆqueryçš„top-Nç»“æœåˆ—è¡¨ï¼‰
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰'
    )

    parser.add_argument(
        '-n', '--top-n',
        type=int,
        default=10,
        help='Top-Næ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )

    parser.add_argument(
        '-c', '--collection',
        type=str,
        default=None,
        help='Collectionåç§°ï¼ˆé»˜è®¤: æ ¹æ®æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼‰'
    )

    parser.add_argument(
        '-g', '--ground-truth',
        type=str,
        default=None,
        help='é¢„è®¡ç®—çš„åŸºå‡†ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†è·³è¿‡åŸºå‡†è®¡ç®—ï¼‰'
    )

    parser.add_argument(
        '--force-recreate',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°åˆ›å»ºcollectionï¼ˆåˆ é™¤å·²å­˜åœ¨çš„collectionå¹¶é‡æ–°æ’å…¥æ•°æ®ï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    try:
        run_benchmark(
            vector_file=args.input,
            top_n=args.top_n,
            collection_name=args.collection,
            ground_truth_file=args.ground_truth,
            force_recreate=args.force_recreate
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

