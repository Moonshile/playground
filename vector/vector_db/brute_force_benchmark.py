"""
æš´åŠ›æ³•åŸºå‡†è®¡ç®—è„šæœ¬

ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—queryå’Œdocumentå‘é‡çš„top-Næœ€è¿‘é‚»ï¼Œä½œä¸ºå‘é‡æ•°æ®åº“è¯„æµ‹çš„åŸºå‡†ã€‚

ä½¿ç”¨æ–¹æ³•:
    python vector/vector_db/brute_force_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json
    python vector/vector_db/brute_force_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json -n 20
"""
import os
import json
import time
import argparse
import hashlib
import numpy as np
from typing import List, Dict, Any, Tuple, Optional


# ==================== å·¥å…·å‡½æ•° ====================

def compute_sha512_hex(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬çš„SHA-512å“ˆå¸Œå€¼ï¼ˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰"""
    return hashlib.sha512(text.encode('utf-8')).hexdigest()


def find_original_data_file(original_file: str) -> Optional[str]:
    """æŸ¥æ‰¾åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„ï¼‰"""
    if not original_file:
        return None

    possible_paths = [
        original_file,
        os.path.join('.data', 'mteb', os.path.basename(original_file)),
        os.path.join('.data/mteb', os.path.basename(original_file))
    ]

    for path in possible_paths:
        if os.path.exists(path):
            return path

    return None


def brute_force_top_n(
    query_vec: np.ndarray,
    doc_vectors_normalized: np.ndarray,
    doc_ids: List[str],
    top_n: int
) -> List[Tuple[str, float]]:
    """
    ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—top-Næœ€è¿‘é‚»å‘é‡ï¼ˆä½¿ç”¨NumPyåŠ é€Ÿï¼‰

    Args:
        query_vec: æŸ¥è¯¢å‘é‡ï¼ˆNumPyæ•°ç»„ï¼Œå·²å½’ä¸€åŒ–ï¼‰
        doc_vectors_normalized: å½’ä¸€åŒ–çš„æ–‡æ¡£å‘é‡çŸ©é˜µ [n_docs, dim]
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        top_n: è¿”å›top-Nç»“æœ

    Returns:
        List of (doc_id, similarity_score) tuples, sorted by similarity descending
    """
    # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
    query_norm = np.linalg.norm(query_vec)
    if query_norm == 0:
        return [(doc_ids[i], 0.0) for i in range(min(top_n, len(doc_ids)))]
    query_normalized = query_vec / query_norm

    # è®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
    similarities = np.dot(doc_vectors_normalized, query_normalized)

    # è·å–top-Nç´¢å¼•
    if top_n >= len(doc_ids):
        top_indices = np.argsort(similarities)[::-1]
    else:
        # ä½¿ç”¨argpartitionåªéƒ¨åˆ†æ’åºï¼Œæ›´å¿«
        top_indices = np.argpartition(similarities, -top_n)[-top_n:]
        top_indices = top_indices[np.argsort(similarities[top_indices])[::-1]]

    return [(doc_ids[idx], float(similarities[idx])) for idx in top_indices]


# ==================== æ•°æ®åŠ è½½ ====================

def load_vector_data(vector_file: str) -> Tuple[Dict[str, Any], str]:
    """åŠ è½½å‘é‡æ•°æ®æ–‡ä»¶"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½å‘é‡æ•°æ®: {vector_file}")
    with open(vector_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    model_name = data.get('metadata', {}).get('model', 'unknown')

    if 'query_vectors' in data and 'document_vectors' in data:
        query_count = len(data.get('query_vectors', []))
        doc_count = len(data.get('document_vectors', []))
        print(f"âœ… å·²åŠ è½½æ•°æ®ï¼ˆæ–°æ ¼å¼ï¼‰")
        print(f"   Queryå‘é‡: {query_count}, Documentå‘é‡: {doc_count}, æ¨¡å‹: {model_name}")
    elif 'results' in data:
        results = data.get('results', [])
        print(f"âœ… å·²åŠ è½½æ•°æ®ï¼ˆæ—§æ ¼å¼ï¼‰")
        print(f"   æ•°æ®æ¡æ•°: {len(results)}, æ¨¡å‹: {model_name}")
    else:
        print(f"âš ï¸  æœªçŸ¥çš„æ•°æ®æ ¼å¼")

    return data, model_name


def load_original_data(original_file: str):
    """åŠ è½½åŸå§‹QAæ•°æ®æ–‡ä»¶"""
    file_path = find_original_data_file(original_file)
    if not file_path:
        return None

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if isinstance(data, dict):
        print(f"âœ… å·²åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ–°æ ¼å¼ï¼‰")
    elif isinstance(data, list):
        print(f"âœ… å·²åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ—§æ ¼å¼ï¼Œ{len(data)} æ¡ï¼‰")
    else:
        print(f"âš ï¸  åŸå§‹æ•°æ®æ ¼å¼æœªçŸ¥")

    return data


# ==================== æ•°æ®æå– ====================

def extract_unique_texts_from_original_data(original_data) -> Tuple[List[str], List[str]]:
    """ä»åŸå§‹æ•°æ®ä¸­æå–å”¯ä¸€çš„queryå’Œdocumentåˆ—è¡¨ï¼ˆæŒ‰é¦–æ¬¡å‡ºç°é¡ºåºï¼‰"""
    if isinstance(original_data, dict):
        # æ–°æ ¼å¼ï¼šç›´æ¥è·å–åˆ—è¡¨
        if 'query_list' in original_data and 'document_list' in original_data:
            return original_data['query_list'], original_data['document_list']
        else:
            raise ValueError("æ–°æ ¼å¼åŸå§‹æ•°æ®åº”åŒ…å«query_listå’Œdocument_list")

    elif isinstance(original_data, list):
        # æ—§æ ¼å¼ï¼šä»åˆ—è¡¨ä¸­æå–å”¯ä¸€å€¼
        query_list = []
        document_list = []
        query_seen = set()
        doc_seen = set()

        for item in original_data:
            query_text = item.get('query', '')
            doc_text = item.get('document', '')

            if query_text and query_text not in query_seen:
                query_list.append(query_text)
                query_seen.add(query_text)

            if doc_text:
                doc_hash = compute_sha512_hex(doc_text)
                if doc_hash not in doc_seen:
                    document_list.append(doc_text)
                    doc_seen.add(doc_hash)

        return query_list, document_list

    else:
        raise ValueError(f"æœªçŸ¥çš„åŸå§‹æ•°æ®æ ¼å¼: {type(original_data)}")


def extract_vectors_new_format(
    vector_data: Dict[str, Any],
    original_data_file: str
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ä»æ–°æ ¼å¼å‘é‡æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡

    æ³¨æ„ï¼šä¸èƒ½å‡è®¾å‘é‡åˆ—è¡¨çš„ç´¢å¼•ä¸æ–‡æœ¬åˆ—è¡¨çš„ç´¢å¼•å¯¹åº”ã€‚
    å‘é‡åŒ–è¿‡ç¨‹ä¸­å¯èƒ½æœ‰å¤±è´¥ã€è·³è¿‡ç­‰æƒ…å†µï¼Œå¯¼è‡´é¡ºåºä¸ä¸€è‡´ã€‚
    å¦‚æœå‘é‡æ•°æ®ä¸­åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ï¼›å¦åˆ™éœ€è¦ä»åŸå§‹æ•°æ®åŒ¹é…ã€‚
    """
    query_vectors = vector_data.get('query_vectors', [])
    document_vectors = vector_data.get('document_vectors', [])

    # æ£€æŸ¥å‘é‡æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼ˆç†æƒ³æƒ…å†µï¼‰
    query_texts = vector_data.get('query_texts', None)
    document_texts = vector_data.get('document_texts', None)

    if query_texts is not None and document_texts is not None:
        # å‘é‡æ•°æ®ä¸­åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
        print(f"âœ… å‘é‡æ•°æ®ä¸­åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼Œç›´æ¥åŒ¹é…")
        queries = []
        for query_text, query_vec in zip(query_texts, query_vectors):
            if query_vec is not None:  # è·³è¿‡Noneå€¼ï¼ˆå‘é‡åŒ–å¤±è´¥çš„ï¼‰
                queries.append({
                    'query': query_text,
                    'vector': query_vec
                })

        documents = []
        for doc_text, doc_vec in zip(document_texts, document_vectors):
            if doc_vec is not None:  # è·³è¿‡Noneå€¼
                documents.append({
                    'document': doc_text,
                    'vector': doc_vec,
                    'hash': compute_sha512_hex(doc_text)
                })

        return queries, documents

    # å‘é‡æ•°æ®ä¸­ä¸åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼Œéœ€è¦ä»åŸå§‹æ•°æ®åŒ¹é…
    print(f"âš ï¸  å‘é‡æ•°æ®ä¸­ä¸åŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼Œä»åŸå§‹æ•°æ®åŒ¹é…ï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼‰")

    original_data = load_original_data(original_data_file)
    if not original_data:
        raise ValueError(f"æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {original_data_file}")

    # æå–å”¯ä¸€çš„queryå’Œdocumentæ–‡æœ¬
    query_list, document_list = extract_unique_texts_from_original_data(original_data)

    # éªŒè¯æ•°é‡
    if len(query_vectors) != len(query_list):
        print(f"âš ï¸  è­¦å‘Š: Queryå‘é‡æ•°é‡ ({len(query_vectors)}) != å”¯ä¸€Queryæ•°é‡ ({len(query_list)})")
        print(f"   æ³¨æ„ï¼šå‘é‡åŒ–è¿‡ç¨‹ä¸­å¯èƒ½æœ‰å¤±è´¥æˆ–è·³è¿‡ï¼Œç´¢å¼•åŒ¹é…å¯èƒ½ä¸å‡†ç¡®")
    if len(document_vectors) != len(document_list):
        print(f"âš ï¸  è­¦å‘Š: Documentå‘é‡æ•°é‡ ({len(document_vectors)}) != å”¯ä¸€Documentæ•°é‡ ({len(document_list)})")
        print(f"   æ³¨æ„ï¼šå‘é‡åŒ–è¿‡ç¨‹ä¸­å¯èƒ½æœ‰å¤±è´¥æˆ–è·³è¿‡ï¼Œç´¢å¼•åŒ¹é…å¯èƒ½ä¸å‡†ç¡®")

    # å°è¯•æŒ‰ç´¢å¼•åŒ¹é…ï¼ˆä½†è¿™æ˜¯ä¸å®‰å…¨çš„å‡è®¾ï¼‰
    # åªåŒ¹é…æœ‰æ•ˆå‘é‡ï¼ˆéNoneï¼‰å’Œå¯¹åº”ç´¢å¼•çš„æ–‡æœ¬
    queries = []
    for i, query_text in enumerate(query_list):
        if i < len(query_vectors) and query_vectors[i] is not None:
            queries.append({
                'query': query_text,
                'vector': query_vectors[i]
            })

    documents = []
    for i, doc_text in enumerate(document_list):
        if i < len(document_vectors) and document_vectors[i] is not None:
            documents.append({
                'document': doc_text,
                'vector': document_vectors[i],
                'hash': compute_sha512_hex(doc_text)
            })

    # å¦‚æœåŒ¹é…ç»“æœæ•°é‡ä¸ä¸€è‡´ï¼Œç»™å‡ºè­¦å‘Š
    matched_query_count = len(queries)
    matched_doc_count = len(documents)
    valid_query_vectors = sum(1 for v in query_vectors if v is not None)
    valid_doc_vectors = sum(1 for v in document_vectors if v is not None)

    if matched_query_count != valid_query_vectors:
        print(f"âš ï¸  è­¦å‘Š: åŒ¹é…çš„Queryæ•°é‡ ({matched_query_count}) != æœ‰æ•ˆQueryå‘é‡æ•°é‡ ({valid_query_vectors})")
        print(f"   å»ºè®®ï¼šå‘é‡æ•°æ®æ–‡ä»¶åº”åŒ…å«query_textså’Œdocument_textså­—æ®µä»¥ç¡®ä¿å‡†ç¡®åŒ¹é…")

    if matched_doc_count != valid_doc_vectors:
        print(f"âš ï¸  è­¦å‘Š: åŒ¹é…çš„Documentæ•°é‡ ({matched_doc_count}) != æœ‰æ•ˆDocumentå‘é‡æ•°é‡ ({valid_doc_vectors})")
        print(f"   å»ºè®®ï¼šå‘é‡æ•°æ®æ–‡ä»¶åº”åŒ…å«query_textså’Œdocument_textså­—æ®µä»¥ç¡®ä¿å‡†ç¡®åŒ¹é…")

    return queries, documents


def extract_vectors_old_format(
    results: List[Dict[str, Any]],
    original_data_file: Optional[str] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """ä»æ—§æ ¼å¼å‘é‡æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡"""
    queries = []
    documents = []

    # ç›´æ¥æå–query_vectorå’Œdocument_vector
    for item in results:
        if 'query_vector' in item and item['query_vector'] is not None:
            queries.append({
                'query': item.get('query', ''),
                'vector': item['query_vector']
            })

        if 'document_vector' in item and item['document_vector'] is not None:
            doc_text = item.get('document', '')
            documents.append({
                'document': doc_text,
                'vector': item['document_vector'],
                'hash': compute_sha512_hex(doc_text)
            })

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…ï¼ˆå¤„ç†text_embeddingæ ¼å¼ï¼‰
    if not queries and not documents and original_data_file:
        print("âš ï¸  æœªæ‰¾åˆ°query_vector/document_vectorï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…...")
        original_data = load_original_data(original_data_file)
        if original_data:
            # æ„å»ºæ–‡æœ¬åˆ°å‘é‡çš„æ˜ å°„
            text_to_vector = {}
            for item in results:
                text = item.get('text', '')
                vector = item.get('text_embedding') or item.get('vector')
                if text and vector:
                    text_to_vector[text] = vector

            # ä»åŸå§‹æ•°æ®åŒ¹é…
            query_list, document_list = extract_unique_texts_from_original_data(original_data)

            for query_text in query_list:
                if query_text in text_to_vector:
                    queries.append({
                        'query': query_text,
                        'vector': text_to_vector[query_text]
                    })

            for doc_text in document_list:
                if doc_text in text_to_vector:
                    documents.append({
                        'document': doc_text,
                        'vector': text_to_vector[doc_text],
                        'hash': compute_sha512_hex(doc_text)
                    })

    return queries, documents


def extract_vectors(
    vector_data: Dict[str, Any],
    original_data_file: Optional[str] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """æå–queryå’Œdocumentå‘é‡ï¼ˆè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼‰"""
    if 'query_vectors' in vector_data and 'document_vectors' in vector_data:
        # æ–°æ ¼å¼
        if not original_data_file:
            raise ValueError("æ–°æ ¼å¼éœ€è¦åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„")
        queries, documents = extract_vectors_new_format(vector_data, original_data_file)
    else:
        # æ—§æ ¼å¼
        results = vector_data.get('results', [])
        queries, documents = extract_vectors_old_format(results, original_data_file)

    # éªŒè¯å’Œå»é‡
    print(f"ğŸ“Š æå–ç»“æœ: Query={len(queries)}, Document={len(documents)}")

    # å»é‡documentsï¼ˆåŸºäºhashï¼‰
    unique_docs = {}
    for doc in documents:
        doc_hash = doc['hash']
        if doc_hash not in unique_docs:
            unique_docs[doc_hash] = doc
    documents = list(unique_docs.values())

    # æ£€æŸ¥æ•°é‡åˆç†æ€§
    if len(queries) == 0:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•queryå‘é‡")
    if len(documents) == 0:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•documentå‘é‡")
    if len(queries) > len(documents) * 2:
        print(f"âš ï¸  è­¦å‘Š: Queryæ•°é‡ ({len(queries)}) æ˜æ˜¾å¤§äºDocumentæ•°é‡ ({len(documents)})")
    elif len(documents) > len(queries) * 10:
        print(f"â„¹ï¸  ä¿¡æ¯: Documentæ•°é‡ ({len(documents)}) è¿œå¤§äºQueryæ•°é‡ ({len(queries)})ï¼Œè¿™æ˜¯æ­£å¸¸çš„")

    print(f"ğŸ“Š å»é‡å: Query={len(queries)}, Document={len(documents)}")
    return queries, documents


# ==================== åŸºå‡†è®¡ç®— ====================

def calculate_benchmark(
    queries: List[Dict[str, Any]],
    documents: List[Dict[str, Any]],
    top_n: int
) -> Tuple[List[List[Tuple[str, float]]], List[float]]:
    """
    ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—åŸºå‡†ï¼ˆNumPyåŠ é€Ÿï¼‰

    æ³¨æ„ï¼šè¿™æ˜¯ã€ŒNumPy BLAS brute-forceã€ï¼Œä¸æ˜¯ç†è®ºä¸Šçš„å•æ ¸brute-forceã€‚
    å®é™…ä½¿ç”¨çš„æ˜¯ï¼š
    - NumPy + MKL/OpenBLAS
    - SIMDæŒ‡ä»¤é›†ä¼˜åŒ–
    - å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—

    å› æ­¤ï¼Œå¦‚æœåç»­å¯¹æ¯” FAISS Flat æˆ– Milvus brute-force çš„ç»“æœï¼Œ
    éœ€è¦è°¨æ…è€ƒè™‘è¿™äº›å®ç°å·®å¼‚å¯¹æ€§èƒ½çš„å½±å“ã€‚
    """
    print(f"\nğŸ” å¼€å§‹è®¡ç®—åŸºå‡†ï¼ˆæš´åŠ›æ³•ï¼ŒNumPyåŠ é€Ÿï¼‰...")
    print(f"   Queryæ•°é‡: {len(queries)}, Documentæ•°é‡: {len(documents)}, Top-N: {top_n}")

    # å‡†å¤‡æ•°æ®ï¼šè½¬æ¢ä¸ºNumPyçŸ©é˜µå¹¶å½’ä¸€åŒ–
    print(f"   æ­£åœ¨å‡†å¤‡æ•°æ®...")

    # æ£€æŸ¥å‘é‡ç»´åº¦ä¸€è‡´æ€§ï¼ˆåœ¨è½¬æ¢ä¸ºNumPyæ•°ç»„ä¹‹å‰ï¼‰
    if not documents:
        raise ValueError("documentsåˆ—è¡¨ä¸ºç©º")

    doc_dim = len(documents[0]['vector'])
    for i, doc in enumerate(documents):
        if len(doc['vector']) != doc_dim:
            raise ValueError(f"Document {i} (hash: {doc.get('hash', 'unknown')}) å‘é‡ç»´åº¦ä¸åŒ¹é…: "
                           f"æœŸæœ› {doc_dim}, å®é™… {len(doc['vector'])}")

    # æ£€æŸ¥queryå‘é‡ç»´åº¦
    if not queries:
        raise ValueError("queriesåˆ—è¡¨ä¸ºç©º")

    query_dim = len(queries[0]['vector'])
    for i, query in enumerate(queries):
        if len(query['vector']) != query_dim:
            raise ValueError(f"Query {i} (text: {query.get('query', 'unknown')[:50]}...) å‘é‡ç»´åº¦ä¸åŒ¹é…: "
                           f"æœŸæœ› {query_dim}, å®é™… {len(query['vector'])}")

    # æ£€æŸ¥queryå’Œdocumentç»´åº¦æ˜¯å¦ä¸€è‡´
    if query_dim != doc_dim:
        raise ValueError(f"Queryå’ŒDocumentå‘é‡ç»´åº¦ä¸ä¸€è‡´: Query={query_dim}, Document={doc_dim}")

    doc_vectors_matrix = np.array([doc['vector'] for doc in documents], dtype=np.float32)
    doc_ids = [doc['hash'] for doc in documents]

    # å½’ä¸€åŒ–æ‰€æœ‰æ–‡æ¡£å‘é‡ï¼ˆä¸€æ¬¡æ€§å½’ä¸€åŒ–ï¼‰
    doc_norms = np.linalg.norm(doc_vectors_matrix, axis=1, keepdims=True)
    doc_norms[doc_norms == 0] = 1.0  # é¿å…é™¤é›¶
    doc_vectors_normalized = doc_vectors_matrix / doc_norms

    print(f"   âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼ˆçŸ©é˜µå½¢çŠ¶: {doc_vectors_matrix.shape}ï¼‰")

    # è®¡ç®—åŸºå‡†
    ground_truth = []
    computation_times = []
    total_start = time.time()

    for i, query in enumerate(queries):
        query_start = time.time()

        query_vec = np.array(query['vector'], dtype=np.float32)
        top_n_results = brute_force_top_n(
            query_vec,
            doc_vectors_normalized,
            doc_ids,
            top_n
        )
        ground_truth.append(top_n_results)
        computation_times.append(time.time() - query_start)

        # è¿›åº¦æŠ¥å‘Š
        if (i + 1) % 100 == 0 or (i + 1) == len(queries):
            elapsed = time.time() - total_start
            avg_time = elapsed / (i + 1)
            remaining = avg_time * (len(queries) - i - 1)
            print(f"   è¿›åº¦: {i + 1}/{len(queries)} | "
                  f"å¹³å‡: {avg_time:.4f}s | "
                  f"å‰©ä½™: {remaining:.1f}s", end='\r')

    total_time = time.time() - total_start
    print(f"\nâœ… åŸºå‡†è®¡ç®—å®Œæˆ")
    print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’, å¹³å‡æ¯ä¸ªquery: {total_time / len(queries):.4f}ç§’")
    if len(documents) > 0:
        print(f"   å¤„ç†é€Ÿåº¦: {len(queries) * len(documents) / total_time:.0f} æ¬¡ç›¸ä¼¼åº¦è®¡ç®—/ç§’")

    return ground_truth, computation_times


def calculate_time_statistics(times: List[float]) -> Dict[str, float]:
    """è®¡ç®—æ—¶é—´ç»Ÿè®¡ä¿¡æ¯"""
    if not times:
        return {}

    times_array = np.array(times)
    return {
        'min': float(np.min(times_array)),
        'max': float(np.max(times_array)),
        'mean': float(np.mean(times_array)),
        'median': float(np.median(times_array)),
        'p25': float(np.percentile(times_array, 25)),
        'p75': float(np.percentile(times_array, 75)),
        'p95': float(np.percentile(times_array, 95)),
        'p99': float(np.percentile(times_array, 99))
    }


# ==================== ä¸»å‡½æ•° ====================

def run_brute_force_benchmark(
    vector_file: str,
    top_n: int = 10,
    output_file: Optional[str] = None,
    save_scores: bool = False
):
    """è¿è¡Œæš´åŠ›æ³•åŸºå‡†è®¡ç®—"""
    print("=" * 80)
    print("ğŸš€ æš´åŠ›æ³•åŸºå‡†è®¡ç®—")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    vector_data, model_name = load_vector_data(vector_file)
    metadata = vector_data.get('metadata', {})
    original_file = metadata.get('input_file', '')

    # 2. æå–å‘é‡
    try:
        queries, documents = extract_vectors(vector_data, original_file)
    except Exception as e:
        print(f"âŒ æå–å‘é‡å¤±è´¥: {e}")
        return

    # 3. è®¡ç®—åŸºå‡†
    print("\n" + "=" * 80)
    ground_truth, computation_times = calculate_benchmark(queries, documents, top_n)

    # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 80)

    time_stats = calculate_time_statistics(computation_times)

    print(f"\nâ±ï¸  è®¡ç®—æ—¶é—´ç»Ÿè®¡ï¼ˆæ¯ä¸ªqueryçš„æ—¶é—´ï¼Œå•ä½ï¼šç§’ï¼‰:")
    if time_stats:
        print(f"   æœ€å°å€¼: {time_stats['min']:.6f}")
        print(f"   æœ€å¤§å€¼: {time_stats['max']:.6f}")
        print(f"   å¹³å‡å€¼: {time_stats['mean']:.6f}")
        print(f"   ä¸­ä½æ•°: {time_stats['median']:.6f}")
        print(f"   P25: {time_stats['p25']:.6f}")
        print(f"   P75: {time_stats['p75']:.6f}")
        print(f"   P95: {time_stats['p95']:.6f}")
        print(f"   P99: {time_stats['p99']:.6f}")

    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")
    print(f"   Top-N: {top_n}")
    print(f"   æ¨¡å‹: {model_name}")

    # 5. ä¿å­˜ç»“æœ
    if output_file is None:
        output_file = vector_file.replace('.json', '_brute_force_benchmark.json')

    ground_truth_ids = [[doc_id for doc_id, _ in results] for results in ground_truth]

    results = {
        'vector_file': vector_file,
        'model': model_name,
        'query_count': len(queries),
        'document_count': len(documents),
        'top_n': top_n,
        'ground_truth': ground_truth_ids,
        'time_statistics': time_stats,
        'computation_times': [float(t) for t in computation_times],
        'metadata': {
            'queries': [{'query': q['query']} for q in queries]
        }
    }

    # åªåœ¨éœ€è¦æ—¶ä¿å­˜åˆ†æ•°ï¼ˆé»˜è®¤ä¸ä¿å­˜ä»¥èŠ‚çœç©ºé—´ï¼‰
    if save_scores:
        results['ground_truth_with_scores'] = ground_truth
        print(f"   â„¹ï¸  å·²ä¿å­˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ–‡ä»¶å¯èƒ½è¾ƒå¤§ï¼‰")
    else:
        print(f"   â„¹ï¸  æœªä¿å­˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä½¿ç”¨ --save-scores å¯å¯ç”¨ï¼‰")

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æš´åŠ›æ³•åŸºå‡†è®¡ç®—è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json

  # æŒ‡å®štop-N
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json -n 20

  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/scidocs_gemini_vectors.json -o benchmark_results.json
        """
    )

    parser.add_argument('-i', '--input', required=True, help='å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('-n', '--top-n', type=int, default=10, help='Top-Næ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('-o', '--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--save-scores', action='store_true',
                       help='ä¿å­˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆé»˜è®¤ä¸ä¿å­˜ä»¥èŠ‚çœç©ºé—´ï¼Œ10k queries Ã— top-100 å¯èƒ½äº§ç”Ÿå‡ ç™¾MBæ–‡ä»¶ï¼‰')

    args = parser.parse_args()

    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    try:
        run_brute_force_benchmark(
            vector_file=args.input,
            top_n=args.top_n,
            output_file=args.output,
            save_scores=args.save_scores
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
