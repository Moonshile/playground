"""
æš´åŠ›æ³•åŸºå‡†è®¡ç®—è„šæœ¬

ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—queryå’Œdocumentå‘é‡çš„top-Næœ€è¿‘é‚»ï¼Œä½œä¸ºå‘é‡æ•°æ®åº“è¯„æµ‹çš„åŸºå‡†ã€‚

ä½¿ç”¨æ–¹æ³•:
    python vector/vector_db/brute_force_benchmark.py -i .data/vectors/nfcorpus_openai_vectors.json
    python vector/vector_db/brute_force_benchmark.py -i .data/vectors/nfcorpus_gemini_vec.json -n 20
"""
import os
import json
import time
import argparse
import hashlib
import numpy as np
from typing import List, Dict, Any, Tuple, Optional


# ==================== å·¥å…·å‡½æ•° ====================

def compute_sha2048(text: str) -> str:
    """
    è®¡ç®—æ–‡æœ¬çš„SHA-2048å“ˆå¸Œå€¼
    æ³¨æ„ï¼šSHA-2048å®é™…ä¸Šæ˜¯æŒ‡SHA-512ç®—æ³•ï¼ˆäº§ç”Ÿ512ä½=64å­—èŠ‚çš„å“ˆå¸Œå€¼ï¼‰
    è¿™é‡Œä½¿ç”¨SHA-512ç®—æ³•
    """
    return hashlib.sha512(text.encode('utf-8')).hexdigest()


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return float(dot_product / (norm1 * norm2))


def brute_force_top_n(
    query_vec: List[float],
    doc_vectors: List[List[float]],
    doc_ids: List[str],
    top_n: int
) -> List[Tuple[str, float]]:
    """
    ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—top-Næœ€è¿‘é‚»å‘é‡

    Args:
        query_vec: æŸ¥è¯¢å‘é‡
        doc_vectors: æ–‡æ¡£å‘é‡åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨ï¼ˆä¸doc_vectorså¯¹åº”ï¼‰
        top_n: è¿”å›top-Nç»“æœ

    Returns:
        List of (doc_id, similarity_score) tuples, sorted by similarity descending
    """
    similarities = []
    for doc_id, doc_vec in zip(doc_ids, doc_vectors):
        sim = cosine_similarity(query_vec, doc_vec)
        similarities.append((doc_id, sim))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_n]


# ==================== æ•°æ®åŠ è½½ ====================

def load_vector_data(vector_file: str) -> Tuple[Dict[str, Any], str]:
    """
    åŠ è½½å‘é‡æ•°æ®æ–‡ä»¶

    Returns:
        (æ•°æ®å­—å…¸ï¼ˆåŒ…å«resultså’Œmetadataï¼‰, æ¨¡å‹åç§°)
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½å‘é‡æ•°æ®: {vector_file}")
    with open(vector_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # è·å–æ¨¡å‹åç§°
    model_name = data.get('metadata', {}).get('model', 'unknown')
    results = data.get('results', [])

    print(f"âœ… å·²åŠ è½½ {len(results)} æ¡æ•°æ®")
    print(f"   æ¨¡å‹: {model_name}")

    return data, model_name


def load_original_data(original_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½åŸå§‹QAæ•°æ®æ–‡ä»¶"""
    if not os.path.exists(original_file):
        return []

    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: {original_file}")
    with open(original_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(data)} æ¡åŸå§‹æ•°æ®")
    return data


def extract_query_document_vectors(
    data: List[Dict[str, Any]],
    original_data_file: Optional[str] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ä»æ•°æ®ä¸­æå–queryå’Œdocumentå‘é‡

    Args:
        data: å‘é‡æ•°æ®åˆ—è¡¨
        original_data_file: åŸå§‹QAæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåŒ¹é…queryå’Œdocumentï¼‰

    Returns:
        (queryåˆ—è¡¨, documentåˆ—è¡¨)
    """
    queries = []
    documents = []

    # é¦–å…ˆå°è¯•ç›´æ¥æå–query_vectorå’Œdocument_vector
    for item in data:
        if 'query_vector' in item and item['query_vector'] is not None:
            queries.append({
                'query': item.get('query', ''),
                'vector': item['query_vector'],
                'document': item.get('document', ''),
                'score': item.get('score')
            })

        if 'document_vector' in item and item['document_vector'] is not None:
            doc_text = item.get('document', '')
            doc_hash = compute_sha2048(doc_text)
            documents.append({
                'document': doc_text,
                'vector': item['document_vector'],
                'hash': doc_hash
            })

    # å¦‚æœæ•°æ®æ ¼å¼ä¸åŒï¼ˆä¾‹å¦‚åªæœ‰text_embeddingï¼‰ï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…
    if not queries and not documents:
        print("âš ï¸  æœªæ‰¾åˆ°query_vector/document_vectorå­—æ®µï¼Œå°è¯•ä»åŸå§‹æ•°æ®åŒ¹é…...")

        # åŠ è½½åŸå§‹æ•°æ®
        original_data = []
        if original_data_file:
            if os.path.exists(original_data_file):
                original_data = load_original_data(original_data_file)
            else:
                # å°è¯•ç›¸å¯¹è·¯å¾„
                possible_paths = [
                    original_data_file,
                    os.path.join('.data', 'mteb', os.path.basename(original_data_file)),
                    os.path.join('.data/mteb', os.path.basename(original_data_file))
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        original_data = load_original_data(path)
                        break

        if not original_data:
            print("âŒ æ— æ³•æ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œæ— æ³•åŒ¹é…queryå’Œdocument")
            return queries, documents

        # æ„å»ºæ–‡æœ¬åˆ°å‘é‡çš„æ˜ å°„
        text_to_vector = {}
        for item in data:
            text = item.get('text', '')
            vector = item.get('text_embedding') or item.get('vector')
            if text and vector:
                text_to_vector[text] = vector

        # ä»åŸå§‹æ•°æ®ä¸­åŒ¹é…queryå’Œdocument
        query_texts_seen = set()
        doc_texts_seen = set()

        for orig_item in original_data:
            query_text = orig_item.get('query', '')
            doc_text = orig_item.get('document', '')

            # åŒ¹é…queryå‘é‡
            if query_text and query_text in text_to_vector and query_text not in query_texts_seen:
                queries.append({
                    'query': query_text,
                    'vector': text_to_vector[query_text],
                    'document': doc_text,
                    'score': orig_item.get('score')
                })
                query_texts_seen.add(query_text)

            # åŒ¹é…documentå‘é‡
            if doc_text and doc_text in text_to_vector and doc_text not in doc_texts_seen:
                doc_hash = compute_sha2048(doc_text)
                documents.append({
                    'document': doc_text,
                    'vector': text_to_vector[doc_text],
                    'hash': doc_hash
                })
                doc_texts_seen.add(doc_text)

    print(f"ğŸ“Š æå–ç»“æœ:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")

    # æ£€æŸ¥æ•°é‡æ˜¯å¦åˆç†ï¼ˆé€šå¸¸queryæ•°é‡åº”è¯¥å°‘äºdocumentæ•°é‡ï¼‰
    if len(queries) > 0 and len(documents) > 0:
        if len(queries) > len(documents) * 2:
            print(f"âš ï¸  è­¦å‘Š: Queryæ•°é‡ ({len(queries)}) æ˜æ˜¾å¤§äºDocumentæ•°é‡ ({len(documents)})")
            print(f"   è¿™å¯èƒ½æ˜¯å¼‚å¸¸çš„ï¼Œè¯·æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®")
        elif len(documents) > len(queries) * 10:
            print(f"â„¹ï¸  ä¿¡æ¯: Documentæ•°é‡ ({len(documents)}) è¿œå¤§äºQueryæ•°é‡ ({len(queries)})ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
    elif len(queries) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•queryå‘é‡")
    elif len(documents) == 0:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•documentå‘é‡")
        print(f"   è¿™å¯èƒ½æ˜¯å› ä¸ºå‘é‡æ•°æ®æ–‡ä»¶ä¸å®Œæ•´ï¼ŒåªåŒ…å«äº†queryå‘é‡è€Œæ²¡æœ‰documentå‘é‡")

    return queries, documents


# ==================== åŸºå‡†è®¡ç®— ====================

def calculate_benchmark(
    queries: List[Dict[str, Any]],
    documents: List[Dict[str, Any]],
    top_n: int
) -> Tuple[List[List[Tuple[str, float]]], List[float]]:
    """
    ä½¿ç”¨æš´åŠ›æ³•è®¡ç®—åŸºå‡†

    Args:
        queries: queryåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«vectorå­—æ®µ
        documents: documentåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«hashå’Œvectorå­—æ®µ
        top_n: top-Næ£€ç´¢æ•°é‡

    Returns:
        (æ¯ä¸ªqueryçš„top-Nç»“æœåˆ—è¡¨, æ¯ä¸ªqueryçš„è®¡ç®—æ—¶é—´åˆ—è¡¨)
    """
    print(f"ğŸ” å¼€å§‹è®¡ç®—åŸºå‡†ï¼ˆæš´åŠ›æ³•ï¼‰...")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")
    print(f"   Top-N: {top_n}")

    # å‡†å¤‡æ•°æ®
    doc_vectors = [doc['vector'] for doc in documents]
    doc_ids = [doc['hash'] for doc in documents]

    ground_truth = []
    computation_times = []

    total_start = time.time()

    for i, query in enumerate(queries):
        query_start = time.time()

        query_vec = query['vector']
        top_n_results = brute_force_top_n(query_vec, doc_vectors, doc_ids, top_n)
        ground_truth.append(top_n_results)

        query_time = time.time() - query_start
        computation_times.append(query_time)

        if (i + 1) % 100 == 0 or (i + 1) == len(queries):
            elapsed = time.time() - total_start
            avg_time = elapsed / (i + 1)
            remaining = avg_time * (len(queries) - i - 1)
            print(f"   è¿›åº¦: {i + 1}/{len(queries)} | "
                  f"å¹³å‡æ—¶é—´: {avg_time:.4f}s | "
                  f"é¢„è®¡å‰©ä½™: {remaining:.1f}s", end='\r')

    total_time = time.time() - total_start
    print(f"\nâœ… åŸºå‡†è®¡ç®—å®Œæˆ")
    print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"   å¹³å‡æ¯ä¸ªquery: {total_time / len(queries):.4f}ç§’")

    return ground_truth, computation_times


def calculate_time_statistics(times: List[float]) -> Dict[str, float]:
    """è®¡ç®—æ—¶é—´ç»Ÿè®¡ä¿¡æ¯"""
    if not times:
        return {}

    times_array = np.array(times)
    return {
        'min': float(np.min(times_array)),
        'max': float(np.max(times_array)),
        'mean': float(np.mean(times_array)),
        'median': float(np.median(times_array)),
        'p25': float(np.percentile(times_array, 25)),
        'p75': float(np.percentile(times_array, 75)),
        'p95': float(np.percentile(times_array, 95)),
        'p99': float(np.percentile(times_array, 99))
    }


# ==================== ä¸»å‡½æ•° ====================

def run_brute_force_benchmark(
    vector_file: str,
    top_n: int = 10,
    output_file: Optional[str] = None
):
    """
    è¿è¡Œæš´åŠ›æ³•åŸºå‡†è®¡ç®—

    Args:
        vector_file: å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„
        top_n: top-Næ£€ç´¢æ•°é‡
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
    """
    print("=" * 80)
    print("ğŸš€ æš´åŠ›æ³•åŸºå‡†è®¡ç®—")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    vector_data, model_name = load_vector_data(vector_file)
    results = vector_data.get('results', [])
    metadata = vector_data.get('metadata', {})

    # å°è¯•è·å–åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    original_file = metadata.get('input_file', '')
    queries, documents = extract_query_document_vectors(results, original_file)

    if not queries:
        print("âŒ æœªæ‰¾åˆ°queryå‘é‡ï¼Œæ— æ³•è¿›è¡ŒåŸºå‡†è®¡ç®—")
        return

    if not documents:
        print("âŒ æœªæ‰¾åˆ°documentå‘é‡ï¼Œæ— æ³•è¿›è¡ŒåŸºå‡†è®¡ç®—")
        return

    # å»é‡documentsï¼ˆåŸºäºhashï¼‰
    unique_docs = {}
    for doc in documents:
        doc_hash = doc['hash']
        if doc_hash not in unique_docs:
            unique_docs[doc_hash] = doc

    documents = list(unique_docs.values())
    print(f"ğŸ“Š å»é‡åDocumentæ•°é‡: {len(documents)}")

    # 2. è®¡ç®—åŸºå‡†
    print("\n" + "=" * 80)
    ground_truth, computation_times = calculate_benchmark(queries, documents, top_n)

    # 3. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 80)

    time_stats = calculate_time_statistics(computation_times)

    print(f"\nâ±ï¸  è®¡ç®—æ—¶é—´ç»Ÿè®¡ï¼ˆæ¯ä¸ªqueryçš„æ—¶é—´ï¼Œå•ä½ï¼šç§’ï¼‰:")
    if time_stats:
        print(f"   æœ€å°å€¼: {time_stats['min']:.6f}")
        print(f"   æœ€å¤§å€¼: {time_stats['max']:.6f}")
        print(f"   å¹³å‡å€¼: {time_stats['mean']:.6f}")
        print(f"   ä¸­ä½æ•°: {time_stats['median']:.6f}")
        print(f"   P25: {time_stats['p25']:.6f}")
        print(f"   P75: {time_stats['p75']:.6f}")
        print(f"   P95: {time_stats['p95']:.6f}")
        print(f"   P99: {time_stats['p99']:.6f}")

    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   Queryæ•°é‡: {len(queries)}")
    print(f"   Documentæ•°é‡: {len(documents)}")
    print(f"   Top-N: {top_n}")
    print(f"   æ¨¡å‹: {model_name}")

    # 4. ä¿å­˜ç»“æœ
    if output_file is None:
        output_file = vector_file.replace('.json', '_brute_force_benchmark.json')

    # æ ¼å¼åŒ–ç»“æœï¼ˆåªä¿å­˜doc_idï¼Œä¸ä¿å­˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä»¥èŠ‚çœç©ºé—´ï¼‰
    ground_truth_ids = [[doc_id for doc_id, _ in results] for results in ground_truth]

    results = {
        'vector_file': vector_file,
        'model': model_name,
        'query_count': len(queries),
        'document_count': len(documents),
        'top_n': top_n,
        'ground_truth': ground_truth_ids,  # åªä¿å­˜IDåˆ—è¡¨
        'ground_truth_with_scores': ground_truth,  # å®Œæ•´ç»“æœï¼ˆåŒ…å«åˆ†æ•°ï¼‰
        'time_statistics': time_stats,
        'computation_times': [float(t) for t in computation_times],
        'metadata': {
            'queries': [
                {
                    'query': q['query'],
                    'document': q.get('document', ''),
                    'score': q.get('score')
                }
                for q in queries
            ]
        }
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æš´åŠ›æ³•åŸºå‡†è®¡ç®—è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/nfcorpus_openai_vectors.json

  # æŒ‡å®štop-N
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/nfcorpus_gemini_vec.json -n 20

  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python vector/vector_db/brute_force_benchmark.py -i .data/vectors/nfcorpus_gemini_multimodal_vec.json -o benchmark_results.json
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='å‘é‡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰'
    )

    parser.add_argument(
        '-n', '--top-n',
        type=int,
        default=10,
        help='Top-Næ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )

    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: è¾“å…¥æ–‡ä»¶å_brute_force_benchmark.jsonï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    try:
        run_brute_force_benchmark(
            vector_file=args.input,
            top_n=args.top_n,
            output_file=args.output
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

