"""
ä½¿ç”¨ Google Gemini Embeddings API å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
ä½¿ç”¨ gemini-embedding-001 æ¨¡å‹ï¼ŒåŒ…å«æ€§èƒ½ç»Ÿè®¡å’Œè´¹ç”¨è®¡ç®—

ä½¿ç”¨æ–¹æ³•:
    python vector/vectorize/gemini_text_test.py -i input.json -o output.json
    python vector/vectorize/gemini_text_test.py -i input.json -o output.json -b 50 -r 10
"""
import os
import json
import time
import argparse
from typing import List, Optional, Dict, Any, Tuple
import google.generativeai as genai

# ==================== é…ç½® ====================

# Gemini Embeddings æ¨¡å‹å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰
EMBEDDING_PRICING = {
    "models/gemini-embedding-001": 0.15,  # $0.15 per 1M tokens
    "gemini-embedding-001": 0.15,  # ç®€åŒ–åç§°
}


# ==================== å·¥å…·å‡½æ•° ====================

def get_gemini_client():
    """è·å– Gemini å®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡è¯»å– API key"""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
    genai.configure(api_key=api_key)
    return genai


def get_model_pricing(model: str) -> float:
    """è·å–æ¨¡å‹çš„å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰"""
    # æ ‡å‡†åŒ–æ¨¡å‹åç§°
    if model.startswith("models/"):
        model_key = model
    else:
        model_key = f"models/{model}"
    return EMBEDDING_PRICING.get(model_key, EMBEDDING_PRICING.get(model, 0.15))


def calculate_cost(tokens: int, model: str) -> float:
    """
    è®¡ç®—è´¹ç”¨ï¼ˆç¾å…ƒï¼‰

    Args:
        tokens: tokenæ•°é‡
        model: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        è´¹ç”¨ï¼ˆç¾å…ƒï¼‰
    """
    price_per_million = get_model_pricing(model)
    return (tokens / 1_000_000) * price_per_million


# ==================== å‘é‡åŒ–å‡½æ•° ====================

def vectorize_text(
    text: str,
    model: str = "models/gemini-embedding-001",
    client=None
) -> List[float]:
    """
    ä½¿ç”¨ Gemini Embeddings API å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        text: è¦è½¬æ¢çš„æ–‡æœ¬
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "models/gemini-embedding-001"
        client: Gemini å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º

    Returns:
        å‘é‡åˆ—è¡¨ï¼ˆæµ®ç‚¹æ•°åˆ—è¡¨ï¼‰
    """
    if client is None:
        client = get_gemini_client()

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    # æ ‡å‡†åŒ–æ¨¡å‹åç§°
    if not model.startswith("models/"):
        model = f"models/{model}"

    result = genai.embed_content(
        model=model,
        content=text
    )

    # Gemini API è¿”å›çš„æ ¼å¼
    if hasattr(result, 'embedding'):
        return result.embedding
    elif isinstance(result, dict) and 'embedding' in result:
        return result['embedding']
    else:
        # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
        return result


def vectorize_texts_batch(
    texts: List[str],
    model: str = "models/gemini-embedding-001",
    client=None,
    batch_size: int = 100
) -> Tuple[List[List[float]], float, Dict[str, int]]:
    """
    ä½¿ç”¨ Gemini Embeddings API æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        texts: è¦è½¬æ¢çš„æ–‡æœ¬åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "models/gemini-embedding-001"
        client: Gemini å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º
        batch_size: æ¯æ¬¡è¯·æ±‚çš„æ‰¹é‡å¤§å°ï¼ˆGemini API æ”¯æŒæ‰¹é‡å¤„ç†ï¼‰

    Returns:
        (å‘é‡åˆ—è¡¨, APIè°ƒç”¨æ€»æ—¶é—´ï¼ˆç§’ï¼‰, tokenä½¿ç”¨ä¿¡æ¯å­—å…¸)
        tokenä½¿ç”¨ä¿¡æ¯åŒ…å«: prompt_tokens, total_tokens
    """
    if client is None:
        client = get_gemini_client()

    if not texts:
        return [], 0.0, {"prompt_tokens": 0, "total_tokens": 0}

    # è¿‡æ»¤ç©ºæ–‡æœ¬
    valid_texts = [(i, text) for i, text in enumerate(texts) if text and text.strip()]
    if not valid_texts:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬")

    # æ ‡å‡†åŒ–æ¨¡å‹åç§°
    if not model.startswith("models/"):
        model = f"models/{model}"

    all_vectors = [None] * len(texts)
    total_api_time = 0.0
    total_prompt_tokens = 0
    total_tokens = 0

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(valid_texts), batch_size):
        batch = valid_texts[i:i + batch_size]
        batch_texts = [text for _, text in batch]
        batch_indices = [idx for idx, _ in batch]

        # åªè®°å½•APIè°ƒç”¨æ—¶é—´
        api_start = time.time()
        try:
            # Gemini API æ”¯æŒæ‰¹é‡å¤„ç†
            result = genai.embed_content(
                model=model,
                content=batch_texts
            )
            api_end = time.time()
            api_time = api_end - api_start
            total_api_time += api_time

            # æå–å‘é‡ç»“æœ
            # Gemini API è¿”å›æ ¼å¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸
            if isinstance(result, dict):
                embeddings = result.get('embeddings', [])
            elif hasattr(result, 'embeddings'):
                embeddings = result.embeddings
            else:
                embeddings = result if isinstance(result, list) else [result]

            # å°†ç»“æœæ”¾å›åŸä½ç½®
            for idx, embedding in zip(batch_indices, embeddings):
                if isinstance(embedding, dict) and 'values' in embedding:
                    all_vectors[idx] = embedding['values']
                elif isinstance(embedding, list):
                    all_vectors[idx] = embedding
                elif hasattr(embedding, 'values'):
                    all_vectors[idx] = embedding.values
                else:
                    all_vectors[idx] = embedding

            # æå–tokenä½¿ç”¨ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # æ³¨æ„ï¼šGemini API å¯èƒ½ä¸ç›´æ¥è¿”å›tokenä¿¡æ¯ï¼Œéœ€è¦ä¼°ç®—
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦çº¦ç­‰äº0.25ä¸ªtoken
            for text in batch_texts:
                estimated_tokens = int(len(text) * 0.25)
                total_prompt_tokens += estimated_tokens
                total_tokens += estimated_tokens

        except Exception as e:
            api_end = time.time()
            api_time = api_end - api_start
            total_api_time += api_time
            raise RuntimeError(f"Gemini API è°ƒç”¨å¤±è´¥: {e}") from e

    return all_vectors, total_api_time, {
        "prompt_tokens": total_prompt_tokens,
        "total_tokens": total_tokens
    }


# ==================== æ•°æ®åŠ è½½å’Œæ£€æŸ¥ç‚¹ ====================

def load_qa_data(input_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½QAæ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(data)} æ¡æ•°æ®")
    return data


def get_default_cumulative_stats() -> Dict[str, Any]:
    """è·å–é»˜è®¤çš„ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "query_prompt_tokens": 0,
        "query_total_tokens": 0,
        "document_prompt_tokens": 0,
        "document_total_tokens": 0,
        "total_tokens": 0,
        "total_cost": 0.0
    }


def get_default_checkpoint() -> Dict[str, Any]:
    """è·å–é»˜è®¤çš„æ£€æŸ¥ç‚¹ç»“æ„"""
    return {
        "query_processed_count": 0,
        "document_processed_count": 0,
        "results": [],
        "performance": [],
        "cumulative_stats": get_default_cumulative_stats(),
        "query_vector_cache": {}  # ç”¨äºå­˜å‚¨å»é‡åçš„queryå‘é‡
    }


def load_checkpoint(output_file: str) -> Dict[str, Any]:
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå·²å¤„ç†çš„æ•°æ®ï¼ŒåŒ…å«æ€§èƒ½å’Œtokenç»Ÿè®¡ï¼‰"""
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        print(f"ğŸ“‚ å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)

            # å…¼å®¹æ—§æ ¼å¼ï¼ˆåªæœ‰processed_countï¼‰
            if 'processed_count' in checkpoint and 'query_processed_count' not in checkpoint:
                # æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                old_count = checkpoint.get('processed_count', 0)
                checkpoint['query_processed_count'] = old_count
                checkpoint['document_processed_count'] = old_count
                del checkpoint['processed_count']

            query_processed = checkpoint.get('query_processed_count', 0)
            document_processed = checkpoint.get('document_processed_count', 0)
            print(f"   Queryå·²å¤„ç†: {query_processed} æ¡")
            print(f"   Documentå·²å¤„ç†: {document_processed} æ¡")

            # æ¢å¤ç´¯è®¡çš„tokenç»Ÿè®¡
            cumulative_stats = checkpoint.get('cumulative_stats', get_default_cumulative_stats())
            if cumulative_stats.get("total_tokens", 0) > 0:
                print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
                print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

            # æ¢å¤queryå‘é‡ç¼“å­˜ï¼ˆç”¨äºå»é‡ï¼‰
            if 'query_vector_cache' not in checkpoint:
                checkpoint['query_vector_cache'] = {}

            return checkpoint
        except json.JSONDecodeError as e:
            print(f"   âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ˆå¯èƒ½ä¿å­˜æ—¶è¢«ä¸­æ–­ï¼‰: {e}")
            print(f"   ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶å¯èƒ½å·²æŸåï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            # å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            backup_file = checkpoint_file + '.corrupted'
            try:
                import shutil
                shutil.move(checkpoint_file, backup_file)
                print(f"   ğŸ“¦ å·²å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°: {backup_file}")
            except Exception as backup_error:
                print(f"   âš ï¸  æ— æ³•å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {backup_error}")
            # è¿”å›ç©ºçš„æ£€æŸ¥ç‚¹
            return get_default_checkpoint()
        except Exception as e:
            print(f"   âŒ åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            print(f"   ğŸ’¡ å°†ä»å¤´å¼€å§‹å¤„ç†")
            return get_default_checkpoint()

    return get_default_checkpoint()


def save_checkpoint(output_file: str, checkpoint: Dict[str, Any], verbose: bool = False):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä½¿ç”¨åŸå­æ€§å†™å…¥ï¼Œé¿å…æ–‡ä»¶æŸåï¼‰

    ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ + é‡å‘½åçš„æ–¹å¼ï¼Œç¡®ä¿å†™å…¥è¿‡ç¨‹çš„åŸå­æ€§ï¼š
    1. å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
    2. å†™å…¥æˆåŠŸåå†é‡å‘½åæ›¿æ¢åŸæ–‡ä»¶
    è¿™æ ·å³ä½¿å†™å…¥è¿‡ç¨‹ä¸­è¢«ä¸­æ–­ï¼ŒåŸæ–‡ä»¶ä¹Ÿä¸ä¼šè¢«æŸå
    """
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    temp_file = checkpoint_file + '.tmp'

    try:
        # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, indent=2, ensure_ascii=False)

        # å†™å…¥æˆåŠŸåå†é‡å‘½åï¼ˆåŸå­æ“ä½œï¼‰
        # åœ¨å¤§å¤šæ•°æ–‡ä»¶ç³»ç»Ÿä¸Šï¼Œé‡å‘½åæ˜¯åŸå­æ“ä½œ
        os.replace(temp_file, checkpoint_file)

        if verbose:
            processed = checkpoint.get('processed_count', 0)
            print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {processed} æ¡å·²å¤„ç†")
    except Exception as e:
        # å¦‚æœå†™å…¥å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“ä¿å­˜å¤±è´¥
        raise RuntimeError(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}") from e


def update_cumulative_stats(
    cumulative_stats: Dict[str, Any],
    token_info: Dict[str, int],
    stats_type: str,
    model: str
):
    """æ›´æ–°ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    cumulative_stats[f"{stats_type}_prompt_tokens"] += token_info.get("prompt_tokens", 0)
    cumulative_stats[f"{stats_type}_total_tokens"] += token_info.get("total_tokens", 0)
    cumulative_stats["total_tokens"] = (
        cumulative_stats["query_total_tokens"] +
        cumulative_stats["document_total_tokens"]
    )
    cumulative_stats["total_cost"] = calculate_cost(cumulative_stats["total_tokens"], model)


# ==================== æ€§èƒ½æŠ¥å‘Š ====================

def print_performance_report(
    performance_log: List[Dict[str, Any]],
    processed_count: int,
    total_count: int,
    report_type: str,
    model: str,
    cumulative_stats: Optional[Dict[str, Any]] = None
):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Šï¼ˆåŒ…å«tokenç»Ÿè®¡å’Œè´¹ç”¨ï¼‰"""
    if not performance_log:
        return

    # ç­›é€‰æŒ‡å®šç±»å‹çš„æ€§èƒ½æ•°æ®
    type_logs = [log for log in performance_log if log.get("type") == report_type]
    if not type_logs:
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_api_time = sum(log.get("time_seconds", 0) for log in type_logs)
    total_items = sum(log.get("batch_size", 0) for log in type_logs)
    total_prompt_tokens = sum(log.get("prompt_tokens", 0) for log in type_logs)
    total_tokens = sum(log.get("total_tokens", 0) for log in type_logs)
    avg_time_per_batch = total_api_time / len(type_logs) if type_logs else 0
    avg_items_per_second = total_items / total_api_time if total_api_time > 0 else 0
    avg_tokens_per_item = total_tokens / total_items if total_items > 0 else 0

    # è®¡ç®—è´¹ç”¨
    cost = calculate_cost(total_tokens, model)

    # ç´¯è®¡ç»Ÿè®¡ï¼ˆå¦‚æœæä¾›ï¼‰
    cumulative_tokens = cumulative_stats.get("total_tokens", 0) if cumulative_stats else 0
    cumulative_cost = cumulative_stats.get("total_cost", 0.0) if cumulative_stats else 0.0

    print(f"\nğŸ“Š {report_type.upper()} æ€§èƒ½æŠ¥å‘Š:")
    print(f"   å·²å¤„ç†æ‰¹æ¬¡: {len(type_logs)}")
    print(f"   æ€»å¤„ç†é¡¹æ•°: {total_items}")
    print(f"   æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"   å¹³å‡æ¯æ‰¹æ¬¡æ—¶é—´: {avg_time_per_batch:.2f}s")
    print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {avg_items_per_second:.2f} é¡¹/ç§’")
    print(f"   Tokenç»Ÿè®¡:")
    print(f"     - æ€»è¾“å…¥Token: {total_prompt_tokens:,}")
    print(f"     - æ€»Token: {total_tokens:,}")
    print(f"     - å¹³å‡æ¯é¡¹Token: {avg_tokens_per_item:.1f}")
    print(f"   è´¹ç”¨ç»Ÿè®¡:")
    print(f"     - æœ¬æ¬¡è´¹ç”¨: ${cost:.4f}")
    if cumulative_stats:
        print(f"     - ç´¯è®¡Token: {cumulative_tokens:,}")
        print(f"     - ç´¯è®¡è´¹ç”¨: ${cumulative_cost:.4f}")
    print(f"   æ€»è¿›åº¦: {processed_count}/{total_count} ({processed_count/total_count*100:.1f}%)")
    print()


# ==================== ä¸»å¤„ç†å‡½æ•° ====================

def process_batch(
    texts: List[str],
    text_type: str,
    batch_index: int,
    model: str,
    client,
    batch_size: int,
    processed_count: int,
    total_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    report_interval: int
) -> Tuple[List[List[float]], float, int, int, bool]:
    """
    å¤„ç†ä¸€æ‰¹æ–‡æœ¬ï¼Œè¿”å›å‘é‡ã€APIæ—¶é—´ã€tokenç»Ÿè®¡

    Returns:
        (å‘é‡åˆ—è¡¨, APIæ€»æ—¶é—´, prompt_tokens, total_tokens, should_report)
        should_report: æ˜¯å¦éœ€è¦æ‰“å°æ€§èƒ½æŠ¥å‘Š
    """
    # APIè°ƒç”¨å‰æç¤ºï¼ˆè®©ç”¨æˆ·çŸ¥é“ç¨‹åºè¿˜åœ¨è¿è¡Œï¼‰
    if batch_index == 0 or (batch_index + 1) % 10 == 0:
        print(f"   â³ æ­£åœ¨è°ƒç”¨APIå¤„ç† {text_type}æ‰¹æ¬¡ {batch_index + 1}...")

    batch_vectors, api_time, token_info = vectorize_texts_batch(
        texts, model=model, client=client, batch_size=batch_size
    )

    # è®°å½•æ€§èƒ½
    batch_perf = {
        "type": text_type,
        "batch_index": batch_index,
        "batch_size": len(texts),
        "time_seconds": round(api_time, 2),
        "items_per_second": round(len(texts) / api_time, 2) if api_time > 0 else 0,
        "prompt_tokens": token_info.get("prompt_tokens", 0),
        "total_tokens": token_info.get("total_tokens", 0)
    }
    performance_log.append(batch_perf)

    # æ›´æ–°ç´¯è®¡ç»Ÿè®¡
    update_cumulative_stats(cumulative_stats, token_info, text_type, model)

    # æ‰“å°è¿›åº¦
    processed = processed_count + (batch_index + 1) * batch_size
    if processed > total_count:
        processed = total_count
    progress = processed / total_count * 100
    print(f"   {text_type.capitalize()}æ‰¹æ¬¡ {batch_index + 1}: {len(texts)} æ¡, "
          f"APIè°ƒç”¨æ—¶é—´ {api_time:.2f}s, "
          f"Token: {token_info.get('total_tokens', 0):,}, "
          f"æ€»è¿›åº¦: {processed}/{total_count} ({progress:.1f}%)")

    # è¿”å›æ˜¯å¦éœ€è¦æ‰“å°æŠ¥å‘Šï¼ˆç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦æ‰“å°å’Œä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
    should_report = (batch_index + 1) % report_interval == 0

    return (
        batch_vectors,
        api_time,
        token_info.get("prompt_tokens", 0),
        token_info.get("total_tokens", 0),
        should_report
    )


def save_checkpoint_with_results(
    output_file: str,
    checkpoint: Dict[str, Any],
    results: List[Dict[str, Any]],
    query_processed_count: int,
    document_processed_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    query_vector_cache: Optional[Dict[str, List[float]]] = None,
    verbose: bool = False
):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ç»“æœï¼‰
    æ³¨æ„ï¼šæ— è®ºæ˜¯å¦ä»å¤´å¼€å§‹ï¼Œè¿è¡Œè¿‡ç¨‹ä¸­éƒ½ä¼šä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿ä¸­æ–­åå¯ä»¥æ¢å¤
    """
    checkpoint["results"] = results
    checkpoint["query_processed_count"] = query_processed_count
    checkpoint["document_processed_count"] = document_processed_count
    checkpoint["performance"] = performance_log
    checkpoint["cumulative_stats"] = cumulative_stats
    if query_vector_cache is not None:
        checkpoint["query_vector_cache"] = query_vector_cache
    save_checkpoint(output_file, checkpoint, verbose=verbose)


def process_qa_data(
    input_file: str,
    output_file: str,
    model: str = "models/gemini-embedding-001",
    batch_size: int = 100,
    from_scratch: bool = False,
    report_interval: int = 5,
    max_items: Optional[int] = None
):
    """
    å¤„ç†QAæ•°æ®ï¼Œä¸ºqueryå’Œdocumentç”Ÿæˆå‘é‡

    Args:
        input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„æ¨¡å‹
        batch_size: æ‰¹é‡å¤„ç†å¤§å°
        from_scratch: æ˜¯å¦ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹
        report_interval: æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼‰
        max_items: æœ€å¤§å¤„ç†æ¡æ•°ï¼ˆNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°æ®ï¼‰
    """
    print("=" * 60)
    print("å¤„ç†QAæ•°æ® - ç”Ÿæˆå‘é‡ (Gemini)")
    print("=" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ¨¡å‹: {model}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    if from_scratch:
        print(f"æ¨¡å¼: ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰")
    else:
        print(f"æ¨¡å¼: è‡ªåŠ¨æ¢å¤ï¼ˆå¦‚æœå­˜åœ¨æ£€æŸ¥ç‚¹ï¼‰")
    print(f"æ€§èƒ½æŠ¥å‘Šé—´éš”: æ¯ {report_interval} ä¸ªæ‰¹æ¬¡")
    if max_items:
        print(f"æœ€å¤§å¤„ç†æ¡æ•°: {max_items}")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    data = load_qa_data(input_file)

    # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ¡æ•°ï¼Œåˆ™æˆªå–æ•°æ®
    original_data_count = len(data)
    if max_items and max_items > 0:
        data = data[:max_items]
        print(f"ğŸ“Š æ•°æ®é™åˆ¶: ä» {original_data_count} æ¡é™åˆ¶åˆ° {len(data)} æ¡")

    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœ from_scratch=Trueï¼Œåˆ™å¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹ï¼‰
    if from_scratch:
        checkpoint = get_default_checkpoint()
        # å¦‚æœå­˜åœ¨æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·
        checkpoint_file = output_file.replace('.json', '_checkpoint.json')
        if os.path.exists(checkpoint_file):
            print(f"âš ï¸  å‘ç°å·²æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½†å°†ä»å¤´å¼€å§‹å¤„ç†ï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ä¼šè¢«åˆ é™¤ï¼‰")
    else:
        checkpoint = load_checkpoint(output_file)

    query_processed_count = checkpoint.get("query_processed_count", 0)
    document_processed_count = checkpoint.get("document_processed_count", 0)
    results = checkpoint.get("results", [])
    performance_log = checkpoint.get("performance", [])
    cumulative_stats = checkpoint.get("cumulative_stats", get_default_cumulative_stats())
    query_vector_cache = checkpoint.get("query_vector_cache", {})

    if query_processed_count > 0 or document_processed_count > 0:
        print(f"ğŸ”„ æ–­ç‚¹ç»­è·‘çŠ¶æ€:")
        print(f"   Query: {query_processed_count}/{len(data)} æ¡å·²å¤„ç†")
        print(f"   Document: {document_processed_count}/{len(data)} æ¡å·²å¤„ç†")
        if cumulative_stats.get("total_tokens", 0) > 0:
            print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
            print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

    # è·å–å®¢æˆ·ç«¯
    client = get_gemini_client()

    # å¤„ç†å‰©ä½™çš„æ•°æ®
    query_remaining_data = data[query_processed_count:]
    document_remaining_data = data[document_processed_count:]

    if len(query_remaining_data) == 0 and len(document_remaining_data) == 0:
        print("âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼")
        return

    print(f"\nğŸ“Š å¤„ç†è¿›åº¦:")
    print(f"   Query: {query_processed_count}/{len(data)} ({query_processed_count/len(data)*100:.1f}%), å‰©ä½™: {len(query_remaining_data)} æ¡")
    print(f"   Document: {document_processed_count}/{len(data)} ({document_processed_count/len(data)*100:.1f}%), å‰©ä½™: {len(document_remaining_data)} æ¡\n")

    # æå–queryå’Œdocumentæ–‡æœ¬ï¼ˆåˆ†åˆ«ä»å‰©ä½™æ•°æ®ä¸­æå–ï¼‰
    queries = [item["query"] for item in query_remaining_data]
    documents = [item["document"] for item in document_remaining_data]

    # Queryå»é‡ï¼šæ„å»ºå»é‡æ˜ å°„
    print(f"ğŸ” Queryå»é‡åˆ†æ: åŸå§‹ {len(queries)} æ¡")
    unique_queries = {}
    query_to_unique_map = []  # åŸå§‹ç´¢å¼• -> å”¯ä¸€queryçš„æ˜ å°„
    for i, query in enumerate(queries):
        if query not in unique_queries:
            unique_queries[query] = len(unique_queries)
        query_to_unique_map.append(unique_queries[query])

    unique_query_list = list(unique_queries.keys())
    print(f"   å»é‡å: {len(unique_query_list)} æ¡å”¯ä¸€query (èŠ‚çœ {len(queries) - len(unique_query_list)} æ¬¡APIè°ƒç”¨)")

    # å¤„ç†queryå‘é‡ï¼ˆä½¿ç”¨å»é‡åçš„å”¯ä¸€queryï¼‰
    print("ğŸ” æ­£åœ¨å¤„ç†queryå‘é‡ï¼ˆå·²å»é‡ï¼‰...")
    query_api_time_total = 0.0
    unique_query_vectors = {}  # å­˜å‚¨å”¯ä¸€queryçš„å‘é‡
    query_total_prompt_tokens = 0
    query_total_tokens = 0

    # æ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„queryå‘é‡
    cached_count = 0
    for query in unique_query_list:
        if query in query_vector_cache:
            unique_query_vectors[query] = query_vector_cache[query]
            cached_count += 1

    if cached_count > 0:
        print(f"   ğŸ’¾ ä»ç¼“å­˜ä¸­æ¢å¤ {cached_count} ä¸ªqueryå‘é‡")

    # åªå¤„ç†æœªç¼“å­˜çš„å”¯ä¸€query
    uncached_unique_queries = [q for q in unique_query_list if q not in unique_query_vectors]
    last_batch_was_reported = False

    if uncached_unique_queries:
        num_batches = (len(uncached_unique_queries) + batch_size - 1) // batch_size

        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(uncached_unique_queries))
            batch_queries = uncached_unique_queries[start_idx:end_idx]

            try:
                batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                    batch_queries, "query", i, model, client, batch_size,
                    query_processed_count, len(data), performance_log, cumulative_stats, report_interval
                )

                query_api_time_total += api_time
                query_total_prompt_tokens += prompt_tokens
                query_total_tokens += total_tokens

                # å­˜å‚¨å”¯ä¸€queryçš„å‘é‡åˆ°ç¼“å­˜
                for j, qv in enumerate(batch_vectors):
                    query_text = batch_queries[j]
                    unique_query_vectors[query_text] = qv
                    query_vector_cache[query_text] = qv

                    # åŒæ—¶æ›´æ–°æ‰€æœ‰ä½¿ç”¨è¿™ä¸ªqueryçš„results
                    for orig_idx in range(len(data)):
                        if data[orig_idx].get("query") == query_text:
                            # ç¡®ä¿resultsåˆ—è¡¨è¶³å¤Ÿé•¿
                            while len(results) <= orig_idx:
                                temp_idx = len(results)
                                results.append({
                                    "query": data[temp_idx].get("query", ""),
                                    "document": data[temp_idx].get("document", ""),
                                    "query_vector": None,
                                    "document_vector": None,
                                    "score": data[temp_idx].get("score")
                                })
                            results[orig_idx]["query_vector"] = qv

                # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
                if should_report:
                    # è®¡ç®—å·²å¤„ç†çš„queryæ•°é‡ï¼ˆæ‰€æœ‰queryéƒ½å·²å¤„ç†ï¼Œå› ä¸ºå»é‡åéƒ½å¤„ç†å®Œäº†ï¼‰
                    current_query_processed = query_processed_count + len(query_remaining_data)
                    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                    print_performance_report(
                        performance_log, current_query_processed, len(data), "query", model, cumulative_stats
                    )
                    # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«queryå‘é‡ç¼“å­˜ï¼‰
                    save_checkpoint_with_results(
                        output_file, checkpoint, results,
                        current_query_processed,
                        document_processed_count,
                        performance_log, cumulative_stats, query_vector_cache, verbose=True
                    )
                    last_batch_was_reported = True
                elif i == num_batches - 1:
                    # æœ€åä¸€æ‰¹ï¼Œå³ä½¿ä¸æ»¡è¶³æŠ¥å‘Šé—´éš”ä¹Ÿè¦æŠ¥å‘Š
                    last_batch_was_reported = False

            except Exception as e:
                print(f"   âŒ Queryæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
                raise
    else:
        print(f"   âœ… æ‰€æœ‰queryå‘é‡å·²ä»ç¼“å­˜ä¸­æ¢å¤ï¼Œæ— éœ€APIè°ƒç”¨")

    # ç¡®ä¿æ‰€æœ‰queryå‘é‡éƒ½å·²ä¿å­˜åˆ°resultsä¸­ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
    for i in range(len(data)):
        if i >= len(results):
            results.append({
                "query": data[i].get("query", ""),
                "document": data[i].get("document", ""),
                "query_vector": query_vector_cache.get(data[i].get("query", "")),
                "document_vector": None,
                "score": data[i].get("score")
            })
        elif results[i].get("query_vector") is None:
            query_text = data[i].get("query", "")
            if query_text in query_vector_cache:
                results[i]["query_vector"] = query_vector_cache[query_text]

    # Queryå¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    final_query_processed = query_processed_count + len(query_remaining_data)
    if not last_batch_was_reported and (len(uncached_unique_queries) > 0 or cached_count > 0):
        # æ‰“å°æœ€ç»ˆqueryæ€§èƒ½æŠ¥å‘Š
        print_performance_report(
            performance_log, final_query_processed, len(data), "query", model, cumulative_stats
        )
        # ä¿å­˜æ£€æŸ¥ç‚¹
        save_checkpoint_with_results(
            output_file, checkpoint, results,
            final_query_processed,
            document_processed_count,
            performance_log, cumulative_stats, query_vector_cache, verbose=True
        )

    print(f"âœ… Queryå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {query_api_time_total:.2f}s, "
          f"æ€»Token: {query_total_tokens:,}, "
          f"å®é™…è°ƒç”¨: {len(uncached_unique_queries)} æ¡å”¯ä¸€query\n")

    # å¤„ç†documentå‘é‡
    print("ğŸ“„ æ­£åœ¨å¤„ç†documentå‘é‡...")
    doc_api_time_total = 0.0
    doc_vectors = []
    doc_total_prompt_tokens = 0
    doc_total_tokens = 0

    num_batches = (len(documents) + batch_size - 1) // batch_size
    last_doc_batch_was_reported = False

    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(documents))
        batch_docs = documents[start_idx:end_idx]

        try:
            batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                batch_docs, "document", i, model, client, batch_size,
                document_processed_count, len(data), performance_log, cumulative_stats, report_interval
            )

            doc_vectors.extend(batch_vectors)
            doc_api_time_total += api_time
            doc_total_prompt_tokens += prompt_tokens
            doc_total_tokens += total_tokens

            # æ›´æ–°ç»“æœï¼ˆå¡«å……documentå‘é‡ï¼‰
            for j, dv in enumerate(batch_vectors):
                idx = document_processed_count + start_idx + j
                # ç¡®ä¿resultsåˆ—è¡¨è¶³å¤Ÿé•¿
                while len(results) <= idx:
                    temp_idx = len(results)
                    query_text = data[temp_idx].get("query", "")
                    results.append({
                        "query": query_text,
                        "document": data[temp_idx].get("document", ""),
                        "query_vector": query_vector_cache.get(query_text),
                        "document_vector": None,
                        "score": data[temp_idx].get("score")
                    })

                results[idx]["document_vector"] = dv
                # å¦‚æœqueryå‘é‡è¿˜æ²¡æœ‰ï¼Œä»ç¼“å­˜ä¸­è·å–
                if results[idx].get("query_vector") is None:
                    query_text = data[idx].get("query", "")
                    if query_text in query_vector_cache:
                        results[idx]["query_vector"] = query_vector_cache[query_text]

            # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
            if should_report:
                current_doc_processed = document_processed_count + end_idx
                # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                print_performance_report(
                    performance_log, current_doc_processed, len(data), "document", model, cumulative_stats
                )
                # ä¿å­˜æ£€æŸ¥ç‚¹
                save_checkpoint_with_results(
                    output_file, checkpoint, results,
                    final_query_processed,  # æ‰€æœ‰queryéƒ½å·²å¤„ç†
                    current_doc_processed,
                    performance_log, cumulative_stats, query_vector_cache, verbose=True
                )
                last_doc_batch_was_reported = True
            elif i == num_batches - 1:
                # æœ€åä¸€æ‰¹ï¼Œå³ä½¿ä¸æ»¡è¶³æŠ¥å‘Šé—´éš”ä¹Ÿè¦æŠ¥å‘Š
                last_doc_batch_was_reported = False

        except Exception as e:
            print(f"   âŒ Documentæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
            raise

    # Documentå¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    final_doc_processed = document_processed_count + len(document_remaining_data)
    if not last_doc_batch_was_reported and num_batches > 0:
        # æ‰“å°æœ€ç»ˆdocumentæ€§èƒ½æŠ¥å‘Š
        print_performance_report(
            performance_log, final_doc_processed, len(data), "document", model, cumulative_stats
        )
        # ä¿å­˜æ£€æŸ¥ç‚¹
        save_checkpoint_with_results(
            output_file, checkpoint, results,
            final_query_processed,
            final_doc_processed,
            performance_log, cumulative_stats, query_vector_cache, verbose=True
        )

    print(f"âœ… Documentå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {doc_api_time_total:.2f}s, "
          f"æ€»Token: {doc_total_tokens:,}\n")

    # ç¡®ä¿æ‰€æœ‰ç»“æœéƒ½æœ‰queryå‘é‡ï¼ˆä»ç¼“å­˜ä¸­è·å–ï¼‰
    for i in range(len(data)):
        if i >= len(results):
            results.append({
                "query": data[i].get("query", ""),
                "document": data[i].get("document", ""),
                "query_vector": query_vector_cache.get(data[i].get("query", "")),
                "document_vector": None,
                "score": data[i].get("score")
            })
        elif results[i].get("query_vector") is None:
            # ä»ç¼“å­˜ä¸­è·å–queryå‘é‡
            query_text = data[i].get("query", "")
            if query_text in query_vector_cache:
                results[i]["query_vector"] = query_vector_cache[query_text]

    # éªŒè¯ç»“æœå®Œæ•´æ€§
    if len(results) != len(data):
        print(f"âš ï¸  è­¦å‘Š: ç»“æœæ•°é‡ ({len(results)}) ä¸æ•°æ®æ•°é‡ ({len(data)}) ä¸åŒ¹é…")

    # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
    total_api_time = query_api_time_total + doc_api_time_total
    final_total_tokens = cumulative_stats["total_tokens"]
    final_total_cost = cumulative_stats["total_cost"]

    performance_summary = {
        "total_items": len(data) * 2,  # query + document
        "query_items": len(data),
        "document_items": len(data),
        "total_api_time_seconds": round(total_api_time, 2),
        "query_api_time_seconds": round(query_api_time_total, 2),
        "document_api_time_seconds": round(doc_api_time_total, 2),
        "items_per_second": round(len(data) * 2 / total_api_time, 2) if total_api_time > 0 else 0,
        "token_usage": {
            "total_prompt_tokens": cumulative_stats["query_prompt_tokens"] + cumulative_stats["document_prompt_tokens"],
            "total_tokens": final_total_tokens,
            "query_prompt_tokens": cumulative_stats["query_prompt_tokens"],
            "query_total_tokens": cumulative_stats["query_total_tokens"],
            "document_prompt_tokens": cumulative_stats["document_prompt_tokens"],
            "document_total_tokens": cumulative_stats["document_total_tokens"],
            "avg_tokens_per_item": round(final_total_tokens / len(data) / 2, 2) if len(data) > 0 else 0
        },
        "cost": {
            "total_cost_usd": round(final_total_cost, 4),
            "query_cost_usd": round(calculate_cost(cumulative_stats["query_total_tokens"], model), 4),
            "document_cost_usd": round(calculate_cost(cumulative_stats["document_total_tokens"], model), 4),
            "price_per_million_tokens": get_model_pricing(model)
        },
        "model": model,
        "batch_size": batch_size,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    final_output = {
        "metadata": {
            "input_file": input_file,
            "model": model,
            "total_items": len(data),
            "processed_items": len(results),
            "vector_dimension": len(results[0]["query_vector"]) if results and results[0].get("query_vector") else 0,
            "performance": performance_summary
        },
        "results": results,
        "performance_log": performance_log
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

    # æ‰“å°æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ï¼ˆä»…APIè°ƒç”¨æ—¶é—´ï¼‰")
    print("=" * 60)
    print(f"æ€»å¤„ç†é¡¹æ•°: {len(data)} (query: {len(data)}, document: {len(data)})")
    print(f"æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"  - Query APIè°ƒç”¨: {query_api_time_total:.2f}s")
    print(f"  - Document APIè°ƒç”¨: {doc_api_time_total:.2f}s")
    print(f"å¤„ç†é€Ÿåº¦: {performance_summary['items_per_second']:.2f} é¡¹/ç§’")
    if len(data) > 0:
        avg_time_per_item = total_api_time / len(data)
        avg_query_time = query_api_time_total / len(data)
        avg_doc_time = doc_api_time_total / len(data)
        print(f"\nå¹³å‡æ¯æ¡æ•°æ®è€—æ—¶:")
        print(f"  æ€»è€—æ—¶: {avg_time_per_item:.4f}s/æ¡")
        print(f"  - Queryè€—æ—¶: {avg_query_time:.4f}s/æ¡")
        print(f"  - Documentè€—æ—¶: {avg_doc_time:.4f}s/æ¡")
    print(f"\nTokenæ¶ˆè€—ç»Ÿè®¡:")
    print(f"  æ€»è¾“å…¥Token: {performance_summary['token_usage']['total_prompt_tokens']:,}")
    print(f"  æ€»Token: {final_total_tokens:,}")
    print(f"  - Query Token: {cumulative_stats['query_total_tokens']:,}")
    print(f"  - Document Token: {cumulative_stats['document_total_tokens']:,}")
    print(f"  å¹³å‡æ¯é¡¹Token: {performance_summary['token_usage']['avg_tokens_per_item']:.1f}")
    print(f"\nè´¹ç”¨ç»Ÿè®¡:")
    print(f"  æ¨¡å‹: {model}")
    print(f"  å®šä»·: ${get_model_pricing(model):.4f} / ç™¾ä¸‡tokens")
    print(f"  æ€»è´¹ç”¨: ${final_total_cost:.4f}")
    print(f"  - Queryè´¹ç”¨: ${performance_summary['cost']['query_cost_usd']:.4f}")
    print(f"  - Documentè´¹ç”¨: ${performance_summary['cost']['document_cost_usd']:.4f}")
    print(f"\nå‘é‡ç»´åº¦: {len(results[0]['query_vector']) if results and results[0].get('query_vector') else 0}")
    print("=" * 60)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸ºQAæ•°æ®ç”Ÿæˆå‘é‡ï¼ˆqueryå’Œdocumentï¼‰- ä½¿ç”¨ Gemini API",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python vector/vectorize/gemini_text_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors_gemini.json

  # æŒ‡å®šæ‰¹é‡å¤§å°
  python vector/vectorize/gemini_text_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors_gemini.json -b 50

  # ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
  python vector/vectorize/gemini_text_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors_gemini.json --restart

  # è°ƒæ•´æ€§èƒ½æŠ¥å‘Šé—´éš”
  python vector/vectorize/gemini_text_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors_gemini.json -r 10

  # é™åˆ¶å¤„ç†æ¡æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  python vector/vectorize/gemini_text_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors_gemini.json --max-items 100
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«queryå’Œdocumentå­—æ®µï¼‰'
    )

    parser.add_argument(
        '-o', '--output',
        required=True,
        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«å‘é‡å’Œæ€§èƒ½æ•°æ®ï¼‰'
    )

    parser.add_argument(
        '-m', '--model',
        default='models/gemini-embedding-001',
        help='ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: models/gemini-embedding-001ï¼‰'
    )

    parser.add_argument(
        '-b', '--batch-size',
        type=int,
        default=100,
        help='æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé»˜è®¤: 100ï¼‰'
    )

    parser.add_argument(
        '--restart', '--from-scratch',
        dest='from_scratch',
        action='store_true',
        help='ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸­æ–­åæ¢å¤'
    )

    parser.add_argument(
        '-r', '--report-interval',
        type=int,
        default=5,
        help='æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼Œé»˜è®¤: 5ï¼‰'
    )

    parser.add_argument(
        '--max-items',
        type=int,
        default=None,
        help='æœ€å¤§å¤„ç†æ¡æ•°ï¼ˆé»˜è®¤: å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç”¨äºæµ‹è¯•æˆ–å°è§„æ¨¡è¿è¡Œï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    try:
        process_qa_data(
            input_file=args.input,
            output_file=args.output,
            model=args.model,
            batch_size=args.batch_size,
            from_scratch=args.from_scratch,
            report_interval=args.report_interval,
            max_items=args.max_items
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

