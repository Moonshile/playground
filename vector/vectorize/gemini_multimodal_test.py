"""
ä½¿ç”¨ Vertex AI Gemini å¤šæ¨¡æ€ Embeddings ç”Ÿæˆå‘é‡çš„æµ‹è¯•è„šæœ¬ã€‚
å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings-api?hl=zh-cn

ç”¨æ³•ç¤ºä¾‹ï¼š
    python vector/vectorize/gemini_multimodal_test.py -i input.json -o output.json --project YOUR_PROJECT --location us-central1
    python vector/vectorize/gemini_multimodal_test.py -i input.json -o output.json -m multimodalembedding@001 -r 5 --restart --max-items 10

è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰ï¼š
1. æ–°æ ¼å¼ï¼ˆQAæ•°æ®ï¼‰: {"query_list": [...], "document_list": [...]}
2. æ—§æ ¼å¼ï¼ˆå¤šæ¨¡æ€æ•°æ®ï¼‰: [
  {
    "id": "item-1",
    "text": "ä¸€æ®µæ–‡æœ¬",
    "image": "path/to/img.png",   # æ”¯æŒæœ¬åœ°è·¯å¾„æˆ– gs://
    "video": "path/to/video.mp4"  # æ”¯æŒæœ¬åœ°è·¯å¾„æˆ– gs://
  }
]
"""

import argparse
import base64
import json
import os
import time
from typing import Any, Dict, List, Optional, Tuple

from google.oauth2 import service_account
import vertexai
from vertexai.vision_models import Image, MultiModalEmbeddingModel, Video, VideoSegmentConfig

# é»˜è®¤æ¨¡å‹ä¸é‰´æƒèŒƒå›´
DEFAULT_MODEL = "multimodalembedding@001"
DEFAULT_SCOPES = ["https://www.googleapis.com/auth/cloud-platform"]
# è´¹ç”¨ï¼ˆæ¯ç™¾ä¸‡ tokenï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼šGEMINI_MULTI_PRICE_PER_M
DEFAULT_PRICE_PER_MILLION = float(os.getenv("GEMINI_MULTI_PRICE_PER_M", "2.0"))


# ==================== å·¥å…·ä¸é…ç½® ====================

def atomic_write(path: str, data: Dict[str, Any]) -> None:
    tmp_path = path + ".tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp_path, path)


def load_data(input_file: str) -> Dict[str, Any]:
    """
    åŠ è½½æ•°æ®
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ–°æ ¼å¼: {"query_list": [...], "document_list": [...]}
    2. æ—§æ ¼å¼: [{"id": ..., "text": ..., "image": ..., "video": ...}]
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æ£€æµ‹æ•°æ®æ ¼å¼
    if isinstance(data, dict) and "query_list" in data and "document_list" in data:
        # æ–°æ ¼å¼ï¼šè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        query_list = data.get("query_list", [])
        document_list = data.get("document_list", [])
        # åˆå¹¶ä¸ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« text å­—æ®µ
        items = []
        for query in query_list:
            items.append({"text": query})
        for document in document_list:
            items.append({"text": document})
        print(f"âœ… å·²åŠ è½½æ–°æ ¼å¼æ•°æ®: {len(query_list)} æ¡query + {len(document_list)} æ¡document = {len(items)} æ¡æ•°æ®")
        return {
            "format": "new",
            "data": items
        }
    elif isinstance(data, list):
        # æ—§æ ¼å¼ï¼šåˆ—è¡¨æ ¼å¼
        print(f"âœ… å·²åŠ è½½æ—§æ ¼å¼æ•°æ®: {len(data)} æ¡æ•°æ®")
        return {
            "format": "old",
            "data": data
        }
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼ã€‚æœŸæœ›æ ¼å¼: {{\"query_list\": [...], \"document_list\": [...]}} æˆ– [{{\"text\": ..., \"image\": ..., \"video\": ...}}]")


def load_credentials():
    """
    ä»ç¯å¢ƒå˜é‡è·å–æœåŠ¡è´¦å·å‡­è¯ï¼š
    - GOOGLE_APPLICATION_CREDENTIALS: JSON æ–‡ä»¶è·¯å¾„
    - GOOGLE_SERVICE_ACCOUNT_JSON: JSON å­—ç¬¦ä¸²
    - GOOGLE_SERVICE_ACCOUNT_JSON_B64: Base64 ç¼–ç çš„ JSON
    è‹¥å‡ä¸å­˜åœ¨åˆ™è¿”å› Noneï¼Œäº¤ç”±é»˜è®¤å‡­è¯ï¼ˆéœ€æœ¬åœ°å·² gcloud auth application-default loginï¼‰ã€‚
    """
    path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
    json_b64 = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON_B64")

    if path:
        if not os.path.exists(path):
            raise FileNotFoundError(f"GOOGLE_APPLICATION_CREDENTIALS æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return service_account.Credentials.from_service_account_file(path, scopes=DEFAULT_SCOPES)

    if json_str:
        info = json.loads(json_str)
        return service_account.Credentials.from_service_account_info(info, scopes=DEFAULT_SCOPES)

    if json_b64:
        decoded = base64.b64decode(json_b64).decode("utf-8")
        info = json.loads(decoded)
        return service_account.Credentials.from_service_account_info(info, scopes=DEFAULT_SCOPES)

    return None


def init_vertex(project: str, location: str, credentials=None) -> None:
    vertexai.init(project=project, location=location, credentials=credentials)


def load_media(path: Optional[str], is_video: bool = False):
    if not path:
        return None
    if path.startswith("gs://"):
        return Video.load_from_file(path) if is_video else Image.load_from_file(path)
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    return Video.load_from_file(path) if is_video else Image.load_from_file(path)


# ==================== å‘é‡åŒ–æ ¸å¿ƒ ====================

def embed_item(
    model: MultiModalEmbeddingModel,
    item: Dict[str, Any],
    video_segment_config: Optional[VideoSegmentConfig] = None,
) -> Dict[str, Any]:
    # å…¼å®¹æ—  text çš„æ•°æ®ï¼Œä¼˜å…ˆ textï¼Œå…¶æ¬¡ query/document
    text = item.get("text") or item.get("query") or item.get("document") or ""
    image_path = item.get("image")
    video_path = item.get("video")

    image_obj = load_media(image_path, is_video=False) if image_path else None
    video_obj = load_media(video_path, is_video=True) if video_path else None

    # å¦‚æœä¸‰è€…éƒ½ä¸ºç©ºï¼Œè¿”å› None ç”±ä¸Šå±‚è·³è¿‡
    if not text and not image_obj and not video_obj:
        return None

    # ç²—ç•¥ä¼°ç®— tokenï¼Œä»…å¯¹æ–‡æœ¬ï¼›å›¾åƒ/è§†é¢‘ä¸è®¡å…¥ï¼ˆç¼ºå°‘å®˜æ–¹è®¡è´¹å£å¾„ï¼‰
    estimated_tokens = int(len(text) * 0.25) if text else 0

    api_start = time.time()
    embeddings = model.get_embeddings(
        image=image_obj,
        video=video_obj,
        video_segment_config=video_segment_config,
        contextual_text=text if text else None,
        dimension=1408,
        # æ–‡æ¡£ç¤ºä¾‹ä½¿ç”¨ text_embeddingï¼›è¿™é‡Œä»æä¾› contextual_text ä»¥ç”Ÿæˆ text embedding
    )
    api_time = time.time() - api_start

    result: Dict[str, Any] = {
        "id": item.get("id"),
        "text": text,
        "image": image_path,
        "video": video_path,
        "api_time_seconds": api_time,
        "estimated_tokens": estimated_tokens,
    }

    # æå– embedding
    if hasattr(embeddings, "text_embedding"):
        result["text_embedding"] = embeddings.text_embedding
    if hasattr(embeddings, "image_embedding") and embeddings.image_embedding:
        result["image_embedding"] = embeddings.image_embedding
    if hasattr(embeddings, "video_embeddings") and embeddings.video_embeddings:
        result["video_embeddings"] = [
            {
                "embedding": ve.embedding,
                "start_offset_sec": ve.start_offset_sec,
                "end_offset_sec": ve.end_offset_sec,
            }
            for ve in embeddings.video_embeddings
        ]

    # ç»Ÿè®¡å‘é‡ç»´åº¦
    result["dimensions"] = {
        "text": len(result["text_embedding"]) if "text_embedding" in result else 0,
        "image": len(result["image_embedding"]) if "image_embedding" in result else 0,
        "video": (
            len(result["video_embeddings"][0]["embedding"])
            if result.get("video_embeddings")
            else 0
        ),
    }

    return result


# ==================== æ£€æŸ¥ç‚¹ä¸æŠ¥å‘Š ====================

def load_checkpoint(output_file: str) -> Dict[str, Any]:
    ckpt_path = output_file.replace(".json", "_checkpoint.json")
    if not os.path.exists(ckpt_path):
        return {
            "processed": 0,
            "results": [],
            "performance": [],
            "cumulative_api_time": 0.0,
            "cumulative_stats": {
                "prompt_tokens": 0,
                "total_tokens": 0,
                "total_cost": 0.0,
            },
            "vector_cache": {},  # ç”¨äºå­˜å‚¨å»é‡åçš„å‘é‡
        }
    try:
        with open(ckpt_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        backup = ckpt_path + ".corrupted"
        os.replace(ckpt_path, backup)
        print(f"âš ï¸ æ£€æŸ¥ç‚¹æŸåï¼Œå·²å¤‡ä»½åˆ° {backup}ï¼Œå°†ä»å¤´å¼€å§‹")
        return {
            "processed": 0,
            "results": [],
            "performance": [],
            "cumulative_api_time": 0.0,
            "cumulative_stats": {
                "prompt_tokens": 0,
                "total_tokens": 0,
                "total_cost": 0.0,
            },
            "vector_cache": {},  # ç”¨äºå­˜å‚¨å»é‡åçš„å‘é‡
        }


def save_checkpoint(output_file: str, checkpoint: Dict[str, Any]) -> None:
    ckpt_path = output_file.replace(".json", "_checkpoint.json")
    atomic_write(ckpt_path, checkpoint)


def print_report(perf_log: List[Dict[str, Any]], total: int, model: str) -> None:
    if not perf_log:
        return
    last = perf_log[-1]
    print(f"ğŸ“Š å·²å¤„ç†: {last['processed']}/{total}, "
          f"APIè€—æ—¶: {last['api_time']:.2f}s, "
          f"ç´¯è®¡è€—æ—¶: {last['cumulative_api_time']:.2f}s, "
          f"é€Ÿåº¦: {last['items_per_sec']:.2f} é¡¹/ç§’, "
          f"Token: {last['total_tokens']:,}, è´¹ç”¨: ${last['total_cost']:.4f}, "
          f"æ¨¡å‹: {model}")


# ==================== å»é‡å·¥å…·å‡½æ•° ====================

def get_item_key(item: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆ item çš„å”¯ä¸€é”®ï¼Œç”¨äºå»é‡
    åŸºäº text/query/document + image + video çš„ç»„åˆ
    """
    text = item.get("text") or item.get("query") or item.get("document") or ""
    image = item.get("image") or ""
    video = item.get("video") or ""
    # ä½¿ç”¨ç»„åˆé”®ç¡®ä¿å”¯ä¸€æ€§
    return f"{text}|{image}|{video}"


# ==================== ä¸»æµç¨‹ ====================

def process(
    input_file: str,
    output_file: str,
    project: str,
    location: str,
    model: str = DEFAULT_MODEL,
    batch_size: int = 1,
    from_scratch: bool = False,
    report_interval: int = 10,
    max_items: Optional[int] = None,
):
    credentials = load_credentials()
    if credentials:
        print("ğŸ”‘ ä½¿ç”¨æœåŠ¡è´¦å·å‡­è¯åˆå§‹åŒ– Vertex AI")
    else:
        print("â„¹ï¸ æœªæä¾›æœåŠ¡è´¦å·å‡­è¯ï¼Œå°†å°è¯•é»˜è®¤å‡­è¯ï¼ˆéœ€å·²é…ç½® ADCï¼‰")

    init_vertex(project, location, credentials=credentials)
    model_name = model if model else DEFAULT_MODEL
    mm_model = MultiModalEmbeddingModel.from_pretrained(model_name)

    loaded_data = load_data(input_file)
    data = loaded_data["data"]
    if max_items:
        data = data[:max_items]

    if from_scratch:
        checkpoint = {
            "processed": 0,
            "results": [],
            "performance": [],
            "cumulative_api_time": 0.0,
        }
        print("ğŸš€ ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰")
    else:
        checkpoint = load_checkpoint(output_file)

    processed = checkpoint.get("processed", 0)
    results: List[Dict[str, Any]] = checkpoint.get("results", [])
    performance: List[Dict[str, Any]] = checkpoint.get("performance", [])
    cumulative_api = checkpoint.get("cumulative_api_time", 0.0)
    cumulative_stats = checkpoint.get("cumulative_stats", {
        "prompt_tokens": 0,
        "total_tokens": 0,
        "total_cost": 0.0,
    })
    vector_cache = checkpoint.get("vector_cache", {})
    start_index = processed

    print(f"âœ… æ¢å¤è¿›åº¦: å·²å¤„ç† {processed} æ¡ï¼Œå‰©ä½™ {len(data) - processed} æ¡")

    # å»é‡åˆ†æï¼šæ„å»ºå»é‡æ˜ å°„
    remaining_data = data[start_index:]
    print(f"ğŸ” å»é‡åˆ†æ: åŸå§‹ {len(remaining_data)} æ¡")
    unique_items = {}
    item_to_unique_map = []  # åŸå§‹ç´¢å¼• -> å”¯ä¸€itemçš„æ˜ å°„
    for i, item in enumerate(remaining_data):
        item_key = get_item_key(item)
        if item_key not in unique_items:
            unique_items[item_key] = len(unique_items)
        item_to_unique_map.append(unique_items[item_key])

    unique_item_list = list(unique_items.keys())
    print(f"   å»é‡å: {len(unique_item_list)} æ¡å”¯ä¸€item (èŠ‚çœ {len(remaining_data) - len(unique_item_list)} æ¬¡APIè°ƒç”¨)")

    # æ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„å‘é‡
    cached_count = 0
    unique_item_vectors = {}  # å­˜å‚¨å”¯ä¸€itemçš„å‘é‡
    for item_key in unique_item_list:
        if item_key in vector_cache:
            unique_item_vectors[item_key] = vector_cache[item_key]
            cached_count += 1

    if cached_count > 0:
        print(f"   ğŸ’¾ ä»ç¼“å­˜ä¸­æ¢å¤ {cached_count} ä¸ªå‘é‡")

    # æ„å»ºå”¯ä¸€itemåˆ—è¡¨ï¼ˆä»åŸå§‹æ•°æ®ä¸­æå–ï¼‰
    unique_item_data = []
    seen_keys = set()
    for item in remaining_data:
        item_key = get_item_key(item)
        if item_key not in seen_keys:
            unique_item_data.append(item)
            seen_keys.add(item_key)

    # åªå¤„ç†æœªç¼“å­˜çš„å”¯ä¸€item
    uncached_unique_items = []
    uncached_indices = []
    for i, item in enumerate(unique_item_data):
        item_key = get_item_key(item)
        if item_key not in unique_item_vectors:
            uncached_unique_items.append(item)
            uncached_indices.append(start_index + i)

    skipped = 0
    last_batch_was_reported = False

    # å…ˆå¤„ç†æœªç¼“å­˜çš„å”¯ä¸€item
    api_calls_made = 0
    for i, item in enumerate(uncached_unique_items):
        try:
            embedded = embed_item(mm_model, item, video_segment_config=VideoSegmentConfig(end_offset_sec=1))

            # å¦‚æœæ•°æ®ä¸ºç©ºï¼ˆæ—  text/image/videoï¼‰ï¼Œè·³è¿‡ä½†ä¸æŠ¥é”™
            if embedded is None:
                skipped += 1
                continue

            item_key = get_item_key(item)
            unique_item_vectors[item_key] = embedded
            vector_cache[item_key] = embedded
            api_calls_made += 1

            cumulative_api += embedded["api_time_seconds"]

            # æ›´æ–°ç´¯è®¡ token / cost
            estimated_tokens = embedded.get("estimated_tokens", 0)
            cumulative_stats["prompt_tokens"] += estimated_tokens
            cumulative_stats["total_tokens"] += estimated_tokens
            cumulative_stats["total_cost"] = (cumulative_stats["total_tokens"] / 1_000_000) * DEFAULT_PRICE_PER_MILLION

        except Exception as e:
            print(f"âŒ å¤„ç†å”¯ä¸€itemå¤±è´¥: {e}")
            save_checkpoint(output_file, {
                "processed": processed,
                "results": results,
                "performance": performance,
                "cumulative_api_time": cumulative_api,
                "vector_cache": vector_cache,
                "model": model_name,
                "error": str(e),
            })
            raise

    # å¤„ç†æ‰€æœ‰å‰©ä½™æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æˆ–å·²ç”Ÿæˆçš„å‘é‡
    for idx in range(start_index, len(data)):
        item = data[idx]
        item_key = get_item_key(item)

        # ä»ç¼“å­˜æˆ–å·²ç”Ÿæˆçš„å‘é‡ä¸­è·å–
        if item_key in unique_item_vectors:
            embedded = unique_item_vectors[item_key]
        elif item_key in vector_cache:
            embedded = vector_cache[item_key]
            unique_item_vectors[item_key] = embedded
        else:
            # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
            continue

        # ç¡®ä¿resultsåˆ—è¡¨è¶³å¤Ÿé•¿
        while len(results) <= idx:
            results.append(None)
        results[idx] = embedded

        processed = idx + 1

        perf = {
            "processed": processed,
            "api_time": embedded.get("api_time_seconds", 0.0),
            "cumulative_api_time": cumulative_api,
            "items_per_sec": processed / cumulative_api if cumulative_api > 0 else 0,
            "prompt_tokens": cumulative_stats["prompt_tokens"],
            "total_tokens": cumulative_stats["total_tokens"],
            "total_cost": cumulative_stats["total_cost"],
        }
        performance.append(perf)

        # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
        if processed % report_interval == 0:
            print_report(performance, len(data), model_name)
            save_checkpoint(output_file, {
                "processed": processed,
                "results": results,
                "performance": performance,
                "cumulative_api_time": cumulative_api,
                "cumulative_stats": cumulative_stats,
                "vector_cache": vector_cache,
                "model": model_name,
            })
            last_batch_was_reported = True
        elif idx == len(data) - 1:
            # æœ€åä¸€æ¡ï¼Œå³ä½¿ä¸æ»¡è¶³æŠ¥å‘Šé—´éš”ä¹Ÿè¦æ ‡è®°
            last_batch_was_reported = False

    if api_calls_made == 0 and cached_count > 0:
        print(f"   âœ… æ‰€æœ‰å‘é‡å·²ä»ç¼“å­˜ä¸­æ¢å¤ï¼Œæ— éœ€APIè°ƒç”¨")

    # å¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    # å¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    if not last_batch_was_reported and processed > 0:
        print_report(performance, len(data), model_name)
        save_checkpoint(output_file, {
            "processed": processed,
            "results": results,
            "performance": performance,
            "cumulative_api_time": cumulative_api,
            "cumulative_stats": cumulative_stats,
            "vector_cache": vector_cache,
            "model": model_name,
        })

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_api_time = cumulative_api
    effective_items = processed - skipped if processed > skipped else 0
    items_per_sec = effective_items / total_api_time if total_api_time > 0 else 0
    avg_time_per_item = total_api_time / effective_items if effective_items > 0 else 0.0

    # ç»ˆæ€æˆæœ¬/Token
    total_tokens = cumulative_stats["total_tokens"]
    total_cost = cumulative_stats["total_cost"]
    avg_tokens_per_item = total_tokens / effective_items if effective_items > 0 else 0

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    final_output = {
        "metadata": {
            "input_file": input_file,
            "model": model_name,
            "project": project,
            "location": location,
            "total_items": len(data),
            "processed": processed,
            "skipped_empty": skipped,
            "batch_size": batch_size,
            "report_interval": report_interval,
        },
        "results": results,
        "performance_log": performance,
        "performance_summary": {
            "total_api_time_seconds": round(total_api_time, 2),
            "items_per_second": round(items_per_sec, 2),
            "avg_time_per_item_seconds": round(avg_time_per_item, 4),
            "token_usage": {
                "total_prompt_tokens": cumulative_stats["prompt_tokens"],
                "total_tokens": total_tokens,
                "avg_tokens_per_item": round(avg_tokens_per_item, 2),
            },
            "cost": {
                "total_cost_usd": round(total_cost, 4),
                "price_per_million_tokens": DEFAULT_PRICE_PER_MILLION,
            },
        },
    }
    atomic_write(output_file, final_output)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

    # æ‰“å°æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ï¼ˆä¸å…¶ä»–ä¸¤ä¸ªè„šæœ¬æ ¼å¼ä¸€è‡´ï¼‰
    print("\n" + "=" * 60)
    print("æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ï¼ˆä»…APIè°ƒç”¨æ—¶é—´ï¼‰")
    print("=" * 60)
    print(f"æ€»å¤„ç†é¡¹æ•°: {len(data)}")
    if skipped > 0:
        print(f"  æœ‰æ•ˆå¤„ç†: {effective_items} æ¡")
        print(f"  è·³è¿‡ç©ºé¡¹: {skipped} æ¡")
    print(f"æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"å¤„ç†é€Ÿåº¦: {items_per_sec:.2f} é¡¹/ç§’")
    if effective_items > 0:
        print(f"\nå¹³å‡æ¯æ¡æ•°æ®è€—æ—¶:")
        print(f"  æ€»è€—æ—¶: {avg_time_per_item:.4f}s/æ¡")
    print(f"\nTokenæ¶ˆè€—ç»Ÿè®¡:")
    print(f"  æ€»è¾“å…¥Token: {cumulative_stats['prompt_tokens']:,}")
    print(f"  æ€»Token: {total_tokens:,}")
    print(f"  å¹³å‡æ¯é¡¹Token: {avg_tokens_per_item:.1f}")
    print(f"\nè´¹ç”¨ç»Ÿè®¡:")
    print(f"  æ¨¡å‹: {model_name}")
    print(f"  å®šä»·: ${DEFAULT_PRICE_PER_MILLION:.4f} / ç™¾ä¸‡tokens")
    print(f"  æ€»è´¹ç”¨: ${total_cost:.4f}")
    # æ˜¾ç¤ºå‘é‡ç»´åº¦ï¼ˆä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆç»“æœä¸­è·å–ï¼‰
    if results:
        first_result = next((r for r in results if r.get("text_embedding") or r.get("image_embedding")), None)
        if first_result:
            dims = first_result.get("dimensions", {})
            text_dim = dims.get("text", 0)
            image_dim = dims.get("image", 0)
            video_dim = dims.get("video", 0)
            if text_dim > 0:
                print(f"\nå‘é‡ç»´åº¦:")
                print(f"  æ–‡æœ¬å‘é‡: {text_dim}")
                if image_dim > 0:
                    print(f"  å›¾åƒå‘é‡: {image_dim}")
                if video_dim > 0:
                    print(f"  è§†é¢‘å‘é‡: {video_dim}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="æµ‹è¯• Vertex AI Gemini å¤šæ¨¡æ€ Embeddingsï¼ˆæ–‡æœ¬/å›¾ç‰‡/è§†é¢‘ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  python vector/vectorize/gemini_multimodal_test.py -i input.json -o output.json --project YOUR_PROJECT --location us-central1
  python vector/vectorize/gemini_multimodal_test.py -i input.json -o output.json --max-items 20 -r 5
        """
    )
    parser.add_argument("-i", "--input", required=True, help="è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", required=True, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--project",
        required=False,
        help="GCP é¡¹ç›® IDï¼ˆä¼˜å…ˆå‘½ä»¤è¡Œï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ GOOGLE_CLOUD_PROJECT/PROJECT_ID/GCP_PROJECTï¼‰",
    )
    parser.add_argument(
        "--location",
        required=False,
        help="Vertex AI åŒºåŸŸï¼ˆä¼˜å…ˆå‘½ä»¤è¡Œï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ GOOGLE_CLOUD_LOCATION/VERTEX_LOCATIONï¼Œé»˜è®¤ us-central1ï¼‰",
    )
    parser.add_argument(
        "-m", "--model",
        default=DEFAULT_MODEL,
        help=f"ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: {DEFAULT_MODEL}ï¼‰"
    )
    parser.add_argument(
        "-b", "--batch-size",
        type=int,
        default=1,
        help="æ‰¹é‡å¤§å°ï¼ˆå¤šæ¨¡æ€æ¥å£æŒ‰æ¡å¤„ç†ï¼Œé»˜è®¤1ï¼‰"
    )
    parser.add_argument(
        "--restart", "--from-scratch",
        dest="from_scratch",
        action="store_true",
        help="ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹"
    )
    parser.add_argument("-r", "--report-interval", type=int, default=10, help="æ€§èƒ½æŠ¥å‘Šé—´éš”ï¼ˆæ¡ï¼‰")
    parser.add_argument("--max-items", type=int, default=None, help="æœ€å¤§å¤„ç†æ¡æ•°ï¼ˆç”¨äºæŠ½æ ·æµ‹è¯•ï¼‰")

    args = parser.parse_args()

    # ç¯å¢ƒå˜é‡å…œåº•
    env_project = os.getenv("GOOGLE_CLOUD_PROJECT") or os.getenv("PROJECT_ID") or os.getenv("GCP_PROJECT")
    env_location = os.getenv("GOOGLE_CLOUD_LOCATION") or os.getenv("VERTEX_LOCATION")

    project = args.project or env_project
    location = args.location or env_location or "us-central1"

    if not project:
        raise ValueError("è¯·é€šè¿‡ --project æˆ–ç¯å¢ƒå˜é‡ GOOGLE_CLOUD_PROJECT / PROJECT_ID / GCP_PROJECT è®¾ç½®é¡¹ç›® ID")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_dir = os.path.dirname(args.output)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    process(
        input_file=args.input,
        output_file=args.output,
        project=project,
        location=location,
        model=args.model,
        batch_size=args.batch_size,
        from_scratch=args.from_scratch,
        report_interval=args.report_interval,
        max_items=args.max_items,
    )


if __name__ == "__main__":
    main()

