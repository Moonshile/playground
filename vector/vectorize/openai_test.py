"""
ä½¿ç”¨ OpenAI Embeddings API å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
æ”¯æŒæ ‡å‡† API å’Œ Batch APIï¼ŒåŒ…å«æ€§èƒ½ç»Ÿè®¡å’Œè´¹ç”¨è®¡ç®—

ä½¿ç”¨æ–¹æ³•:
    python vector/vectorize/openai_test.py -i input.json -o output.json
    python vector/vectorize/openai_test.py -i input.json -o output.json -m text-embedding-3-large -b 50 -r 10
"""
import os
import json
import time
import argparse
from typing import List, Optional, Dict, Any, Tuple
from openai import OpenAI

# ==================== é…ç½® ====================

# OpenAI Embeddings æ¨¡å‹å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰
EMBEDDING_PRICING = {
    "text-embedding-3-small": 0.02,  # $0.02 per 1M tokens
    "text-embedding-3-large": 0.13,  # $0.13 per 1M tokens
    "text-embedding-ada-002": 0.10,  # $0.10 per 1M tokens (æ—§æ¨¡å‹)
}


# ==================== å·¥å…·å‡½æ•° ====================

def get_openai_client() -> OpenAI:
    """è·å– OpenAI å®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡è¯»å– API key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    return OpenAI(api_key=api_key)


def get_model_pricing(model: str) -> float:
    """è·å–æ¨¡å‹çš„å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰"""
    return EMBEDDING_PRICING.get(model, 0.02)  # é»˜è®¤ä½¿ç”¨text-embedding-3-smallçš„å®šä»·


def calculate_cost(tokens: int, model: str) -> float:
    """
    è®¡ç®—è´¹ç”¨ï¼ˆç¾å…ƒï¼‰

    Args:
        tokens: tokenæ•°é‡
        model: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        è´¹ç”¨ï¼ˆç¾å…ƒï¼‰
    """
    price_per_million = get_model_pricing(model)
    return (tokens / 1_000_000) * price_per_million


# ==================== å‘é‡åŒ–å‡½æ•° ====================

def vectorize_text(
    text: str,
    model: str = "text-embedding-3-small",
    client: Optional[OpenAI] = None
) -> List[float]:
    """
    ä½¿ç”¨ OpenAI Embeddings API å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        text: è¦è½¬æ¢çš„æ–‡æœ¬
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "text-embedding-3-small"
        client: OpenAI å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º

    Returns:
        å‘é‡åˆ—è¡¨ï¼ˆæµ®ç‚¹æ•°åˆ—è¡¨ï¼‰
    """
    if client is None:
        client = get_openai_client()

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    response = client.embeddings.create(model=model, input=text)
    return response.data[0].embedding


def vectorize_texts_batch(
    texts: List[str],
    model: str = "text-embedding-3-small",
    client: Optional[OpenAI] = None,
    batch_size: int = 100
) -> Tuple[List[List[float]], float, Dict[str, int]]:
    """
    ä½¿ç”¨ OpenAI Embeddings API æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        texts: è¦è½¬æ¢çš„æ–‡æœ¬åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "text-embedding-3-small"
        client: OpenAI å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º
        batch_size: æ¯æ¬¡è¯·æ±‚çš„æ‰¹é‡å¤§å°ï¼ˆOpenAI API æ”¯æŒæœ€å¤š 2048 ä¸ªè¾“å…¥ï¼‰

    Returns:
        (å‘é‡åˆ—è¡¨, APIè°ƒç”¨æ€»æ—¶é—´ï¼ˆç§’ï¼‰, tokenä½¿ç”¨ä¿¡æ¯å­—å…¸)
        tokenä½¿ç”¨ä¿¡æ¯åŒ…å«: prompt_tokens, total_tokens
    """
    if client is None:
        client = get_openai_client()

    if not texts:
        return [], 0.0, {"prompt_tokens": 0, "total_tokens": 0}

    # è¿‡æ»¤ç©ºæ–‡æœ¬
    valid_texts = [(i, text) for i, text in enumerate(texts) if text and text.strip()]
    if not valid_texts:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬")

    all_vectors = [None] * len(texts)
    total_api_time = 0.0
    total_prompt_tokens = 0
    total_tokens = 0

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(valid_texts), batch_size):
        batch = valid_texts[i:i + batch_size]
        batch_texts = [text for _, text in batch]
        batch_indices = [idx for idx, _ in batch]

        # åªè®°å½•APIè°ƒç”¨æ—¶é—´
        api_start = time.time()
        response = client.embeddings.create(model=model, input=batch_texts)
        api_end = time.time()
        api_time = api_end - api_start
        total_api_time += api_time

        # æå–tokenä½¿ç”¨ä¿¡æ¯
        if hasattr(response, 'usage') and response.usage:
            total_prompt_tokens += response.usage.prompt_tokens
            total_tokens += response.usage.total_tokens

        # å°†ç»“æœæ”¾å›åŸä½ç½®
        for idx, embedding in zip(batch_indices, response.data):
            all_vectors[idx] = embedding.embedding

    return all_vectors, total_api_time, {
        "prompt_tokens": total_prompt_tokens,
        "total_tokens": total_tokens
    }


# ==================== æ•°æ®åŠ è½½å’Œæ£€æŸ¥ç‚¹ ====================

def load_qa_data(input_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½QAæ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(data)} æ¡æ•°æ®")
    return data


def get_default_cumulative_stats() -> Dict[str, Any]:
    """è·å–é»˜è®¤çš„ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "query_prompt_tokens": 0,
        "query_total_tokens": 0,
        "document_prompt_tokens": 0,
        "document_total_tokens": 0,
        "total_tokens": 0,
        "total_cost": 0.0
    }


def load_checkpoint(output_file: str) -> Dict[str, Any]:
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå·²å¤„ç†çš„æ•°æ®ï¼ŒåŒ…å«æ€§èƒ½å’Œtokenç»Ÿè®¡ï¼‰"""
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        print(f"ğŸ“‚ å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)
            processed_count = checkpoint.get('processed_count', 0)
            print(f"   å·²å¤„ç†: {processed_count} æ¡")

            # æ¢å¤ç´¯è®¡çš„tokenç»Ÿè®¡
            cumulative_stats = checkpoint.get('cumulative_stats', get_default_cumulative_stats())
            if cumulative_stats.get("total_tokens", 0) > 0:
                print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
                print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

            return checkpoint
        except json.JSONDecodeError as e:
            print(f"   âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ˆå¯èƒ½ä¿å­˜æ—¶è¢«ä¸­æ–­ï¼‰: {e}")
            print(f"   ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶å¯èƒ½å·²æŸåï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            # å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            backup_file = checkpoint_file + '.corrupted'
            try:
                import shutil
                shutil.move(checkpoint_file, backup_file)
                print(f"   ğŸ“¦ å·²å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°: {backup_file}")
            except Exception as backup_error:
                print(f"   âš ï¸  æ— æ³•å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {backup_error}")
            # è¿”å›ç©ºçš„æ£€æŸ¥ç‚¹
            return {
                "processed_count": 0,
                "results": [],
                "performance": [],
                "cumulative_stats": get_default_cumulative_stats()
            }
        except Exception as e:
            print(f"   âŒ åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            print(f"   ğŸ’¡ å°†ä»å¤´å¼€å§‹å¤„ç†")
            return {
                "processed_count": 0,
                "results": [],
                "performance": [],
                "cumulative_stats": get_default_cumulative_stats()
            }

    return {
        "processed_count": 0,
        "results": [],
        "performance": [],
        "cumulative_stats": get_default_cumulative_stats()
    }


def save_checkpoint(output_file: str, checkpoint: Dict[str, Any], verbose: bool = False):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä½¿ç”¨åŸå­æ€§å†™å…¥ï¼Œé¿å…æ–‡ä»¶æŸåï¼‰

    ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ + é‡å‘½åçš„æ–¹å¼ï¼Œç¡®ä¿å†™å…¥è¿‡ç¨‹çš„åŸå­æ€§ï¼š
    1. å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
    2. å†™å…¥æˆåŠŸåå†é‡å‘½åæ›¿æ¢åŸæ–‡ä»¶
    è¿™æ ·å³ä½¿å†™å…¥è¿‡ç¨‹ä¸­è¢«ä¸­æ–­ï¼ŒåŸæ–‡ä»¶ä¹Ÿä¸ä¼šè¢«æŸå
    """
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    temp_file = checkpoint_file + '.tmp'

    try:
        # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, indent=2, ensure_ascii=False)

        # å†™å…¥æˆåŠŸåå†é‡å‘½åï¼ˆåŸå­æ“ä½œï¼‰
        # åœ¨å¤§å¤šæ•°æ–‡ä»¶ç³»ç»Ÿä¸Šï¼Œé‡å‘½åæ˜¯åŸå­æ“ä½œ
        os.replace(temp_file, checkpoint_file)

        if verbose:
            processed = checkpoint.get('processed_count', 0)
            print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {processed} æ¡å·²å¤„ç†")
    except Exception as e:
        # å¦‚æœå†™å…¥å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“ä¿å­˜å¤±è´¥
        raise RuntimeError(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}") from e


def update_cumulative_stats(
    cumulative_stats: Dict[str, Any],
    token_info: Dict[str, int],
    stats_type: str,
    model: str
):
    """æ›´æ–°ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    cumulative_stats[f"{stats_type}_prompt_tokens"] += token_info.get("prompt_tokens", 0)
    cumulative_stats[f"{stats_type}_total_tokens"] += token_info.get("total_tokens", 0)
    cumulative_stats["total_tokens"] = (
        cumulative_stats["query_total_tokens"] +
        cumulative_stats["document_total_tokens"]
    )
    cumulative_stats["total_cost"] = calculate_cost(cumulative_stats["total_tokens"], model)


# ==================== æ€§èƒ½æŠ¥å‘Š ====================

def print_performance_report(
    performance_log: List[Dict[str, Any]],
    processed_count: int,
    total_count: int,
    report_type: str,
    model: str,
    cumulative_stats: Optional[Dict[str, Any]] = None
):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Šï¼ˆåŒ…å«tokenç»Ÿè®¡å’Œè´¹ç”¨ï¼‰"""
    if not performance_log:
        return

    # ç­›é€‰æŒ‡å®šç±»å‹çš„æ€§èƒ½æ•°æ®
    type_logs = [log for log in performance_log if log.get("type") == report_type]
    if not type_logs:
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_api_time = sum(log.get("time_seconds", 0) for log in type_logs)
    total_items = sum(log.get("batch_size", 0) for log in type_logs)
    total_prompt_tokens = sum(log.get("prompt_tokens", 0) for log in type_logs)
    total_tokens = sum(log.get("total_tokens", 0) for log in type_logs)
    avg_time_per_batch = total_api_time / len(type_logs) if type_logs else 0
    avg_items_per_second = total_items / total_api_time if total_api_time > 0 else 0
    avg_tokens_per_item = total_tokens / total_items if total_items > 0 else 0

    # è®¡ç®—è´¹ç”¨
    cost = calculate_cost(total_tokens, model)

    # ç´¯è®¡ç»Ÿè®¡ï¼ˆå¦‚æœæä¾›ï¼‰
    cumulative_tokens = cumulative_stats.get("total_tokens", 0) if cumulative_stats else 0
    cumulative_cost = cumulative_stats.get("total_cost", 0.0) if cumulative_stats else 0.0

    print(f"\nğŸ“Š {report_type.upper()} æ€§èƒ½æŠ¥å‘Š:")
    print(f"   å·²å¤„ç†æ‰¹æ¬¡: {len(type_logs)}")
    print(f"   æ€»å¤„ç†é¡¹æ•°: {total_items}")
    print(f"   æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"   å¹³å‡æ¯æ‰¹æ¬¡æ—¶é—´: {avg_time_per_batch:.2f}s")
    print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {avg_items_per_second:.2f} é¡¹/ç§’")
    print(f"   Tokenç»Ÿè®¡:")
    print(f"     - æ€»è¾“å…¥Token: {total_prompt_tokens:,}")
    print(f"     - æ€»Token: {total_tokens:,}")
    print(f"     - å¹³å‡æ¯é¡¹Token: {avg_tokens_per_item:.1f}")
    print(f"   è´¹ç”¨ç»Ÿè®¡:")
    print(f"     - æœ¬æ¬¡è´¹ç”¨: ${cost:.4f}")
    if cumulative_stats:
        print(f"     - ç´¯è®¡Token: {cumulative_tokens:,}")
        print(f"     - ç´¯è®¡è´¹ç”¨: ${cumulative_cost:.4f}")
    print(f"   æ€»è¿›åº¦: {processed_count}/{total_count} ({processed_count/total_count*100:.1f}%)")
    print()


# ==================== ä¸»å¤„ç†å‡½æ•° ====================

def process_batch(
    texts: List[str],
    text_type: str,
    batch_index: int,
    model: str,
    client: OpenAI,
    batch_size: int,
    processed_count: int,
    total_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    report_interval: int
) -> Tuple[List[List[float]], float, int, int, bool]:
    """
    å¤„ç†ä¸€æ‰¹æ–‡æœ¬ï¼Œè¿”å›å‘é‡ã€APIæ—¶é—´ã€tokenç»Ÿè®¡

    Returns:
        (å‘é‡åˆ—è¡¨, APIæ€»æ—¶é—´, prompt_tokens, total_tokens, should_report)
        should_report: æ˜¯å¦éœ€è¦æ‰“å°æ€§èƒ½æŠ¥å‘Š
    """
    # APIè°ƒç”¨å‰æç¤ºï¼ˆè®©ç”¨æˆ·çŸ¥é“ç¨‹åºè¿˜åœ¨è¿è¡Œï¼‰
    if batch_index == 0 or (batch_index + 1) % 10 == 0:
        print(f"   â³ æ­£åœ¨è°ƒç”¨APIå¤„ç† {text_type}æ‰¹æ¬¡ {batch_index + 1}...")

    batch_vectors, api_time, token_info = vectorize_texts_batch(
        texts, model=model, client=client, batch_size=batch_size
    )

    # è®°å½•æ€§èƒ½
    batch_perf = {
        "type": text_type,
        "batch_index": batch_index,
        "batch_size": len(texts),
        "time_seconds": round(api_time, 2),
        "items_per_second": round(len(texts) / api_time, 2) if api_time > 0 else 0,
        "prompt_tokens": token_info.get("prompt_tokens", 0),
        "total_tokens": token_info.get("total_tokens", 0)
    }
    performance_log.append(batch_perf)

    # æ›´æ–°ç´¯è®¡ç»Ÿè®¡
    update_cumulative_stats(cumulative_stats, token_info, text_type, model)

    # æ‰“å°è¿›åº¦
    processed = processed_count + (batch_index + 1) * batch_size
    if processed > total_count:
        processed = total_count
    progress = processed / total_count * 100
    print(f"   {text_type.capitalize()}æ‰¹æ¬¡ {batch_index + 1}: {len(texts)} æ¡, "
          f"APIè°ƒç”¨æ—¶é—´ {api_time:.2f}s, "
          f"Token: {token_info.get('total_tokens', 0):,}, "
          f"æ€»è¿›åº¦: {processed}/{total_count} ({progress:.1f}%)")

    # è¿”å›æ˜¯å¦éœ€è¦æ‰“å°æŠ¥å‘Šï¼ˆç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦æ‰“å°å’Œä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
    should_report = (batch_index + 1) % report_interval == 0

    return (
        batch_vectors,
        api_time,
        token_info.get("prompt_tokens", 0),
        token_info.get("total_tokens", 0),
        should_report
    )


def save_checkpoint_with_results(
    output_file: str,
    checkpoint: Dict[str, Any],
    results: List[Dict[str, Any]],
    processed_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    verbose: bool = False
):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ç»“æœï¼‰
    æ³¨æ„ï¼šæ— è®ºæ˜¯å¦ä»å¤´å¼€å§‹ï¼Œè¿è¡Œè¿‡ç¨‹ä¸­éƒ½ä¼šä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿ä¸­æ–­åå¯ä»¥æ¢å¤
    """
    checkpoint["results"] = results
    checkpoint["processed_count"] = processed_count
    checkpoint["performance"] = performance_log
    checkpoint["cumulative_stats"] = cumulative_stats
    save_checkpoint(output_file, checkpoint, verbose=verbose)


def process_qa_data(
    input_file: str,
    output_file: str,
    model: str = "text-embedding-3-small",
    batch_size: int = 100,
    from_scratch: bool = False,
    report_interval: int = 5
):
    """
    å¤„ç†QAæ•°æ®ï¼Œä¸ºqueryå’Œdocumentç”Ÿæˆå‘é‡

    Args:
        input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„æ¨¡å‹
        batch_size: æ‰¹é‡å¤„ç†å¤§å°
        from_scratch: æ˜¯å¦ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹
        report_interval: æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼‰
    """
    print("=" * 60)
    print("å¤„ç†QAæ•°æ® - ç”Ÿæˆå‘é‡")
    print("=" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ¨¡å‹: {model}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    if from_scratch:
        print(f"æ¨¡å¼: ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰")
    else:
        print(f"æ¨¡å¼: è‡ªåŠ¨æ¢å¤ï¼ˆå¦‚æœå­˜åœ¨æ£€æŸ¥ç‚¹ï¼‰")
    print(f"æ€§èƒ½æŠ¥å‘Šé—´éš”: æ¯ {report_interval} ä¸ªæ‰¹æ¬¡")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    data = load_qa_data(input_file)

    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœ from_scratch=Trueï¼Œåˆ™å¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹ï¼‰
    if from_scratch:
        checkpoint = {
            "processed_count": 0,
            "results": [],
            "performance": [],
            "cumulative_stats": get_default_cumulative_stats()
        }
        # å¦‚æœå­˜åœ¨æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·
        checkpoint_file = output_file.replace('.json', '_checkpoint.json')
        if os.path.exists(checkpoint_file):
            print(f"âš ï¸  å‘ç°å·²æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½†å°†ä»å¤´å¼€å§‹å¤„ç†ï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ä¼šè¢«åˆ é™¤ï¼‰")
    else:
        checkpoint = load_checkpoint(output_file)
    processed_count = checkpoint["processed_count"]
    results = checkpoint.get("results", [])
    performance_log = checkpoint.get("performance", [])
    cumulative_stats = checkpoint.get("cumulative_stats", get_default_cumulative_stats())

    if processed_count > 0:
        print(f"ğŸ”„ ä»ç¬¬ {processed_count + 1} æ¡å¼€å§‹ç»§ç»­å¤„ç†...")
        if cumulative_stats.get("total_tokens", 0) > 0:
            print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
            print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

    # è·å–å®¢æˆ·ç«¯
    client = get_openai_client()

    # å¤„ç†å‰©ä½™çš„æ•°æ®
    remaining_data = data[processed_count:]
    total_remaining = len(remaining_data)

    if total_remaining == 0:
        print("âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼")
        return

    print(f"\nğŸ“Š å¤„ç†è¿›åº¦: {processed_count}/{len(data)} ({processed_count/len(data)*100:.1f}%)")
    print(f"   å‰©ä½™: {total_remaining} æ¡\n")

    # æå–queryå’Œdocumentæ–‡æœ¬
    queries = [item["query"] for item in remaining_data]
    documents = [item["document"] for item in remaining_data]

    # å¤„ç†queryå‘é‡
    print("ğŸ” æ­£åœ¨å¤„ç†queryå‘é‡...")
    query_api_time_total = 0.0
    query_vectors = []
    query_total_prompt_tokens = 0
    query_total_tokens = 0

    num_batches = (len(queries) + batch_size - 1) // batch_size
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(queries))
        batch_queries = queries[start_idx:end_idx]

        try:
            batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                batch_queries, "query", i, model, client, batch_size,
                processed_count, len(data), performance_log, cumulative_stats, report_interval
            )

            query_vectors.extend(batch_vectors)
            query_api_time_total += api_time
            query_total_prompt_tokens += prompt_tokens
            query_total_tokens += total_tokens

            # æ›´æ–°ç»“æœï¼ˆåªæ·»åŠ æ–°å¤„ç†çš„æ•°æ®ï¼‰
            current_processed = processed_count + end_idx
            for j, qv in enumerate(batch_vectors):
                idx = processed_count + start_idx + j
                if idx < len(results):
                    # æ›´æ–°å·²æœ‰ç»“æœ
                    results[idx]["query_vector"] = qv
                else:
                    # æ·»åŠ æ–°ç»“æœ
                    results.append({
                        "query": queries[start_idx + j],
                        "document": documents[start_idx + j],
                        "query_vector": qv,
                        "document_vector": None,
                        "score": remaining_data[start_idx + j].get("score")
                    })

            # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
            if should_report:
                # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                print_performance_report(
                    performance_log, current_processed, len(data), "query", model, cumulative_stats
                )
                # ä¿å­˜æ£€æŸ¥ç‚¹
                save_checkpoint_with_results(
                    output_file, checkpoint, results, current_processed,
                    performance_log, cumulative_stats, verbose=True
                )

        except Exception as e:
            print(f"   âŒ Queryæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
            raise

    print(f"âœ… Queryå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {query_api_time_total:.2f}s, "
          f"æ€»Token: {query_total_tokens:,}\n")

    # å¤„ç†documentå‘é‡
    print("ğŸ“„ æ­£åœ¨å¤„ç†documentå‘é‡...")
    doc_api_time_total = 0.0
    doc_vectors = []
    doc_total_prompt_tokens = 0
    doc_total_tokens = 0

    num_batches = (len(documents) + batch_size - 1) // batch_size
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(documents))
        batch_docs = documents[start_idx:end_idx]

        try:
            batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                batch_docs, "document", i, model, client, batch_size,
                processed_count, len(data), performance_log, cumulative_stats, report_interval
            )

            doc_vectors.extend(batch_vectors)
            doc_api_time_total += api_time
            doc_total_prompt_tokens += prompt_tokens
            doc_total_tokens += total_tokens

            # æ›´æ–°ç»“æœï¼ˆå¡«å……documentå‘é‡ï¼‰
            for j, dv in enumerate(batch_vectors):
                idx = processed_count + start_idx + j
                if idx < len(results):
                    results[idx]["document_vector"] = dv
                else:
                    # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨è¿˜æ˜¯å¤„ç†
                    print(f"   âš ï¸  è­¦å‘Š: ç´¢å¼• {idx} è¶…å‡ºç»“æœåˆ—è¡¨èŒƒå›´")

            # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
            if should_report:
                current_processed = processed_count + end_idx
                # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                print_performance_report(
                    performance_log, current_processed, len(data), "document", model, cumulative_stats
                )
                # ä¿å­˜æ£€æŸ¥ç‚¹
                save_checkpoint_with_results(
                    output_file, checkpoint, results, current_processed,
                    performance_log, cumulative_stats, verbose=True
                )

        except Exception as e:
            print(f"   âŒ Documentæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
            raise

    print(f"âœ… Documentå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {doc_api_time_total:.2f}s, "
          f"æ€»Token: {doc_total_tokens:,}\n")

    # éªŒè¯ç»“æœå®Œæ•´æ€§
    if len(results) != len(data):
        print(f"âš ï¸  è­¦å‘Š: ç»“æœæ•°é‡ ({len(results)}) ä¸æ•°æ®æ•°é‡ ({len(data)}) ä¸åŒ¹é…")
        # è¡¥é½ç¼ºå¤±çš„ç»“æœ
        for i in range(len(results), len(data)):
            results.append({
                "query": data[i].get("query", ""),
                "document": data[i].get("document", ""),
                "query_vector": None,
                "document_vector": None,
                "score": data[i].get("score")
            })

    # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
    total_api_time = query_api_time_total + doc_api_time_total
    final_total_tokens = cumulative_stats["total_tokens"]
    final_total_cost = cumulative_stats["total_cost"]

    performance_summary = {
        "total_items": len(data) * 2,  # query + document
        "query_items": len(data),
        "document_items": len(data),
        "total_api_time_seconds": round(total_api_time, 2),
        "query_api_time_seconds": round(query_api_time_total, 2),
        "document_api_time_seconds": round(doc_api_time_total, 2),
        "items_per_second": round(len(remaining_data) * 2 / total_api_time, 2) if total_api_time > 0 else 0,
        "token_usage": {
            "total_prompt_tokens": cumulative_stats["query_prompt_tokens"] + cumulative_stats["document_prompt_tokens"],
            "total_tokens": final_total_tokens,
            "query_prompt_tokens": cumulative_stats["query_prompt_tokens"],
            "query_total_tokens": cumulative_stats["query_total_tokens"],
            "document_prompt_tokens": cumulative_stats["document_prompt_tokens"],
            "document_total_tokens": cumulative_stats["document_total_tokens"],
            "avg_tokens_per_item": round(final_total_tokens / len(data) / 2, 2) if len(data) > 0 else 0
        },
        "cost": {
            "total_cost_usd": round(final_total_cost, 4),
            "query_cost_usd": round(calculate_cost(cumulative_stats["query_total_tokens"], model), 4),
            "document_cost_usd": round(calculate_cost(cumulative_stats["document_total_tokens"], model), 4),
            "price_per_million_tokens": get_model_pricing(model)
        },
        "model": model,
        "batch_size": batch_size,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    final_output = {
        "metadata": {
            "input_file": input_file,
            "model": model,
            "total_items": len(data),
            "processed_items": len(results),
            "vector_dimension": len(query_vectors[0]) if query_vectors else 0,
            "performance": performance_summary
        },
        "results": results,
        "performance_log": performance_log
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

    # æ‰“å°æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ï¼ˆä»…APIè°ƒç”¨æ—¶é—´ï¼‰")
    print("=" * 60)
    print(f"æ€»å¤„ç†é¡¹æ•°: {len(data)} (query: {len(data)}, document: {len(data)})")
    print(f"æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"  - Query APIè°ƒç”¨: {query_api_time_total:.2f}s")
    print(f"  - Document APIè°ƒç”¨: {doc_api_time_total:.2f}s")
    print(f"å¤„ç†é€Ÿåº¦: {performance_summary['items_per_second']:.2f} é¡¹/ç§’")
    print(f"\nTokenæ¶ˆè€—ç»Ÿè®¡:")
    print(f"  æ€»è¾“å…¥Token: {performance_summary['token_usage']['total_prompt_tokens']:,}")
    print(f"  æ€»Token: {final_total_tokens:,}")
    print(f"  - Query Token: {cumulative_stats['query_total_tokens']:,}")
    print(f"  - Document Token: {cumulative_stats['document_total_tokens']:,}")
    print(f"  å¹³å‡æ¯é¡¹Token: {performance_summary['token_usage']['avg_tokens_per_item']:.1f}")
    print(f"\nè´¹ç”¨ç»Ÿè®¡:")
    print(f"  æ¨¡å‹: {model}")
    print(f"  å®šä»·: ${get_model_pricing(model):.4f} / ç™¾ä¸‡tokens")
    print(f"  æ€»è´¹ç”¨: ${final_total_cost:.4f}")
    print(f"  - Queryè´¹ç”¨: ${performance_summary['cost']['query_cost_usd']:.4f}")
    print(f"  - Documentè´¹ç”¨: ${performance_summary['cost']['document_cost_usd']:.4f}")
    print(f"\nå‘é‡ç»´åº¦: {len(query_vectors[0]) if query_vectors else 0}")
    print("=" * 60)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸ºQAæ•°æ®ç”Ÿæˆå‘é‡ï¼ˆqueryå’Œdocumentï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json

  # æŒ‡å®šæ¨¡å‹å’Œæ‰¹é‡å¤§å°
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json -m text-embedding-3-large -b 50

  # ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json --restart

  # è°ƒæ•´æ€§èƒ½æŠ¥å‘Šé—´éš”
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json -r 10
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«queryå’Œdocumentå­—æ®µï¼‰'
    )

    parser.add_argument(
        '-o', '--output',
        required=True,
        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«å‘é‡å’Œæ€§èƒ½æ•°æ®ï¼‰'
    )

    parser.add_argument(
        '-m', '--model',
        default='text-embedding-3-small',
        help='ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: text-embedding-3-smallï¼‰'
    )

    parser.add_argument(
        '-b', '--batch-size',
        type=int,
        default=100,
        help='æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé»˜è®¤: 100ï¼‰'
    )

    parser.add_argument(
        '--restart', '--from-scratch',
        dest='from_scratch',
        action='store_true',
        help='ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸­æ–­åæ¢å¤'
    )

    parser.add_argument(
        '-r', '--report-interval',
        type=int,
        default=5,
        help='æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼Œé»˜è®¤: 5ï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    try:
        process_qa_data(
            input_file=args.input,
            output_file=args.output,
            model=args.model,
            batch_size=args.batch_size,
            from_scratch=args.from_scratch,
            report_interval=args.report_interval
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
