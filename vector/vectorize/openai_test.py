"""
ä½¿ç”¨ OpenAI Embeddings API å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
æ”¯æŒæ ‡å‡† API å’Œ Batch APIï¼ŒåŒ…å«æ€§èƒ½ç»Ÿè®¡å’Œè´¹ç”¨è®¡ç®—

ä½¿ç”¨æ–¹æ³•:
    python vector/vectorize/openai_test.py -i input.json -o output.json
    python vector/vectorize/openai_test.py -i input.json -o output.json -m text-embedding-3-large -b 50 -r 10
"""
import os
import json
import time
import argparse
from typing import List, Optional, Dict, Any, Tuple
from openai import OpenAI

# ==================== é…ç½® ====================

# OpenAI Embeddings æ¨¡å‹å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰
EMBEDDING_PRICING = {
    "text-embedding-3-small": 0.02,  # $0.02 per 1M tokens
    "text-embedding-3-large": 0.13,  # $0.13 per 1M tokens
    "text-embedding-ada-002": 0.10,  # $0.10 per 1M tokens (æ—§æ¨¡å‹)
}


# ==================== å·¥å…·å‡½æ•° ====================

def get_openai_client() -> OpenAI:
    """è·å– OpenAI å®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡è¯»å– API key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    return OpenAI(api_key=api_key)


def get_model_pricing(model: str) -> float:
    """è·å–æ¨¡å‹çš„å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokensï¼Œç¾å…ƒï¼‰"""
    return EMBEDDING_PRICING.get(model, 0.02)  # é»˜è®¤ä½¿ç”¨text-embedding-3-smallçš„å®šä»·


def calculate_cost(tokens: int, model: str) -> float:
    """
    è®¡ç®—è´¹ç”¨ï¼ˆç¾å…ƒï¼‰

    Args:
        tokens: tokenæ•°é‡
        model: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        è´¹ç”¨ï¼ˆç¾å…ƒï¼‰
    """
    price_per_million = get_model_pricing(model)
    return (tokens / 1_000_000) * price_per_million


# ==================== å‘é‡åŒ–å‡½æ•° ====================

def vectorize_text(
    text: str,
    model: str = "text-embedding-3-small",
    client: Optional[OpenAI] = None
) -> List[float]:
    """
    ä½¿ç”¨ OpenAI Embeddings API å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        text: è¦è½¬æ¢çš„æ–‡æœ¬
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "text-embedding-3-small"
        client: OpenAI å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º

    Returns:
        å‘é‡åˆ—è¡¨ï¼ˆæµ®ç‚¹æ•°åˆ—è¡¨ï¼‰
    """
    if client is None:
        client = get_openai_client()

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    response = client.embeddings.create(model=model, input=text)
    return response.data[0].embedding


def vectorize_texts_batch(
    texts: List[str],
    model: str = "text-embedding-3-small",
    client: Optional[OpenAI] = None,
    batch_size: int = 100
) -> Tuple[List[List[float]], float, Dict[str, int]]:
    """
    ä½¿ç”¨ OpenAI Embeddings API æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

    Args:
        texts: è¦è½¬æ¢çš„æ–‡æœ¬åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º "text-embedding-3-small"
        client: OpenAI å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»º
        batch_size: æ¯æ¬¡è¯·æ±‚çš„æ‰¹é‡å¤§å°ï¼ˆOpenAI API æ”¯æŒæœ€å¤š 2048 ä¸ªè¾“å…¥ï¼‰

    Returns:
        (å‘é‡åˆ—è¡¨, APIè°ƒç”¨æ€»æ—¶é—´ï¼ˆç§’ï¼‰, tokenä½¿ç”¨ä¿¡æ¯å­—å…¸)
        tokenä½¿ç”¨ä¿¡æ¯åŒ…å«: prompt_tokens, total_tokens
    """
    if client is None:
        client = get_openai_client()

    if not texts:
        return [], 0.0, {"prompt_tokens": 0, "total_tokens": 0}

    # è¿‡æ»¤ç©ºæ–‡æœ¬
    valid_texts = [(i, text) for i, text in enumerate(texts) if text and text.strip()]
    if not valid_texts:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬")

    all_vectors = [None] * len(texts)
    total_api_time = 0.0
    total_prompt_tokens = 0
    total_tokens = 0

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(valid_texts), batch_size):
        batch = valid_texts[i:i + batch_size]
        batch_texts = [text for _, text in batch]
        batch_indices = [idx for idx, _ in batch]

        # åªè®°å½•APIè°ƒç”¨æ—¶é—´
        api_start = time.time()
        response = client.embeddings.create(model=model, input=batch_texts)
        api_end = time.time()
        api_time = api_end - api_start
        total_api_time += api_time

        # æå–tokenä½¿ç”¨ä¿¡æ¯
        if hasattr(response, 'usage') and response.usage:
            total_prompt_tokens += response.usage.prompt_tokens
            total_tokens += response.usage.total_tokens

        # å°†ç»“æœæ”¾å›åŸä½ç½®
        for idx, embedding in zip(batch_indices, response.data):
            all_vectors[idx] = embedding.embedding

    return all_vectors, total_api_time, {
        "prompt_tokens": total_prompt_tokens,
        "total_tokens": total_tokens
    }


# ==================== æ•°æ®åŠ è½½å’Œæ£€æŸ¥ç‚¹ ====================

def load_qa_data(input_file: str) -> Dict[str, Any]:
    """
    åŠ è½½QAæ•°æ®
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ–°æ ¼å¼: {"query_list": [...], "document_list": [...]}
    2. æ—§æ ¼å¼: [{"query": "...", "document": "...", "score": ...}]
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # æ£€æµ‹æ•°æ®æ ¼å¼
    if isinstance(data, dict) and "query_list" in data and "document_list" in data:
        # æ–°æ ¼å¼ï¼š{"query_list": [...], "document_list": [...]}
        query_list = data.get("query_list", [])
        document_list = data.get("document_list", [])
        print(f"âœ… å·²åŠ è½½æ–°æ ¼å¼æ•°æ®: {len(query_list)} æ¡å”¯ä¸€query, {len(document_list)} æ¡å”¯ä¸€document")
        return {
            "format": "new",
            "query_list": query_list,
            "document_list": document_list
        }
    elif isinstance(data, list):
        # æ—§æ ¼å¼ï¼šåˆ—è¡¨æ ¼å¼
        print(f"âœ… å·²åŠ è½½æ—§æ ¼å¼æ•°æ®: {len(data)} æ¡æ•°æ®")
        return {
            "format": "old",
            "data": data
        }
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼ã€‚æœŸæœ›æ ¼å¼: {{\"query_list\": [...], \"document_list\": [...]}} æˆ– [{{\"query\": ..., \"document\": ...}}]")


def get_default_cumulative_stats() -> Dict[str, Any]:
    """è·å–é»˜è®¤çš„ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "query_prompt_tokens": 0,
        "query_total_tokens": 0,
        "document_prompt_tokens": 0,
        "document_total_tokens": 0,
        "total_tokens": 0,
        "total_cost": 0.0
    }


def get_default_checkpoint() -> Dict[str, Any]:
    """è·å–é»˜è®¤çš„æ£€æŸ¥ç‚¹ç»“æ„"""
    return {
        "query_processed_count": 0,
        "document_processed_count": 0,
        "results": [],
        "performance": [],
        "cumulative_stats": get_default_cumulative_stats(),
        "query_vector_cache": {},  # ç”¨äºå­˜å‚¨å»é‡åçš„queryå‘é‡
        "document_vector_cache": {}  # ç”¨äºå­˜å‚¨å»é‡åçš„documentå‘é‡
    }


def load_checkpoint(output_file: str) -> Dict[str, Any]:
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå·²å¤„ç†çš„æ•°æ®ï¼ŒåŒ…å«æ€§èƒ½å’Œtokenç»Ÿè®¡ï¼‰"""
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        print(f"ğŸ“‚ å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)

            # å…¼å®¹æ—§æ ¼å¼ï¼ˆåªæœ‰processed_countï¼‰
            if 'processed_count' in checkpoint and 'query_processed_count' not in checkpoint:
                # æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                old_count = checkpoint.get('processed_count', 0)
                checkpoint['query_processed_count'] = old_count
                checkpoint['document_processed_count'] = old_count
                del checkpoint['processed_count']

            query_processed = checkpoint.get('query_processed_count', 0)
            document_processed = checkpoint.get('document_processed_count', 0)
            print(f"   Queryå·²å¤„ç†: {query_processed} æ¡")
            print(f"   Documentå·²å¤„ç†: {document_processed} æ¡")

            # æ¢å¤ç´¯è®¡çš„tokenç»Ÿè®¡
            cumulative_stats = checkpoint.get('cumulative_stats', get_default_cumulative_stats())
            if cumulative_stats.get("total_tokens", 0) > 0:
                print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
                print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

            # æ¢å¤queryå‘é‡ç¼“å­˜ï¼ˆç”¨äºå»é‡ï¼‰
            if 'query_vector_cache' not in checkpoint:
                checkpoint['query_vector_cache'] = {}
            # æ¢å¤documentå‘é‡ç¼“å­˜ï¼ˆç”¨äºå»é‡ï¼‰
            if 'document_vector_cache' not in checkpoint:
                checkpoint['document_vector_cache'] = {}

            return checkpoint
        except json.JSONDecodeError as e:
            print(f"   âš ï¸  æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ˆå¯èƒ½ä¿å­˜æ—¶è¢«ä¸­æ–­ï¼‰: {e}")
            print(f"   ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶å¯èƒ½å·²æŸåï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            # å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            backup_file = checkpoint_file + '.corrupted'
            try:
                import shutil
                shutil.move(checkpoint_file, backup_file)
                print(f"   ğŸ“¦ å·²å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°: {backup_file}")
            except Exception as backup_error:
                print(f"   âš ï¸  æ— æ³•å¤‡ä»½æŸåçš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {backup_error}")
            # è¿”å›ç©ºçš„æ£€æŸ¥ç‚¹
            return get_default_checkpoint()
        except Exception as e:
            print(f"   âŒ åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            print(f"   ğŸ’¡ å°†ä»å¤´å¼€å§‹å¤„ç†")
            return get_default_checkpoint()

    return get_default_checkpoint()


def save_checkpoint(output_file: str, checkpoint: Dict[str, Any], verbose: bool = False):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä½¿ç”¨åŸå­æ€§å†™å…¥ï¼Œé¿å…æ–‡ä»¶æŸåï¼‰

    ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ + é‡å‘½åçš„æ–¹å¼ï¼Œç¡®ä¿å†™å…¥è¿‡ç¨‹çš„åŸå­æ€§ï¼š
    1. å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
    2. å†™å…¥æˆåŠŸåå†é‡å‘½åæ›¿æ¢åŸæ–‡ä»¶
    è¿™æ ·å³ä½¿å†™å…¥è¿‡ç¨‹ä¸­è¢«ä¸­æ–­ï¼ŒåŸæ–‡ä»¶ä¹Ÿä¸ä¼šè¢«æŸå
    """
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    temp_file = checkpoint_file + '.tmp'

    try:
        # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, indent=2, ensure_ascii=False)

        # å†™å…¥æˆåŠŸåå†é‡å‘½åï¼ˆåŸå­æ“ä½œï¼‰
        # åœ¨å¤§å¤šæ•°æ–‡ä»¶ç³»ç»Ÿä¸Šï¼Œé‡å‘½åæ˜¯åŸå­æ“ä½œ
        os.replace(temp_file, checkpoint_file)

        if verbose:
            processed = checkpoint.get('processed_count', 0)
            print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {processed} æ¡å·²å¤„ç†")
    except Exception as e:
        # å¦‚æœå†™å…¥å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“ä¿å­˜å¤±è´¥
        raise RuntimeError(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}") from e


def update_cumulative_stats(
    cumulative_stats: Dict[str, Any],
    token_info: Dict[str, int],
    stats_type: str,
    model: str
):
    """æ›´æ–°ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯"""
    cumulative_stats[f"{stats_type}_prompt_tokens"] += token_info.get("prompt_tokens", 0)
    cumulative_stats[f"{stats_type}_total_tokens"] += token_info.get("total_tokens", 0)
    cumulative_stats["total_tokens"] = (
        cumulative_stats["query_total_tokens"] +
        cumulative_stats["document_total_tokens"]
    )
    cumulative_stats["total_cost"] = calculate_cost(cumulative_stats["total_tokens"], model)


# ==================== æ€§èƒ½æŠ¥å‘Š ====================

def print_performance_report(
    performance_log: List[Dict[str, Any]],
    processed_count: int,
    total_count: int,
    report_type: str,
    model: str,
    cumulative_stats: Optional[Dict[str, Any]] = None
):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Šï¼ˆåŒ…å«tokenç»Ÿè®¡å’Œè´¹ç”¨ï¼‰"""
    if not performance_log:
        return

    # ç­›é€‰æŒ‡å®šç±»å‹çš„æ€§èƒ½æ•°æ®
    type_logs = [log for log in performance_log if log.get("type") == report_type]
    if not type_logs:
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_api_time = sum(log.get("time_seconds", 0) for log in type_logs)
    total_items = sum(log.get("batch_size", 0) for log in type_logs)
    total_prompt_tokens = sum(log.get("prompt_tokens", 0) for log in type_logs)
    total_tokens = sum(log.get("total_tokens", 0) for log in type_logs)
    avg_time_per_batch = total_api_time / len(type_logs) if type_logs else 0
    avg_items_per_second = total_items / total_api_time if total_api_time > 0 else 0
    avg_tokens_per_item = total_tokens / total_items if total_items > 0 else 0

    # è®¡ç®—è´¹ç”¨
    cost = calculate_cost(total_tokens, model)

    # ç´¯è®¡ç»Ÿè®¡ï¼ˆå¦‚æœæä¾›ï¼‰
    cumulative_tokens = cumulative_stats.get("total_tokens", 0) if cumulative_stats else 0
    cumulative_cost = cumulative_stats.get("total_cost", 0.0) if cumulative_stats else 0.0

    print(f"\nğŸ“Š {report_type.upper()} æ€§èƒ½æŠ¥å‘Š:")
    print(f"   å·²å¤„ç†æ‰¹æ¬¡: {len(type_logs)}")
    print(f"   æ€»å¤„ç†é¡¹æ•°: {total_items}")
    print(f"   æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"   å¹³å‡æ¯æ‰¹æ¬¡æ—¶é—´: {avg_time_per_batch:.2f}s")
    print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {avg_items_per_second:.2f} é¡¹/ç§’")
    print(f"   Tokenç»Ÿè®¡:")
    print(f"     - æ€»è¾“å…¥Token: {total_prompt_tokens:,}")
    print(f"     - æ€»Token: {total_tokens:,}")
    print(f"     - å¹³å‡æ¯é¡¹Token: {avg_tokens_per_item:.1f}")
    print(f"   è´¹ç”¨ç»Ÿè®¡:")
    print(f"     - æœ¬æ¬¡è´¹ç”¨: ${cost:.4f}")
    if cumulative_stats:
        print(f"     - ç´¯è®¡Token: {cumulative_tokens:,}")
        print(f"     - ç´¯è®¡è´¹ç”¨: ${cumulative_cost:.4f}")
    print(f"   æ€»è¿›åº¦: {processed_count}/{total_count} ({processed_count/total_count*100:.1f}%)")
    print()


# ==================== ä¸»å¤„ç†å‡½æ•° ====================

def process_batch(
    texts: List[str],
    text_type: str,
    batch_index: int,
    model: str,
    client: OpenAI,
    batch_size: int,
    processed_unique_count: int,
    unique_total_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    report_interval: int
) -> Tuple[List[List[float]], float, int, int, bool]:
    """
    å¤„ç†ä¸€æ‰¹æ–‡æœ¬ï¼Œè¿”å›å‘é‡ã€APIæ—¶é—´ã€tokenç»Ÿè®¡

    Args:
        processed_unique_count: å·²å¤„ç†çš„å”¯ä¸€é¡¹æ•°é‡ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºåˆ†å­ï¼‰
        unique_total_count: å»é‡åçš„æ€»æ•°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºåˆ†æ¯ï¼‰

    Returns:
        (å‘é‡åˆ—è¡¨, APIæ€»æ—¶é—´, prompt_tokens, total_tokens, should_report)
        should_report: æ˜¯å¦éœ€è¦æ‰“å°æ€§èƒ½æŠ¥å‘Š
    """
    # APIè°ƒç”¨å‰æç¤ºï¼ˆè®©ç”¨æˆ·çŸ¥é“ç¨‹åºè¿˜åœ¨è¿è¡Œï¼‰
    if batch_index == 0 or (batch_index + 1) % 10 == 0:
        print(f"   â³ æ­£åœ¨è°ƒç”¨APIå¤„ç† {text_type}æ‰¹æ¬¡ {batch_index + 1}...")

    batch_vectors, api_time, token_info = vectorize_texts_batch(
        texts, model=model, client=client, batch_size=batch_size
    )

    # è®°å½•æ€§èƒ½
    batch_perf = {
        "type": text_type,
        "batch_index": batch_index,
        "batch_size": len(texts),
        "time_seconds": round(api_time, 2),
        "items_per_second": round(len(texts) / api_time, 2) if api_time > 0 else 0,
        "prompt_tokens": token_info.get("prompt_tokens", 0),
        "total_tokens": token_info.get("total_tokens", 0)
    }
    performance_log.append(batch_perf)

    # æ›´æ–°ç´¯è®¡ç»Ÿè®¡
    update_cumulative_stats(cumulative_stats, token_info, text_type, model)

    # æ‰“å°è¿›åº¦ï¼šä½¿ç”¨å®é™…å·²å¤„ç†çš„å”¯ä¸€é¡¹æ•°é‡ä½œä¸ºåˆ†å­ï¼Œå»é‡åçš„æ€»æ•°ä½œä¸ºåˆ†æ¯
    processed = processed_unique_count + len(texts)
    if processed > unique_total_count:
        processed = unique_total_count
    progress = processed / unique_total_count * 100 if unique_total_count > 0 else 0
    print(f"   {text_type.capitalize()}æ‰¹æ¬¡ {batch_index + 1}: {len(texts)} æ¡, "
          f"APIè°ƒç”¨æ—¶é—´ {api_time:.2f}s, "
          f"Token: {token_info.get('total_tokens', 0):,}, "
          f"æ€»è¿›åº¦: {processed}/{unique_total_count} ({progress:.1f}%)")

    # è¿”å›æ˜¯å¦éœ€è¦æ‰“å°æŠ¥å‘Šï¼ˆç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦æ‰“å°å’Œä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
    should_report = (batch_index + 1) % report_interval == 0

    return (
        batch_vectors,
        api_time,
        token_info.get("prompt_tokens", 0),
        token_info.get("total_tokens", 0),
        should_report
    )


def save_checkpoint_with_results(
    output_file: str,
    checkpoint: Dict[str, Any],
    results: List[Dict[str, Any]],
    query_processed_count: int,
    document_processed_count: int,
    performance_log: List[Dict[str, Any]],
    cumulative_stats: Dict[str, Any],
    query_vector_cache: Optional[Dict[str, List[float]]] = None,
    document_vector_cache: Optional[Dict[str, List[float]]] = None,
    verbose: bool = False
):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ç»“æœï¼‰
    æ³¨æ„ï¼šæ— è®ºæ˜¯å¦ä»å¤´å¼€å§‹ï¼Œè¿è¡Œè¿‡ç¨‹ä¸­éƒ½ä¼šä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿ä¸­æ–­åå¯ä»¥æ¢å¤
    """
    checkpoint["results"] = results
    checkpoint["query_processed_count"] = query_processed_count
    checkpoint["document_processed_count"] = document_processed_count
    checkpoint["performance"] = performance_log
    checkpoint["cumulative_stats"] = cumulative_stats
    if query_vector_cache is not None:
        checkpoint["query_vector_cache"] = query_vector_cache
    if document_vector_cache is not None:
        checkpoint["document_vector_cache"] = document_vector_cache
    save_checkpoint(output_file, checkpoint, verbose=verbose)


def process_qa_data(
    input_file: str,
    output_file: str,
    model: str = "text-embedding-3-small",
    batch_size: int = 100,
    from_scratch: bool = False,
    report_interval: int = 5,
    max_items: Optional[int] = None
):
    """
    å¤„ç†QAæ•°æ®ï¼Œä¸ºqueryå’Œdocumentç”Ÿæˆå‘é‡

    Args:
        input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„æ¨¡å‹
        batch_size: æ‰¹é‡å¤„ç†å¤§å°
        from_scratch: æ˜¯å¦ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹
        report_interval: æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼‰
        max_items: æœ€å¤§å¤„ç†æ¡æ•°ï¼ˆNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°æ®ï¼‰
    """
    print("=" * 60)
    print("å¤„ç†QAæ•°æ® - ç”Ÿæˆå‘é‡")
    print("=" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ¨¡å‹: {model}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    if from_scratch:
        print(f"æ¨¡å¼: ä»å¤´å¼€å§‹ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰")
    else:
        print(f"æ¨¡å¼: è‡ªåŠ¨æ¢å¤ï¼ˆå¦‚æœå­˜åœ¨æ£€æŸ¥ç‚¹ï¼‰")
    print(f"æ€§èƒ½æŠ¥å‘Šé—´éš”: æ¯ {report_interval} ä¸ªæ‰¹æ¬¡")
    if max_items:
        print(f"æœ€å¤§å¤„ç†æ¡æ•°: {max_items}")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    loaded_data = load_qa_data(input_file)

    # æ ¹æ®æ•°æ®æ ¼å¼å¤„ç†
    if loaded_data["format"] == "new":
        # æ–°æ ¼å¼ï¼šå·²ç»å»é‡çš„åˆ—è¡¨
        query_list = loaded_data["query_list"]
        document_list = loaded_data["document_list"]

        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ¡æ•°ï¼Œåˆ™æˆªå–æ•°æ®
        if max_items and max_items > 0:
            original_query_count = len(query_list)
            original_doc_count = len(document_list)
            query_list = query_list[:max_items]
            document_list = document_list[:max_items]
            print(f"ğŸ“Š æ•°æ®é™åˆ¶: Queryä» {original_query_count} æ¡é™åˆ¶åˆ° {len(query_list)} æ¡")
            print(f"ğŸ“Š æ•°æ®é™åˆ¶: Documentä» {original_doc_count} æ¡é™åˆ¶åˆ° {len(document_list)} æ¡")

        # è½¬æ¢ä¸ºæ—§æ ¼å¼çš„å…¼å®¹ç»“æ„ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        data_format = "new"
        data = None  # æ–°æ ¼å¼ä¸éœ€è¦ data åˆ—è¡¨
    else:
        # æ—§æ ¼å¼ï¼šåˆ—è¡¨æ ¼å¼
        data = loaded_data["data"]
        data_format = "old"

        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ¡æ•°ï¼Œåˆ™æˆªå–æ•°æ®
        original_data_count = len(data)
        if max_items and max_items > 0:
            data = data[:max_items]
            print(f"ğŸ“Š æ•°æ®é™åˆ¶: ä» {original_data_count} æ¡é™åˆ¶åˆ° {len(data)} æ¡")

        # ä»æ—§æ ¼å¼ä¸­æå– query_list å’Œ document_list
        query_list = [item["query"] for item in data if item.get("query")]
        document_list = [item["document"] for item in data if item.get("document")]

    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœ from_scratch=Trueï¼Œåˆ™å¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹ï¼‰
    if from_scratch:
        checkpoint = get_default_checkpoint()
        # å¦‚æœå­˜åœ¨æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·
        checkpoint_file = output_file.replace('.json', '_checkpoint.json')
        if os.path.exists(checkpoint_file):
            print(f"âš ï¸  å‘ç°å·²æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½†å°†ä»å¤´å¼€å§‹å¤„ç†ï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ä¼šè¢«åˆ é™¤ï¼‰")
    else:
        checkpoint = load_checkpoint(output_file)

    query_processed_count = checkpoint.get("query_processed_count", 0)
    document_processed_count = checkpoint.get("document_processed_count", 0)
    results = checkpoint.get("results", [])
    performance_log = checkpoint.get("performance", [])
    cumulative_stats = checkpoint.get("cumulative_stats", get_default_cumulative_stats())
    query_vector_cache = checkpoint.get("query_vector_cache", {})
    document_vector_cache = checkpoint.get("document_vector_cache", {})

    if query_processed_count > 0 or document_processed_count > 0:
        print(f"ğŸ”„ æ–­ç‚¹ç»­è·‘çŠ¶æ€:")
        print(f"   Query: {query_processed_count}/{len(query_list)} æ¡å·²å¤„ç†")
        print(f"   Document: {document_processed_count}/{len(document_list)} æ¡å·²å¤„ç†")
        if cumulative_stats.get("total_tokens", 0) > 0:
            print(f"   ç´¯è®¡Token: {cumulative_stats['total_tokens']:,}")
            print(f"   ç´¯è®¡è´¹ç”¨: ${cumulative_stats.get('total_cost', 0):.4f}")

    # è·å–å®¢æˆ·ç«¯
    client = get_openai_client()

    # å¤„ç†å‰©ä½™çš„æ•°æ®ï¼ˆæ–°æ ¼å¼å·²ç»æ˜¯å»é‡åçš„åˆ—è¡¨ï¼‰
    query_remaining_data = query_list[query_processed_count:]
    document_remaining_data = document_list[document_processed_count:]

    if len(query_remaining_data) == 0 and len(document_remaining_data) == 0:
        print("âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼")
        return

    print(f"\nğŸ“Š å¤„ç†è¿›åº¦:")
    print(f"   Query: {query_processed_count}/{len(query_list)} ({query_processed_count/len(query_list)*100:.1f}%), å‰©ä½™: {len(query_remaining_data)} æ¡")
    print(f"   Document: {document_processed_count}/{len(document_list)} ({document_processed_count/len(document_list)*100:.1f}%), å‰©ä½™: {len(document_remaining_data)} æ¡\n")

    # æ–°æ ¼å¼å·²ç»æ˜¯å»é‡åçš„åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
    unique_query_list = query_remaining_data
    unique_document_list = document_remaining_data

    print(f"ğŸ” Queryå»é‡åˆ†æ: æ•°æ®å·²å»é‡ï¼Œå…± {len(unique_query_list)} æ¡å”¯ä¸€query")
    print(f"ğŸ” Documentå»é‡åˆ†æ: æ•°æ®å·²å»é‡ï¼Œå…± {len(unique_document_list)} æ¡å”¯ä¸€document")

    # å¤„ç†queryå‘é‡ï¼ˆä½¿ç”¨å»é‡åçš„å”¯ä¸€queryï¼‰
    print("ğŸ” æ­£åœ¨å¤„ç†queryå‘é‡ï¼ˆå·²å»é‡ï¼‰...")
    query_api_time_total = 0.0
    unique_query_vectors = {}  # å­˜å‚¨å”¯ä¸€queryçš„å‘é‡
    query_total_prompt_tokens = 0
    query_total_tokens = 0

    # æ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„queryå‘é‡
    cached_count = 0
    for query in unique_query_list:
        if query in query_vector_cache:
            unique_query_vectors[query] = query_vector_cache[query]
            cached_count += 1

    if cached_count > 0:
        print(f"   ğŸ’¾ ä»ç¼“å­˜ä¸­æ¢å¤ {cached_count} ä¸ªqueryå‘é‡")

    # åªå¤„ç†æœªç¼“å­˜çš„å”¯ä¸€query
    uncached_unique_queries = [q for q in unique_query_list if q not in unique_query_vectors]
    last_batch_was_reported = False

    if uncached_unique_queries:
        num_batches = (len(uncached_unique_queries) + batch_size - 1) // batch_size
        processed_unique_queries = 0  # è·Ÿè¸ªå·²å¤„ç†çš„å”¯ä¸€queryæ•°é‡

        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(uncached_unique_queries))
            batch_queries = uncached_unique_queries[start_idx:end_idx]

            try:
                batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                    batch_queries, "query", i, model, client, batch_size,
                    processed_unique_queries, len(uncached_unique_queries), performance_log, cumulative_stats, report_interval
                )

                # æ›´æ–°å·²å¤„ç†çš„å”¯ä¸€queryæ•°é‡
                processed_unique_queries += len(batch_queries)

                query_api_time_total += api_time
                query_total_prompt_tokens += prompt_tokens
                query_total_tokens += total_tokens

                # å­˜å‚¨å”¯ä¸€queryçš„å‘é‡åˆ°ç¼“å­˜
                for j, qv in enumerate(batch_vectors):
                    query_text = batch_queries[j]
                    unique_query_vectors[query_text] = qv
                    query_vector_cache[query_text] = qv

                # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
                if should_report:
                    # è®¡ç®—å·²å¤„ç†çš„queryæ•°é‡
                    current_query_processed = query_processed_count + processed_unique_queries
                    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                    print_performance_report(
                        performance_log, current_query_processed, len(query_list), "query", model, cumulative_stats
                    )
                    # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«queryå‘é‡ç¼“å­˜ï¼‰
                    save_checkpoint_with_results(
                        output_file, checkpoint, results,
                        current_query_processed,
                        document_processed_count,
                        performance_log, cumulative_stats, query_vector_cache, document_vector_cache, verbose=True
                    )
                    last_batch_was_reported = True
                elif i == num_batches - 1:
                    # æœ€åä¸€æ‰¹ï¼Œå³ä½¿ä¸æ»¡è¶³æŠ¥å‘Šé—´éš”ä¹Ÿè¦æŠ¥å‘Š
                    last_batch_was_reported = False

            except Exception as e:
                print(f"   âŒ Queryæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
                raise
    else:
        print(f"   âœ… æ‰€æœ‰queryå‘é‡å·²ä»ç¼“å­˜ä¸­æ¢å¤ï¼Œæ— éœ€APIè°ƒç”¨")

    # Queryå¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    final_query_processed = query_processed_count + len(query_remaining_data)
    if not last_batch_was_reported and (len(uncached_unique_queries) > 0 or cached_count > 0):
        # æ‰“å°æœ€ç»ˆqueryæ€§èƒ½æŠ¥å‘Š
        print_performance_report(
            performance_log, final_query_processed, len(query_list), "query", model, cumulative_stats
        )
        # ä¿å­˜æ£€æŸ¥ç‚¹
        save_checkpoint_with_results(
            output_file, checkpoint, results,
            final_query_processed,
            document_processed_count,
            performance_log, cumulative_stats, query_vector_cache, document_vector_cache, verbose=True
        )

    print(f"âœ… Queryå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {query_api_time_total:.2f}s, "
          f"æ€»Token: {query_total_tokens:,}, "
          f"å®é™…è°ƒç”¨: {len(uncached_unique_queries)} æ¡å”¯ä¸€query\n")

    # å¤„ç†documentå‘é‡
    print("ğŸ“„ æ­£åœ¨å¤„ç†documentå‘é‡...")

    # æ–°æ ¼å¼å·²ç»æ˜¯å»é‡åçš„åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
    unique_document_list = document_remaining_data

    doc_api_time_total = 0.0
    unique_document_vectors = {}  # å­˜å‚¨å”¯ä¸€documentçš„å‘é‡
    doc_total_prompt_tokens = 0
    doc_total_tokens = 0

    # æ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„documentå‘é‡
    cached_doc_count = 0
    for document in unique_document_list:
        if document in document_vector_cache:
            unique_document_vectors[document] = document_vector_cache[document]
            cached_doc_count += 1

    if cached_doc_count > 0:
        print(f"   ğŸ’¾ ä»ç¼“å­˜ä¸­æ¢å¤ {cached_doc_count} ä¸ªdocumentå‘é‡")

    # åªå¤„ç†æœªç¼“å­˜çš„å”¯ä¸€document
    uncached_unique_documents = [d for d in unique_document_list if d not in unique_document_vectors]

    num_batches = (len(uncached_unique_documents) + batch_size - 1) // batch_size if uncached_unique_documents else 0
    last_doc_batch_was_reported = False

    if uncached_unique_documents:
        processed_unique_documents = 0  # è·Ÿè¸ªå·²å¤„ç†çš„å”¯ä¸€documentæ•°é‡

        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(uncached_unique_documents))
            batch_docs = uncached_unique_documents[start_idx:end_idx]

            try:
                batch_vectors, api_time, prompt_tokens, total_tokens, should_report = process_batch(
                    batch_docs, "document", i, model, client, batch_size,
                    processed_unique_documents, len(uncached_unique_documents), performance_log, cumulative_stats, report_interval
                )

                # æ›´æ–°å·²å¤„ç†çš„å”¯ä¸€documentæ•°é‡
                processed_unique_documents += len(batch_docs)

                doc_api_time_total += api_time
                doc_total_prompt_tokens += prompt_tokens
                doc_total_tokens += total_tokens

                # å­˜å‚¨å”¯ä¸€documentçš„å‘é‡åˆ°ç¼“å­˜
                for j, dv in enumerate(batch_vectors):
                    document_text = batch_docs[j]
                    unique_document_vectors[document_text] = dv
                    document_vector_cache[document_text] = dv

                # åœ¨æ‰“å°æ€§èƒ½æŠ¥å‘Šæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆé€»è¾‘åˆ†ç¦»ï¼šå…ˆæ‰“å°æŠ¥å‘Šï¼Œå†ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
                if should_report:
                    current_doc_processed = document_processed_count + processed_unique_documents
                    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
                    print_performance_report(
                        performance_log, current_doc_processed, len(document_list), "document", model, cumulative_stats
                    )
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    save_checkpoint_with_results(
                        output_file, checkpoint, results,
                        final_query_processed,  # æ‰€æœ‰queryéƒ½å·²å¤„ç†
                        current_doc_processed,
                        performance_log, cumulative_stats, query_vector_cache, document_vector_cache, verbose=True
                    )
                    last_doc_batch_was_reported = True
                elif i == num_batches - 1:
                    # æœ€åä¸€æ‰¹ï¼Œå³ä½¿ä¸æ»¡è¶³æŠ¥å‘Šé—´éš”ä¹Ÿè¦æŠ¥å‘Š
                    last_doc_batch_was_reported = False

            except Exception as e:
                print(f"   âŒ Documentæ‰¹æ¬¡ {i + 1} å¤„ç†å¤±è´¥: {e}")
                raise
    else:
        print(f"   âœ… æ‰€æœ‰documentå‘é‡å·²ä»ç¼“å­˜ä¸­æ¢å¤ï¼Œæ— éœ€APIè°ƒç”¨")

    # Documentå¤„ç†å®Œæˆï¼Œå¦‚æœæœ€åä¸€æ‰¹æ²¡æœ‰æŠ¥å‘Šï¼Œåˆ™è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    final_doc_processed = document_processed_count + len(document_remaining_data)
    if not last_doc_batch_was_reported and (num_batches > 0 or cached_doc_count > 0):
        # æ‰“å°æœ€ç»ˆdocumentæ€§èƒ½æŠ¥å‘Š
        print_performance_report(
            performance_log, final_doc_processed, len(document_list), "document", model, cumulative_stats
        )
        # ä¿å­˜æ£€æŸ¥ç‚¹
        save_checkpoint_with_results(
            output_file, checkpoint, results,
            final_query_processed,
            final_doc_processed,
            performance_log, cumulative_stats, query_vector_cache, document_vector_cache, verbose=True
        )

    print(f"âœ… Documentå‘é‡å¤„ç†å®Œæˆï¼Œæ€»APIè°ƒç”¨æ—¶é—´: {doc_api_time_total:.2f}s, "
          f"æ€»Token: {doc_total_tokens:,}, "
          f"å®é™…è°ƒç”¨: {len(uncached_unique_documents)} æ¡å”¯ä¸€document\n")

    # æ„å»ºè¾“å‡ºå‘é‡åˆ—è¡¨ï¼ˆæŒ‰åŸå§‹é¡ºåºï¼‰
    query_vectors = []
    document_vectors = []

    if data_format == "new":
        # æ–°æ ¼å¼ï¼šç›´æ¥æŒ‰é¡ºåºæ„å»ºå‘é‡åˆ—è¡¨
        for query in query_list:
            query_vectors.append(query_vector_cache.get(query))
        for document in document_list:
            document_vectors.append(document_vector_cache.get(document))
    else:
        # æ—§æ ¼å¼ï¼šä» results ä¸­æå–
        for item in data:
            query = item.get("query", "")
            document = item.get("document", "")
            query_vectors.append(query_vector_cache.get(query))
            document_vectors.append(document_vector_cache.get(document))

    # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
    total_api_time = query_api_time_total + doc_api_time_total
    final_total_tokens = cumulative_stats["total_tokens"]
    final_total_cost = cumulative_stats["total_cost"]

    total_query_count = len(query_list)
    total_doc_count = len(document_list)

    performance_summary = {
        "total_items": total_query_count + total_doc_count,
        "query_items": total_query_count,
        "document_items": total_doc_count,
        "total_api_time_seconds": round(total_api_time, 2),
        "query_api_time_seconds": round(query_api_time_total, 2),
        "document_api_time_seconds": round(doc_api_time_total, 2),
        "items_per_second": round((total_query_count + total_doc_count) / total_api_time, 2) if total_api_time > 0 else 0,
        "token_usage": {
            "total_prompt_tokens": cumulative_stats["query_prompt_tokens"] + cumulative_stats["document_prompt_tokens"],
            "total_tokens": final_total_tokens,
            "query_prompt_tokens": cumulative_stats["query_prompt_tokens"],
            "query_total_tokens": cumulative_stats["query_total_tokens"],
            "document_prompt_tokens": cumulative_stats["document_prompt_tokens"],
            "document_total_tokens": cumulative_stats["document_total_tokens"],
            "avg_tokens_per_item": round(final_total_tokens / (total_query_count + total_doc_count), 2) if (total_query_count + total_doc_count) > 0 else 0
        },
        "cost": {
            "total_cost_usd": round(final_total_cost, 4),
            "query_cost_usd": round(calculate_cost(cumulative_stats["query_total_tokens"], model), 4),
            "document_cost_usd": round(calculate_cost(cumulative_stats["document_total_tokens"], model), 4),
            "price_per_million_tokens": get_model_pricing(model)
        },
        "model": model,
        "batch_size": batch_size,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    vector_dimension = len(query_vectors[0]) if query_vectors and query_vectors[0] else (len(document_vectors[0]) if document_vectors and document_vectors[0] else 0)

    final_output = {
        "metadata": {
            "input_file": input_file,
            "model": model,
            "query_count": total_query_count,
            "document_count": total_doc_count,
            "vector_dimension": vector_dimension,
            "performance": performance_summary
        },
        "query_vectors": query_vectors,
        "document_vectors": document_vectors,
        "performance_log": performance_log
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¤„ç†å®Œæˆï¼‰
    checkpoint_file = output_file.replace('.json', '_checkpoint.json')
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

    # æ‰“å°æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ï¼ˆä»…APIè°ƒç”¨æ—¶é—´ï¼‰")
    print("=" * 60)
    print(f"æ€»å¤„ç†é¡¹æ•°: {total_query_count + total_doc_count} (query: {total_query_count}, document: {total_doc_count})")
    print(f"æ€»APIè°ƒç”¨æ—¶é—´: {total_api_time:.2f}s")
    print(f"  - Query APIè°ƒç”¨: {query_api_time_total:.2f}s")
    print(f"  - Document APIè°ƒç”¨: {doc_api_time_total:.2f}s")
    print(f"å¤„ç†é€Ÿåº¦: {performance_summary['items_per_second']:.2f} é¡¹/ç§’")
    if total_query_count + total_doc_count > 0:
        avg_time_per_item = total_api_time / (total_query_count + total_doc_count)
        avg_query_time = query_api_time_total / total_query_count if total_query_count > 0 else 0
        avg_doc_time = doc_api_time_total / total_doc_count if total_doc_count > 0 else 0
        print(f"\nå¹³å‡æ¯æ¡æ•°æ®è€—æ—¶:")
        print(f"  æ€»è€—æ—¶: {avg_time_per_item:.4f}s/æ¡")
        print(f"  - Queryè€—æ—¶: {avg_query_time:.4f}s/æ¡")
        print(f"  - Documentè€—æ—¶: {avg_doc_time:.4f}s/æ¡")
    print(f"\nTokenæ¶ˆè€—ç»Ÿè®¡:")
    print(f"  æ€»è¾“å…¥Token: {performance_summary['token_usage']['total_prompt_tokens']:,}")
    print(f"  æ€»Token: {final_total_tokens:,}")
    print(f"  - Query Token: {cumulative_stats['query_total_tokens']:,}")
    print(f"  - Document Token: {cumulative_stats['document_total_tokens']:,}")
    print(f"  å¹³å‡æ¯é¡¹Token: {performance_summary['token_usage']['avg_tokens_per_item']:.1f}")
    print(f"\nè´¹ç”¨ç»Ÿè®¡:")
    print(f"  æ¨¡å‹: {model}")
    print(f"  å®šä»·: ${get_model_pricing(model):.4f} / ç™¾ä¸‡tokens")
    print(f"  æ€»è´¹ç”¨: ${final_total_cost:.4f}")
    print(f"  - Queryè´¹ç”¨: ${performance_summary['cost']['query_cost_usd']:.4f}")
    print(f"  - Documentè´¹ç”¨: ${performance_summary['cost']['document_cost_usd']:.4f}")
    print(f"\nå‘é‡ç»´åº¦: {vector_dimension}")
    print("=" * 60)


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸ºQAæ•°æ®ç”Ÿæˆå‘é‡ï¼ˆqueryå’Œdocumentï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json

  # æŒ‡å®šæ¨¡å‹å’Œæ‰¹é‡å¤§å°
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json -m text-embedding-3-large -b 50

  # ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json --restart

  # è°ƒæ•´æ€§èƒ½æŠ¥å‘Šé—´éš”
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json -r 10

  # é™åˆ¶å¤„ç†æ¡æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  python vector/vectorize/openai_test.py -i .data/mteb/nfcorpus.json -o .data/vectors/nfcorpus_vectors.json --max-items 100
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«queryå’Œdocumentå­—æ®µï¼‰'
    )

    parser.add_argument(
        '-o', '--output',
        required=True,
        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«å‘é‡å’Œæ€§èƒ½æ•°æ®ï¼‰'
    )

    parser.add_argument(
        '-m', '--model',
        default='text-embedding-3-small',
        help='ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤: text-embedding-3-smallï¼‰'
    )

    parser.add_argument(
        '-b', '--batch-size',
        type=int,
        default=100,
        help='æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé»˜è®¤: 100ï¼‰'
    )

    parser.add_argument(
        '--restart', '--from-scratch',
        dest='from_scratch',
        action='store_true',
        help='ä»å¤´å¼€å§‹å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰æ£€æŸ¥ç‚¹ï¼‰ï¼Œä½†è¿è¡Œè¿‡ç¨‹ä¸­ä»ä¼šä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸­æ–­åæ¢å¤'
    )

    parser.add_argument(
        '-r', '--report-interval',
        type=int,
        default=5,
        help='æ€§èƒ½æŠ¥å‘Šæ‰“å°é—´éš”ï¼ˆæ¯Nä¸ªæ‰¹æ¬¡ï¼Œé»˜è®¤: 5ï¼‰'
    )

    parser.add_argument(
        '--max-items',
        type=int,
        default=None,
        help='æœ€å¤§å¤„ç†æ¡æ•°ï¼ˆé»˜è®¤: å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç”¨äºæµ‹è¯•æˆ–å°è§„æ¨¡è¿è¡Œï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    try:
        process_qa_data(
            input_file=args.input,
            output_file=args.output,
            model=args.model,
            batch_size=args.batch_size,
            from_scratch=args.from_scratch,
            report_interval=args.report_interval,
            max_items=args.max_items
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
